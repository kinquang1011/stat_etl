DKV dbggame

spark.read.option("delimiter","\t").csv("/ge/gamelogs/dbg/2017-09-{02,03}").where("_c0 like '20170903%' and _c26 = 'SUCCESSFUL' and _c1 = '10314'").count

spark.read.option("delimiter","\t").csv("/ge/gamelogs/dbg/2018-01-{16,17}").where("_c0 like '20180117%' and _c26 = 'SUCCESSFUL' and _c1 = '10314'").agg(sum("_c33").cast("long")).show

ttk

  //split string
  val parseId = udf { (s: String) => {
      if(s.contains("@vng.win.163.com")){
        s.substring(0,s.lastIndexOf("@"))
      }
  }
  }

####### check ccu
spark.read.parquet("/ge/fairy/warehouse/flumeccu/2018-01-17").where("cmdb_prd_code like '323' and log_date like '2018-01-17%'").agg(avg("ccu"), max("ccu")).show

#### check Code CCU
spark.read.parquet("/ge/fairy/warehouse/flumeccu/2018-01-17").select("name","cmdb_prd_code").distinct().show(200,false)

##### hk - dbggames - pc

##### ck - dbggames - pc

##### kv - dbggames - pc


#### cgmfbs

 spark.read.option("delimiter","\t").csv("/ge/warehouse/sgmb/recharge/2018-01-17/part-v000-o000-r-00000.gz").agg(sum("_c5").cast("long")*100).show

#### tlbbm
 val converRate = udf { (s13: String,s10:String) => {
     var amt = s13.toLong * 200
     if(s13 == "3000" && s10 < "50"){
       amt = 500000L
     }
     amt
   }
   }

 val df = spark.read.option("delimiter","\t").csv("/ge/warehouse/tlbbm/recharge/2018-01-17/*.gz")

var dv = df.withColumn("net_amt",converRate(col("_c13"),col("_c10")))
dv.where("_c0 like '2018-01-17%' and _c6 != '23000'").agg(sum("net_amt").cast("long")).show

#### tlbbm

 val df = spark.read.option("delimiter","\t").csv("/ge/warehouse/tlbbm/recharge/2017-11-*/*.gz")

 val converRate = udf { (s13: String,s10:String) => {
     var amt = s13.toLong * 200
     if(s13 == "3000" && s10 < "50"){
       amt = 500000L
     }
     amt
   }
   }

var dv = df.withColumn("net_amt",converRate(col("_c13"),col("_c10")))
dv.where("_c0 like '2017-11-%' and _c6 != '23000'").withColumn("log_date",substring(col("_c0"),0,10)).groupBy("log_date").agg(sum("net_amt").cast("long")).show


##### cack

spark.read.json("/ge/gamelogs/sdk/2018-01-{16,17}/Log_CACK_DBGAdd/Log_CACK_DBGAdd-2018-01-{16,17}.gz").where("updateTime like '2018-01-17%' and resultCode = '1'").agg(sum("pmcNetChargeAmt").cast("long")).show


##### project c
import vng.ge.stats.etl.utils.{Common, DateTimeUtils}

  val convertDate = udf { (timeStamp: Long) => {
    DateTimeUtils.getDate(timeStamp*1000)
  }
  }


spark.read.option("delimiter","\t").csv("/ge/gamelogs/projectc/20180117/pjc_20180117_logdb/t_acct_water*").withColumn("_c19",convertDate(col("_c19"))).where("_c11 = '1' and _c10 = '1' and _c9 = '1' and _c19 like '2018-01-17%'").agg(sum("_c15").cast("long")*100).show

##### pjsea

spark.read.option("delimiter","\t").csv("/ge/gamelogs/pjcsea/20180117/logdumpdb/*/t_acct_water_*").where("_c11 = '1' and _c10 = '1' and _c9 = '1'").agg((sum("_c15")*0.25*670).cast("long")).show


##### HPT

spark.read.json("/ge/gamelogs/sdk/2018-01-{16,17}/Log_HPT_DBGAdd/Log_HPT_DBGAdd-2018-01-{16,17}.gz").where("updateTime like '2018-01-17%' and resultCode = '1'").agg(sum("pmcNetChargeAmt").cast("long")).show

##### jxm

val raw = spark.read.textFile("/ge/warehouse/jxm/RechargeFlow/2018-02-04/*.gz")
    val paymentDs = raw.map(line => line.split("\t")).map { line =>
      val net = line(9)
      (net)
    }.toDF("net_amt")

paymentDs.agg(sum("net_amt").cast("long")*100).show

##### jxm

        def loginFilter(arr: Array[String]): Boolean = {
            arr(2).startsWith("2018-01-17") && arr.length >= 26
        }


val raw = spark.read.textFile("/ge/warehouse/jxm/PlayerLogin/2018-01-17/*.gz")
    val activity = raw.map(line => line.split("\t")).filter(line => loginFilter(line)).map { line =>
      val net = line(6)
      (net)
    }.toDF("net_amt")

activity.show


###### stct
import vng.ge.stats.etl.utils.{Common, DateTimeUtils}

  val dateGmt7 = udf { (dateTime: String) => {
    val timeStamp = DateTimeUtils.getTimestamp(dateTime)+7*60*60*1000
    DateTimeUtils.getDate(timeStamp)
  }
  }

  val df = spark.read.json("/ge/gamelogs/sdk_sea/2018-01-{16,17}/Log_STCT_DBGAdd/Log_STCT_DBGAdd-2018-01-{16,17}.gz")
  df.withColumn("log_time",dateGmt7(col("updatetime"))).filter("log_time like '2018-01-17%' and resultCode = '1'").agg(sum("pmcNetChargeAmt").cast("long")).show

###### nikki thai
import vng.ge.stats.etl.utils.{Common, DateTimeUtils}

  val dateGmt7 = udf { (dateTime: String) => {
    val timeStamp = DateTimeUtils.getTimestamp(dateTime)+25200000L
    DateTimeUtils.getDate(timeStamp)
  }
  }
      val getGross = udf {(itemId: String, pmcId:String, gross:Long) => {

      val convertMapStore = Map(
 "com.pg2.nikkithai.diamond38"->69,
        "com.pg2.nikkithai.diamond90"->99,
        "com.pg2.nikkithai.diamond188"->139,
        "com.pg2.nikkithai.diamond377"->249,
        "com.pg2.nikkithai.diamond578"->349,
        "com.pg2.nikkithai.diamond968"->559,
        "com.pg2.nikkithai.diamond1968"->1100,
        "com.pg2.nikkithai.diamond4188"->2300,
        "com.pg2.nikkithai.gift"->35,
        "com.pg2.nikkithai.packchangename"->349,
        "com.pg2.nikkithai.packstarlight"->139,
        "com.pg2.nikkithai.packstamina"->139,
        "com.pg2.nikkithai.packlargestarlight"->349,
        "com.pg2.nikkithai.packlargestamina"->349,
        "com.pg2.nikkithai.giftdoubleseven"->35,
        "com.pg2.nikkithai.packluckstamina"->35,
        "com.pg2.nikkithai.packlucklargestamina"->69,
        "com.pg2.nikkithai.packluckcoin"->35,
        "com.pg2.nikkithai.packlucklargecoin"->69,
        "com.pg2.nikkithai.pack99a"->35,
        "com.pg2.nikkithai.pack99b"->35,
        "com.pg2.nikkithai.pack99c"->35,
        "com.pg2.nikkithai.pack99d"->35,
        "com.pg2.nikkithai.pack99e"->35,
        "com.pg2.nikkithai.pack99f"->35,
        "com.pg2.nikkithai.pack199a"->69,
        "com.pg2.nikkithai.pack199b"->69,
        "com.pg2.nikkithai.pack199c"->69,
        "com.pg2.nikkithai.pack199d"->69,
        "com.pg2.nikkithai.pack199e"->69,
        "com.pg2.nikkithai.pack199f"->69,
        "com.pg2.nikkithai.pack299a"->99,
        "com.pg2.nikkithai.pack299b"->99,
        "com.pg2.nikkithai.pack299c"->99,
        "com.pg2.nikkithai.pack299d"->99,
        "com.pg2.nikkithai.pack299e"->99,
        "com.pg2.nikkithai.pack299f"->99,
        "com.pg2.nikkithai.pack399a"->139,
        "com.pg2.nikkithai.pack399b"->139,
        "com.pg2.nikkithai.pack399c"->139,
        "com.pg2.nikkithai.pack399d"->139,
        "com.pg2.nikkithai.pack399e"->139,
        "com.pg2.nikkithai.pack399f"->139,
        "com.pg2.nikkithai.pack499a"->179,
        "com.pg2.nikkithai.pack499b"->179,
        "com.pg2.nikkithai.pack499c"->179,
        "com.pg2.nikkithai.pack499d"->179,
        "com.pg2.nikkithai.pack499e"->179,
        "com.pg2.nikkithai.pack499f"->179,
        "com.pg2.nikkithai.pack599a"->209,
        "com.pg2.nikkithai.pack599b"->209,
        "com.pg2.nikkithai.pack599c"->209,
        "com.pg2.nikkithai.pack599d"->209,
        "com.pg2.nikkithai.pack599e"->209,
        "com.pg2.nikkithai.pack599f"->209,
        "com.pg2.nikkithai.pack699a"->249,
        "com.pg2.nikkithai.pack699b"->249,
        "com.pg2.nikkithai.pack699c"->249,
        "com.pg2.nikkithai.pack699d"->249,
        "com.pg2.nikkithai.pack699e"->249,
        "com.pg2.nikkithai.pack699f"->249,
        "com.pg2.nikkithai.pack799a"->279,
        "com.pg2.nikkithai.pack799b"->279,
        "com.pg2.nikkithai.pack799c"->279,
        "com.pg2.nikkithai.pack999a"->349,
        "com.pg2.nikkithai.pack999b"->349,
        "com.pg2.nikkithai.pack999c"->349,
        "com.pg2.nikkithai.pack1099a"->389,
        "com.pg2.nikkithai.pack1099b"->389,
        "com.pg2.nikkithai.pack1099c"->389,
        "com.pg2.nikkithai.pack1599a"->559,
        "com.pg2.nikkithai.pack1599b"->559,
        "com.pg2.nikkithai.pack1599c"->559,
        "com.pg2.nikkithai.pack2099a"->739,
        "com.pg2.nikkithai.pack2099b"->739,
        "com.pg2.nikkithai.pack2099c"->739,
        "com.pg2.nikkithai.pack3099a"->1100,
        "com.pg2.nikkithai.pack3099b"->1100,
        "com.pg2.nikkithai.pack3099c"->1100,
        "com.pg2.nikkithai.pack99g"->35,
        "com.pg2.nikkithai.pack99h"->35,
        "com.pg2.nikkithai.pack99i"->35,
        "com.pg2.nikkithai.pack99j"->35,
        "com.pg2.nikkithai.pack99k"->35,
        "com.pg2.nikkithai.pack99l"->35,
        "com.pg2.nikkithai.pack99m"->35,
        "com.pg2.nikkithai.pack99n"->35,
        "com.pg2.nikkithai.pack99o"->35,
        "com.pg2.nikkithai.pack99p"->35,
        "com.pg2.nikkithai.pack199g"->69,
        "com.pg2.nikkithai.pack199h"->69,
        "com.pg2.nikkithai.pack199i"->69,
        "com.pg2.nikkithai.pack199j"->69,
        "com.pg2.nikkithai.pack199k"->69,
        "com.pg2.nikkithai.pack199l"->69,
        "com.pg2.nikkithai.pack199m"->69,
        "com.pg2.nikkithai.pack199n"->69,
        "com.pg2.nikkithai.pack199o"->69,
        "com.pg2.nikkithai.pack199p"->69,
        "com.pg2.nikkithai.pack299g"->99,
        "com.pg2.nikkithai.pack299h"->99,
        "com.pg2.nikkithai.pack299i"->99,
        "com.pg2.nikkithai.pack299j"->99,
        "com.pg2.nikkithai.pack299k"->99,
        "com.pg2.nikkithai.pack299l"->99,
        "com.pg2.nikkithai.pack299m"->99,
        "com.pg2.nikkithai.pack299n"->99,
        "com.pg2.nikkithai.pack299o"->99,
        "com.pg2.nikkithai.pack299p"->99,
        "com.pg2.nikkithai.pack399g"->139,
        "com.pg2.nikkithai.pack399h"->139,
        "com.pg2.nikkithai.pack399i"->139,
        "com.pg2.nikkithai.pack399j"->139,
        "com.pg2.nikkithai.pack399k"->139,
        "com.pg2.nikkithai.pack399l"->139,
        "com.pg2.nikkithai.pack399m"->139,
        "com.pg2.nikkithai.pack399n"->139,
        "com.pg2.nikkithai.pack399o"->139,
        "com.pg2.nikkithai.pack399p"->139,
        "com.pg2.nikkithai.pack99q"->35,
        "com.pg2.nikkithai.pack99r"->35,
        "com.pg2.nikkithai.pack99s"->35,
        "com.pg2.nikkithai.pack99t"->35,
        "com.pg2.nikkithai.pack99u"->35,
        "com.pg2.nikkithai.pack99v"->35,
        "com.pg2.nikkithai.pack99w"->35,
        "com.pg2.nikkithai.pack99x"->35,
        "com.pg2.nikkithai.pack99y"->35,
        "com.pg2.nikkithai.pack99z"->35,
        "com.pg2.nikkithai.pack199q"->69,
        "com.pg2.nikkithai.pack199r"->69,
        "com.pg2.nikkithai.pack199s"->69,
        "com.pg2.nikkithai.pack199t"->69,
        "com.pg2.nikkithai.pack199u"->69,
        "com.pg2.nikkithai.pack199v"->69,
        "com.pg2.nikkithai.pack199w"->69,
        "com.pg2.nikkithai.pack199x"->69,
        "com.pg2.nikkithai.pack199y"->69,
        "com.pg2.nikkithai.pack199z"->69
      )

      var grossRev = gross*650
          if (itemId != null && pmcId.toLowerCase.contains("store")) {
                    var vnd: Double = 0
                    var bac: Double = 0
                    if (convertMapStore.contains(itemId)) {
                      bac = convertMapStore(itemId)
                    }
                    vnd = bac * 650
                    grossRev=vnd.toLong
          }
        grossRev
      }

}
val dbgAddRaw = spark.read.json("/ge/gamelogs/sdk_sea/2018-01-{16,17}/Log_NIKKITHAI_DBGAdd-2018-01-{16,17}.gz").withColumn("updatetime",dateGmt7(col("updatetime"))).where("updatetime like '2018-01-17%' and resultCode ='1'")
val gameAddRaw = spark.read.json("/ge/gamelogs/sdk_sea/2018-01-{16,17}/Log_NIKKITHAI_GameAdd-2018-01-{16,17}.gz").withColumn("updatetime",dateGmt7(col("updatetime"))).where("updatetime like '2018-01-17%' and resultCode ='1'")
      var df = dbgAddRaw.as('a).join(gameAddRaw.as('b), dbgAddRaw("transactionID") === gameAddRaw("transactionID"), "left_outer").where("a.resultCode == '1'")
        .select("a.updatetime", "a.gameID", "a.userID", "a.transactionID", "a.pmcID", "pmcNetChargeAmt", "pmcGrossChargeAmt", "a.roleID", "b.itemID")
val dv = df
println("net_rev")
dbgAddRaw.agg(sum("pmcNetChargeAmt").cast("long")*650).show
println("Gross_rev")
df.withColumn("grossRev",getGross(col("b.itemId"),col("a.pmcId"),col("pmcGrossChargeAmt"))).agg(sum("grossRev").cast("long")).show

#####  NIKKISEA
  import org.apache.spark.rdd.RDD
  import vng.ge.stats.etl.transform.udf.MyUdf.dateTimeIncrement
  import vng.ge.stats.etl.constant.Constants
  val sf = Constants.FIELD_NAME
  var rs = spark.read.textFile("/ge/gamelogs/nikkisea/201801{14,15}/log_recharge/Nikki_Logcharge_2018-01-{16,17}.csv.gz")
  val paymentDs = rs.map(line => line.split("\",\"")).map { line =>

            val money = line(5)
            val dateTime = line(11)
            val netRev = money
            val trans = line(2)
            ("nikkisea", dateTime, netRev,trans)
        }.toDF(sf.GAME_CODE, sf.LOG_DATE, sf.NET_AMT, sf.TRANS_ID)
  val logDateWithTimeZone = udf { (datetime: String, timezone: String) => {
    val time = dateTimeIncrement(datetime, -3600)
    time
  }
  }

  var rsmore = spark.read.textFile("/ge/gamelogs/nikkisea/20180117/chargelog_hourly/Nikki_Logcharge_2018-01-17-01.csv")

  val paymentDsMore = rsmore.map(line => line.split("\",\"")).map { line =>

            val money = line(5)
            val dateTime = line(11)
            val netRev = money
            val trans = line(2)
            ("nikkisea", dateTime, netRev,trans)
        }.toDF(sf.GAME_CODE, sf.LOG_DATE, sf.NET_AMT, sf.TRANS_ID)

   val join = paymentDs.union(paymentDsMore)
 join.withColumn("log_date",logDateWithTimeZone(col("log_date"),lit("0"))).where("log_date like '%2018-01-17%'").agg(sum("net_amt").cast("long")).show

###### nikkisea sdk sea
import vng.ge.stats.etl.utils.{Common, DateTimeUtils}

  val dateGmt7 = udf { (dateTime: String) => {
    val timeStamp = DateTimeUtils.getTimestamp(dateTime)+7*60*60*1000
    DateTimeUtils.getDate(timeStamp)
  }
  }

  val df = spark.read.json("/ge/gamelogs/sdk_sea/2018-01-{16,17}/Log_NIKKISEA_DBGAdd/Log_NIKKISEA_DBGAdd-2018-01-{16,17}.gz")
  df.withColumn("log_time",dateGmt7(col("updatetime"))).filter("log_time like '2018-01-17%' and resultCode = '1'").agg(sum("pmcNetChargeAmt").cast("long")).show

  df.filter(resultCode = '1'").agg(sum("pmcNetChargeAmt").cast("long")).show


############### nikki
  spark.read.option("delimiter","\t").csv("/ge/warehouse/nikki/recharge/2018-01-17/part-v000-o000-r-00000.gz").agg(sum("_c5").cast("long")).show

######## fishot - hc
select sum(net_revenue) from fishot.recharge where ds='2018-01-17';

#####CK - Ingame
 hdfs dfs -text /ge/warehouse/ck/recharge/2017-08-22/part-v000-o000-r-00000.gz | awk -F"\t" '{sum+=$8} END {printf "%.f\n",sum}'
spark.read.option("delimiter","\t").csv("/ge/warehouse/ck/recharge/2017-08-22/part-v000-o000-r-00000.gz").agg(sum("_c7").cast("long")).show

##### - ZTM
hdfs dfs -text /ge/gamelogs/ztm/20180117/logs/*/scenetlog* | grep -i RechargeFlow | awk -F"|" '{sum+=$15} END {printf "%.f\n",sum*20}'

### - ZTM
spark.read.option("delimiter","|").csv("/ge/gamelogs/ztm/20180117/logs/*/scenetlog*").where("_c0 like 'RechargeFlow'").agg((sum("_c14")*20).cast("long")).show

ty gia:  20

### dttk

spark.read.option("delimiter","\t").csv("/ge/warehouse/dttk/paying/2018-01-17/part-m-00000.gz").where("_c18 = '1'").agg(sum("_c10").cast("long")).show


#### ddd2mp2 - hcatalog
select sum(zing_xu) from ddd2mp2.recharge where ds='2018-01-17';
ddd2
### coccgsn - ingame
spark.read.option("delimiter","\t").csv("/ge/warehouse/coccgsn/recharge/2018-01-17/part-v000-o000-r-00000.gz").agg(sum("_c11").cast("long")).show

### vcth - ingame
spark.read.option("delimiter","\t").csv("/ge/warehouse/vcth/recharge/2018-01-17/part-v000-o000-r-00000.gz").agg(sum("_c6").cast("long")*100).show

### gnm - hcatalog
select sum(money)*100 from gnm.recharge where ds = '2018-01-17'

### ntgh -ingame
hdfs dfs -text /ge/gamelogs/vhw/2018-01-17/*/2018-01-17_RECHARGE_LOG.log.gz | awk -F" " '{sum+=$12} END {printf "%.f\n",sum*2.5*100}'

### ntgh -ingame
  val convertDate = udf { (timeStamp: Long) => {
    DateTimeUtils.getDate(timeStamp*1000)
  }
  }


spark.read.option("delimiter"," ").csv("/ge/gamelogs/vhw/2018-01-17/*/2018-01-17_RECHARGE_LOG.log.gz").withColumn("logdate",convertDate(col("_c12"))).where("logdate like '2018-01-17%' and _c1 not like '9997' ").agg((sum("_c11")*2.5*100).cast("long")).show

###### SIAMPLAY
import vng.ge.stats.etl.utils.{Common, DateTimeUtils}

  val dateGmt7 = udf { (dateTime: String) => {
    val timeStamp = DateTimeUtils.getTimestamp(dateTime)+7*60*60*1000
    DateTimeUtils.getDate(timeStamp)
  }
  }
spark.read.json("/ge/gamelogs/sdk_sea/2018-01-{16,17}/Log_SIAMPLAY_DBGAdd/Log_SIAMPLAY_DBGAdd-2018-01-{16,17}.gz").withColumn("updatetime",dateGmt7(col("updatetime"))).where("updatetime like '2018-01-17%' ").agg(sum("pmcNetChargeAmt").cast("long")*638).show

spark.read.json("/ge/gamelogs/sdk_sea/2018-01-{16,17}/Log_SIAMPLAY_DBGAdd/Log_SIAMPLAY_DBGAdd-2018-01-{16,17}.gz").where("updatetime like '2018-01-17%' ").agg(sum("pmcNetChargeAmt").cast("long")*638).show


### coccmgsn - ingame
52001 - gross rev
spark.read.option("delimiter","\t").csv("/ge/warehouse/coccm/recharge/2017-08-{26,27}/part-v000-o000-r-00000.gz").where("_c3 like '2017-08-27%'").agg(sum("_c11").cast("long")).show

16001 - net rev
spark.read.option("delimiter","\t").csv("/ge/warehouse/coccm/recharge/2017-08-{22,23}/part-v000-o000-r-00000.gz").where("_c3 like '2018-01-17%'").agg(sum("_c10").cast("long")).show

##### tftb2s

spark.read.option("delimiter","\t").csv("/ge/gamelogs/tftb2s/2018-01-17/Paying/Paying*.gz").where("_c9 not like '0'").agg(sum("_c10").cast("long")).show

52001
spark.read.option("delimiter","\t").csv("/ge/gamelogs/tftb2s/2018-01-17/Paying/Paying*.gz").where("_c9 not like '0'").agg(sum("_c9").cast("long")).show

###### 3qmobile
spark.read.option("delimiter","\t").csv("/ge/warehouse/3qmobile/recharge/2018-01-17/*.gz").agg(sum("_c10").cast("long")).show

######
cffbs
spark.read.option("delimiter",",").csv("/ge/gamelogs/cube/2018-01-17/IFRS/IFRS.gz").agg(sum("_c11").cast("long")).show

##### hctq

spark.read.option("header","true").option("delimiter",",").csv("/ge/gamelogs/tqvs/20180117/log/*/*_t_u_pay_order_20180117.csv").where("PaySuccess == 1 and CurrencyType not like 'PW_INTERNAL_PAY' ").agg(sum("Points").cast("long")*100).show


##### NLMB - Hcatalog
select sum(gold)*100 from nlmb.recharge where ds='2018-01-17';


##### TNUH
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.DataFrame
import vng.ge.stats.etl.constant.Constants
import vng.ge.stats.etl.transform.adapter.base.{FairyFormatter}
import vng.ge.stats.etl.utils.{DateTimeUtils, PathUtils}
import com.fasterxml.jackson.databind.ObjectMapper
import com.fasterxml.jackson.module.scala.DefaultScalaModule
import com.fasterxml.jackson.module.scala.experimental.ScalaObjectMapper

    val pattern = "/ge/gamelogs/tnuh/20180117/datalog/l10vn-*"
    val raw = spark.read.textFile(pattern)


    val filterlog = (line:String) => {
      var rs = false
      if(line.length>0 && line.contains(",")){
        val str = line.substring(0,line.indexOf(",")).toLowerCase
        if (str.contains("2018-01-17") && (str.contains("prepaid") )){
          rs = true
        }
      }
      rs
    }
    val payment = raw.filter(line => filterlog(line)).map { line =>
      val str = line.substring(line.indexOf(",")+1,line.length)
      val mapper = new ObjectMapper() with ScalaObjectMapper
      mapper.registerModule(DefaultScalaModule)
      val obj = mapper.readValue[Map[String, Object]](str)
      val net = (obj("jade").toString.toDouble*250).toString
      val devic = (obj("jade").toString.toDouble*250).toString
      val device_type = obj("device_type").toString



      ("tnuh",net,device_type)
    }.toDF("gameCode","net","device_type")


    payment.agg(sum("net").cast("long")).show

##### Aumobile
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.DataFrame
import vng.ge.stats.etl.constant.Constants
import vng.ge.stats.etl.transform.adapter.base.{FairyFormatter}
import vng.ge.stats.etl.utils.{DateTimeUtils, PathUtils}
import com.fasterxml.jackson.databind.ObjectMapper
import com.fasterxml.jackson.module.scala.DefaultScalaModule
import com.fasterxml.jackson.module.scala.experimental.ScalaObjectMapper

    val pattern = "/ge/gamelogs/aumobile/20180117/datalog/livelog_20180117/*.log"
    val raw = spark.read.textFile(pattern)


    val filterlog = (line:String) => {
      var rs = false
      if(line.length>0 && line.contains(",")){
        val str = line.substring(0,line.indexOf(",")).toLowerCase
        if (str.contains("2018-01-17") && (str.contains("delivery") )){
          rs = true
        }
      }
      rs
    }
    val payment = raw.filter(line => filterlog(line)).map { line =>
      val str = line.substring(line.indexOf(",")+1,line.length)
      val mapper = new ObjectMapper() with ScalaObjectMapper
      mapper.registerModule(DefaultScalaModule)
      val datetime =line.substring(line.indexOf("[")+1,line.indexOf("]"))
      val obj = mapper.readValue[Map[String, Object]](str)
      val net = (obj("cash")).toString
      val transactionId =  obj("sn").toString
      val id = obj("account_id").toString
      val sid = obj("server").toString
      (datetime,id,net,sid,transactionId)
    }.toDF("datetime","id","net","sid","transactionId")
    payment.agg(sum("net").cast("long")).show


###### cubefarm_global
import vng.ge.stats.etl.utils.{Common, DateTimeUtils}

  val dateGmt7 = udf { (dateTime: String) => {
    val timeStamp = DateTimeUtils.getTimestamp(dateTime)+25200000L
    DateTimeUtils.getDate(timeStamp)
  }
  }
     val df = spark.read.option("delimiter","\t").csv("/ge/warehouse/cfgfbs1/paying/2018-01-{16,17}/part-v000-o000-r-00000.gz")

  df.withColumn("datetime",dateGmt7(col("_c0"))).where("datetime like '2018-01-17%'").agg(sum("_c10").cast("long")).show

########### dptk
spark.read.option("delimiter","\t").csv("/ge/warehouse/dppg3/recharge/2018-01-17").agg(sum("_c10").cast("long")).show



##########Tvc - ingame
 spark.read.option("delimiter",",").csv("/ge/gamelogs/tvc/20180117/*/t_user_pay*").agg(sum("_c6").cast("long")).show(false)

 ############# HKH
 spark.read.option("delimiter","\t").csv("/ge/dragon/warehouse/hkh/recharge/2018-01-17/part-r-00000.gz").agg(sum("_c7").cast("long")).show

###### omg2
import vng.ge.stats.etl.utils.{Common, DateTimeUtils}

  val convertDate = udf { (timeStamp: Long) => {
    DateTimeUtils.getDate(timeStamp*1000)
  }
  }

val df = spark.read.option("delimiter","|").csv("/ge/gamelogs/omg2/20180204/datalog/web/*/stat.log.*").where("_c0 like '%Charge%' and _c7 like '4'")

df.withColumn("_c5",convertDate(col("_c5"))).where("_c5 like '%2018-02-04%'").agg(sum("_c11").cast("long")*250).show

######## sglfbs

spark.read.option("delimiter","\t").csv("/ge/gamelogs/sglfbs/2018-01-17/paying/paying-2018-01-17").agg(sum("_c7").cast("long")).show

######## izfbs2
import vng.ge.stats.etl.utils.{Common, DateTimeUtils}

  val converDate = udf { (dateTime: String) => {
    val timeStamp = DateTimeUtils.getTimestamp(dateTime)+39600000
    DateTimeUtils.getDate(timeStamp)
  }
  }

val df = spark.read.option("delimiter","\t").csv("/ge/gamelogs/izfbs2/2018-01-{16,17}/user_paying/user_paying-2018-01-{16,17}")

df.withColumn("log_date",converDate(col("_c1"))).where("log_date like '%2018-01-17%'").agg(sum("_c7").cast("long")).show

####### H5SS
select sum(money)from h5ss.recharge where ds like '%2018-01-17%'and state == 2

###### SIAMPLAYINDO
import vng.ge.stats.etl.utils.{Common, DateTimeUtils}

  val dateGmt7 = udf { (dateTime: String) => {
    val timeStamp = DateTimeUtils.getTimestamp(dateTime)+7*60*60*1000
    DateTimeUtils.getDate(timeStamp)
  }
  }

  val df = spark.read.json("/ge/gamelogs/sdk_sea/2018-01-{16,17}/Log_SIAMPLAYINDO_DBGAdd/Log_SIAMPLAYINDO_DBGAdd-2018-01-{16,17}.gz")
  df.withColumn("log_time",dateGmt7(col("updatetime"))).filter("log_time like '2018-01-17%' and resultCode = '1'").agg((sum("pmcNetChargeAmt")*1.724).cast("long")).show


######## icamfbs2
56001
spark.read.option("delimiter","\t").csv("/ge/warehouse/ica/ingame_recharge/2018-01-17/part-v000-o000-r-00000.gz").where("_c18='1' and _c0 like '%2018-01-17%' and _c21 is not null").agg(sum("_c9").cast("long")).show

16001
spark.read.option("delimiter","\t").csv("/ge/warehouse/ica/ingame_recharge/2018-01-17/part-v000-o000-r-00000.gz").where("_c18='1' and _c0 like '%2018-01-17%' and _c21 is not null").agg(sum("_c10").cast("long")).show

######## 360live



     val getMoney = udf { (money: String,currency:String) => {
      var vnd = 0.0
          val convertMap = Map(
            "AUD" -> 16543.0,
            "VND" -> 1.0,
            "EUR" -> 24585.87,
            "USD" -> 22700.0,
            "GBP" -> 29099.24,
            "HKD" -> 2878.57,
            "JPY" -> 196.87,
            "KRW" -> 18.49,
            "SGD" -> 15925.7,
            "THB" -> 641.5,
            "CAD" -> 16336.68,
            "CHF" -> 22284.26
          )

      if (convertMap.contains(currency)) {
        vnd = money.toDouble * convertMap(currency)
      }
      vnd
       }
       }


  val df = spark.read.option("delimiter","\t").csv("/ge/gamelogs/360live/2018-01-17/360live_payment_stats/360live_payment_stats-2018-01-17.gz").where("_c9 != 'GMTOOL'")
   df.withColumn("net_amt",getMoney(col("_c4"),col("_c5"))).agg(sum("net_amt").cast("long")).show


############ KVM - ingame Log ngày hôm trước nằm trong thư mục ngày hôm sau

val df = spark.read.option("delimiter","\t").option("header","true").csv("/ge/gamelogs/kvm/2018-01-17/datalog/LOG_*/recharge.txt")
df.withColumn("log_date",date_format(col("pay_time").cast("double").cast("timestamp"),"yyyy-MM-dd HH:mm:ss")).where("log_date like '2018-01-17%' and is_succ='1'").agg(sum("money").cast("long")).show

############ PNTT
spark.read.option("delimiter","\t").csv("/ge/gamelogs/pnttmobi/2018-01-17/datalog/gamelog_20180117/log_recharge/log_recharge*").agg(sum("_c10").cast("long")).show

############ h5ss
check A1
select count(distinct id) from h5ss.`user` where ds like '2018-01-17%' and logintime like '%2018-01-17%'

###### SIAMPLAY INDO
import vng.ge.stats.etl.utils.{Common, DateTimeUtils}

  val dateGmt7 = udf { (dateTime: String) => {
    val timeStamp = DateTimeUtils.getTimestamp(dateTime)+7*60*60*1000
    DateTimeUtils.getDate(timeStamp)
  }
  }


spark.read.json("/ge/gamelogs/sdk_sea/2018-01-{16,17}/Log_SIAMPLAYINDO_DBGAdd/Log_SIAMPLAYINDO_DBGAdd-2018-01-{16,17}.gz").withColumn("updatetime",dateGmt7(col("updatetime"))).where("updatetime like '2018-01-17%' ").agg((sum("pmcNetChargeAmt")*1.724).cast("long")).show

###### TDCL
spark.read.option("delimiter","\t").option("header","true").csv("/ge/gamelogs/tdcl/2018-01-17/datalog/loggame_20180117/*/recharge.csv").agg((sum("gold")*100).cast("long")).show

###### TTTD
spark.read.option("delimiter","\t").csv("/ge/warehouse/tttd/recharge/2018-01-17/part-v000-o000-r-00000.gz").agg(sum("_c5").cast("long")).show

######## MUW
spark.read.option("delimiter",",").csv("/ge/gamelogs/muw/2018-01-17/recharge_s*.csv").where("_c12 !='9999999999'").agg((sum("_c7")*200).cast("long")).show

######## Kiem tra payment trong thang

spark.read.parquet("/ge/warehouse/tlbbm/ub/data/payment_2/2017-11-*/").withColumn("log_date_cal",substring(col("log_date"),0,13)).withColumn("log_date_to",substring(col("log_date"),0,10)).groupBy("log_date_to").agg(countDistinct("log_date_cal")).distinct.orderBy("log_date_to").show(1000)




spark.read.parquet("/ge/fairy/warehouse/sdk/monthly/game_add/2017-11").withColumn("log_date_cal",substring(col("log_date"),0,10)).where("game_id = 'tlbbm' and game_result='1' and log_date like '2017-11-01%'").select("server_id").distinct.show


####check tlbb server
val a = spark.read.parquet("/ge/fairy/warehouse/sdk/monthly/game_add/2017-11").withColumn("log_date_ca",substring(col("log_date"),0,10)).where("game_id = 'tlbbm' and game_result='1' and log_date like '2017-11%'").select("log_date_ca","server_id").distinct.orderBy("log_date_ca","server_id")
a: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [log_date_ca: string, server_id: string]

scala> val b = spark.read.option("delimiter","\t").csv("/ge/warehouse/tlbbm/recharge/2017-11-*/*.gz").withColumn("log_date_cal",substring(col("_c0"),0,10)).select("log_date_cal","_c6").distinct.orderBy("log_date_cal","_c6")
b: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [log_date_cal: string, _c6: string]

scala> val join = a.join(b,a("server_id")===b("_c6") and a("log_date_ca")===b("log_date_cal"),"left_outer").where("_c6 is null").coalesce(1).write.mode("overwrite").format("csv").option("header", "true").save(s"tmp/tlbbm-crosscheck.csv")

#Tdcl
spark.read.option("delimiter","\t").option("header","true").csv("/ge/gamelogs/tdcl/2018-01-17/datalog/loggame_20180117/*/recharge.csv").agg((sum("gold")*100).cast("long")).show

## phuclong
spark.read.option("delimiter","\t").csv("/ge/gamelogs/nh/20180117/*/recharge.txt").agg((sum("_c3")*100).cast("long")).show

### ygh5
    val setAmtWithRate = udf { (amt: String) => {
      val convertMap = Map(
        1 -> 20000,
        2 -> 40000,
        3 -> 60000,
        4 -> 100000,
        5 -> 200000,
        6 -> 500000,
        201 -> 20000,
        202 -> 50000,
        203 -> 100000,
        101-> 50000,
        102-> 200000,
        103-> 200000
      )

      val vnd = convertMap(amt.toInt)
      vnd.toString
    }
    }

var df = spark.read.option("delimiter",",").csv("/ge/gamelogs/ygh5/20180117/Log/daily/money_tb_iap_20180117.csv").where("_c5 = '1' and _c8 != '1000' and _c1 like '2018-01-17%'")
df=df.withColumn("net_amt",setAmtWithRate(col("_c9")))
df.agg(sum("net_amt").cast("long")).show


###### cgmbgfbs1
import vng.ge.stats.etl.utils.{Common, DateTimeUtils}

  val dateGmt7 = udf { (dateTime: String) => {
    val timeStamp = DateTimeUtils.getTimestamp(dateTime)+25200000L
    DateTimeUtils.getDate(timeStamp)
  }
  }

spark.read.option("delimiter","\t").csv("/ge/warehouse/cgmbgfbs1/payment/2018-01-17/part-v000-o000-r-00000.gz").withColumn("updatetime",dateGmt7(col("_c0"))).where("updatetime like '2018-01-17%'").agg(sum("_c7").cast("long")).show

### DTM

import org.apache.spark.rdd.RDD
import org.apache.spark.sql.DataFrame
import vng.ge.stats.etl.constant.Constants
import vng.ge.stats.etl.transform.adapter.base.{FairyFormatter}
import vng.ge.stats.etl.utils.{DateTimeUtils, PathUtils}
import com.fasterxml.jackson.databind.ObjectMapper
import com.fasterxml.jackson.module.scala.DefaultScalaModule
import com.fasterxml.jackson.module.scala.experimental.ScalaObjectMapper

    val pattern = "/ge/gamelogs/dtm/20180117/datalog/gamelog_20180117/*/*/*.log"
    val raw = spark.read.textFile(pattern)


    val filterlog = (line:String) => {
      var rs = false
      if(line.length>0 && line.contains(",")){
        val str = line.substring(0,line.indexOf(",")).toLowerCase
        if (str.contains("2018-01-17") && (str.contains("prepaid") )){
          rs = true
        }
      }
      rs
    }
    val payment = raw.filter(line => filterlog(line)).map { line =>
      val str = line.substring(line.indexOf(",")+1,line.length)
      val mapper = new ObjectMapper() with ScalaObjectMapper
      mapper.registerModule(DefaultScalaModule)
      val datetime =line.substring(line.indexOf("[")+1,line.indexOf("]"))
      val obj = mapper.readValue[Map[String, Object]](str)
      val net = (obj("cash")).toString
      val transactionId =  obj("sn").toString
      val id = obj("account_id").toString
      val sid = obj("server").toString
      (datetime,id,net,sid,transactionId)
    }.toDF("datetime","id","net","sid","transactionId")
    payment.agg(sum("net").cast("long")).show

######### dmn360mobi


  val setAmt = udf { (s: String) => {
       val convertMapStore = Map(
          "com.vng.dmzvn.item20"->2000000,
          "com.vng.dmzvn.item50"->5000000,
          "com.vng.dmzvn.item100"->10000000,
          "com.vng.dmzvn.item200"->20000000,
          "com.vng.dmzvn.item500"->50000000,
          "com.vng.dmzvn.item1000"->100000000,
          "com.vng.dmzvn.itemvip"->10000000
        )
        val vnd = convertMapStore(s)/100
    vnd.toString()
  }
  }
spark.read.option("delimiter",",").csv("/ge/gamelogs/dmn360mobi/20180117/loggame/logserver/*/tab_pay_20180117*.log").withColumn("net_amt",setAmt(col("_c8"))).agg(sum("net_amt").cast("long")).show

spark.read.option("delimiter",",").csv("/ge/gamelogs/dmn360mobi/20180117/loggame/logserver/*/tab_pay_20180117*.log").agg((sum("_c13")/100).cast("long")).show


###### Excel
val df = spark.read.format("com.crealytics.spark.excel").option("sheetName", "payment").option("useHeader", "true").load("/user/fairy/tmp/testData.xlsx")

###### VTL
spark.read.json("/ge/gamelogs/vtl/2018-01-17/datalog/*normal_recharge_2018-01-17.log").agg((sum("addCoin")*100).cast("long")).show