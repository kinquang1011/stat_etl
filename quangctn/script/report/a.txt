

Container: container_e81_1494395298335_303847_02_000003 on c402.hadoop.gda.lo_45454
=====================================================================================
LogType:directory.info
Log Upload Time:Tue May 16 11:12:19 +0700 2017
LogLength:28872
Log Contents:
ls -l:
total 112
lrwxrwxrwx 1 yarn hadoop    69 May 16 11:11 __app__.jar -> /hadoop/yarn/local/usercache/fairy/filecache/33842/stats-etlr-1.0.jar
-rw-r--r-- 1 yarn hadoop    87 May 16 11:11 container_tokens
-rwx------ 1 yarn hadoop   676 May 16 11:11 default_container_executor_session.sh
-rwx------ 1 yarn hadoop   730 May 16 11:11 default_container_executor.sh
-rwx------ 1 yarn hadoop 75972 May 16 11:11 launch_container.sh
lrwxrwxrwx 1 yarn hadoop    69 May 16 11:11 __spark_conf__ -> /hadoop/yarn/local/usercache/fairy/filecache/33843/__spark_conf__.zip
drwxr-xr-x 2 yarn hadoop 12288 May 16 11:11 __spark_libs__
drwx--x--- 2 yarn hadoop  4096 May 16 11:11 tmp
find -L . -maxdepth 5 -ls:
8127777    4 drwx--x---   4 yarn     hadoop       4096 May 16 11:11 .
8127780    4 -rw-r--r--   1 yarn     hadoop         12 May 16 11:11 ./.container_tokens.crc
8127783    4 -rwx------   1 yarn     hadoop        676 May 16 11:11 ./default_container_executor_session.sh
8127781   76 -rwx------   1 yarn     hadoop      75972 May 16 11:11 ./launch_container.sh
8127779    4 -rw-r--r--   1 yarn     hadoop         87 May 16 11:11 ./container_tokens
8127784    4 -rw-r--r--   1 yarn     hadoop         16 May 16 11:11 ./.default_container_executor_session.sh.crc
8127786    4 -rw-r--r--   1 yarn     hadoop         16 May 16 11:11 ./.default_container_executor.sh.crc
8127787   12 drwxr-xr-x   2 yarn     hadoop      12288 May 16 11:11 ./__spark_libs__
17171817 2780 -rwxr-xr-x   1 yarn     hadoop    2842667 Mar 29 19:40 ./__spark_libs__/bcprov-jdk15on-1.51.jar
16910592  224 -rwxr-xr-x   1 yarn     hadoop     227712 Apr  5 13:33 ./__spark_libs__/libthrift-0.9.2.jar
16515644   24 -rwxr-xr-x   1 yarn     hadoop      23346 Apr  4 14:45 ./__spark_libs__/stax-api-1.0-2.jar
16516250   80 -rwxr-xr-x   1 yarn     hadoop      79912 Apr  4 14:45 ./__spark_libs__/api-util-1.0.0-M20.jar
16910721    8 -rwxr-xr-x   1 yarn     hadoop       5950 Apr  5 13:33 ./__spark_libs__/javax.inject-2.4.0-b34.jar
17172813  280 -rwxr-xr-x   1 yarn     hadoop     284220 Mar 29 19:40 ./__spark_libs__/commons-lang-2.6.jar
17046772  256 -rwxr-xr-x   1 yarn     hadoop     258876 Apr  5 13:33 ./__spark_libs__/jackson-core-2.6.5.jar
16910724 1444 -rwxr-xr-x   1 yarn     hadoop    1475955 Apr  5 13:33 ./__spark_libs__/htrace-core-3.1.0-incubating.jar
16516253 2304 -rwxr-xr-x   1 yarn     hadoop    2355465 Apr  4 14:45 ./__spark_libs__/spark-network-common_2.11-2.0.1.jar
17172574  164 -rwxr-xr-x   1 yarn     hadoop     164422 Mar 29 19:40 ./__spark_libs__/core-1.1.2.jar
16515658  380 -rwxr-xr-x   1 yarn     hadoop     387188 Apr  4 14:45 ./__spark_libs__/parquet-format-2.3.0-incubating.jar
16515794   20 -rwxr-xr-x   1 yarn     hadoop      18336 Apr  4 14:45 ./__spark_libs__/jackson-jaxrs-1.9.13.jar
16515159  200 -rwxr-xr-x   1 yarn     hadoop     201124 Apr  4 15:15 ./__spark_libs__/jdo-api-3.0.1.jar
16516243 6144 -rwxr-xr-x   1 yarn     hadoop    6290315 Apr  4 14:45 ./__spark_libs__/spark-sql_2.11-2.0.1.jar
17046790   64 -rwxr-xr-x   1 yarn     hadoop      63316 Apr  5 13:33 ./__spark_libs__/spark-repl_2.11-2.0.1.jar
17172670   20 -rwxr-xr-x   1 yarn     hadoop      17008 Mar 29 19:40 ./__spark_libs__/base64-2.3.8.jar
17172687  436 -rwxr-xr-x   1 yarn     hadoop     442406 Mar 29 19:40 ./__spark_libs__/calcite-linq4j-1.2.0-incubating.jar
17172811   16 -rwxr-xr-x   1 yarn     hadoop      14766 Mar 29 19:40 ./__spark_libs__/aopalliance-repackaged-2.4.0-b34.jar
16516327  172 -rwxr-xr-x   1 yarn     hadoop     174351 Apr  4 14:45 ./__spark_libs__/stream-2.7.0.jar
17172723  524 -rwxr-xr-x   1 yarn     hadoop     533455 Mar 29 19:40 ./__spark_libs__/protobuf-java-2.5.0.jar
16516273 1144 -rwxr-xr-x   1 yarn     hadoop    1171380 Apr  4 14:45 ./__spark_libs__/jackson-databind-2.6.5.jar
17046968   32 -rwxr-xr-x   1 yarn     hadoop      29555 Apr  5 13:33 ./__spark_libs__/paranamer-2.3.jar
16910717   96 -rwxr-xr-x   1 yarn     hadoop      95806 Apr  5 13:33 ./__spark_libs__/javax.servlet-api-3.1.0.jar
17172564 1996 -rwxr-xr-x   1 yarn     hadoop    2041628 Mar 29 19:40 ./__spark_libs__/jets3t-0.9.3.jar
17172704 4468 -rwxr-xr-x   1 yarn     hadoop    4573750 Mar 29 19:40 ./__spark_libs__/scala-reflect-2.11.8.jar
17171510  104 -rwxr-xr-x   1 yarn     hadoop     105134 Mar 29 19:40 ./__spark_libs__/jaxb-api-2.2.2.jar
16516343  164 -rwxr-xr-x   1 yarn     hadoop     167421 Apr  4 14:45 ./__spark_libs__/jersey-client-2.22.2.jar
16515778  896 -rwxr-xr-x   1 yarn     hadoop     917052 Apr  4 14:45 ./__spark_libs__/parquet-column-1.7.0.jar
17172692 5612 -rwxr-xr-x   1 yarn     hadoop    5744974 Mar 29 19:40 ./__spark_libs__/scala-library-2.11.8.jar
17047135  144 -rwxr-xr-x   1 yarn     hadoop     144660 Apr  5 13:33 ./__spark_libs__/xbean-asm5-shaded-4.4.jar
16516297 1024 -rwxr-xr-x   1 yarn     hadoop    1044511 Apr  4 14:45 ./__spark_libs__/spark-hive_2.11-2.0.1.jar
17172662 1024 -rwxr-xr-x   1 yarn     hadoop    1045744 Mar 29 19:40 ./__spark_libs__/leveldbjni-all-1.8.jar
16515686  356 -rwxr-xr-x   1 yarn     hadoop     363908 Apr  4 14:45 ./__spark_libs__/hadoop-yarn-server-common-2.7.1.jar
16910745 15128 -rwxr-xr-x   1 yarn     hadoop   15487351 Apr  5 13:33 ./__spark_libs__/scala-compiler-2.11.8.jar
17172733   68 -rwxr-xr-x   1 yarn     hadoop      68866 Mar 29 19:40 ./__spark_libs__/curator-client-2.6.0.jar
17046870 1988 -rwxr-xr-x   1 yarn     hadoop    2035066 Apr  5 13:33 ./__spark_libs__/commons-math3-3.4.1.jar
17172617  764 -rwxr-xr-x   1 yarn     hadoop     780664 Mar 29 19:40 ./__spark_libs__/jackson-mapper-asl-1.9.13.jar
16910703  636 -rwxr-xr-x   1 yarn     hadoop     648678 Apr  5 13:33 ./__spark_libs__/scala-xml_2.11-1.0.2.jar
16516272   68 -rwxr-xr-x   1 yarn     hadoop      69409 Apr  4 14:45 ./__spark_libs__/activation-1.1.1.jar
17172676  180 -rwxr-xr-x   1 yarn     hadoop     181271 Mar 29 19:40 ./__spark_libs__/hk2-locator-2.4.0-b34.jar
16515693  576 -rwxr-xr-x   1 yarn     hadoop     588337 Apr  4 14:45 ./__spark_libs__/commons-collections-3.2.2.jar
16516044   40 -rwxr-xr-x   1 yarn     hadoop      38134 Apr  4 14:45 ./__spark_libs__/hadoop-mapreduce-client-jobclient-2.7.1.jar
16515663   12 -rwxr-xr-x   1 yarn     hadoop      12131 Apr  4 14:45 ./__spark_libs__/jpam-1.1.jar
17172581  228 -rwxr-xr-x   1 yarn     hadoop     232248 Mar 29 19:40 ./__spark_libs__/jackson-core-asl-1.9.13.jar
16515402  184 -rwxr-xr-x   1 yarn     hadoop     185245 Apr  4 14:45 ./__spark_libs__/curator-framework-2.6.0.jar
16515757   24 -rwxr-xr-x   1 yarn     hadoop      20852 Apr  4 14:45 ./__spark_libs__/metrics-graphite-3.1.2.jar
16910628  176 -rwxr-xr-x   1 yarn     hadoop     177131 Apr  5 13:33 ./__spark_libs__/jetty-util-6.1.26.jar
16515911  504 -rwxr-xr-x   1 yarn     hadoop     515604 Apr  4 14:45 ./__spark_libs__/jackson-module-scala_2.11-2.6.5.jar
16910586   48 -rwxr-xr-x   1 yarn     hadoop      46983 Apr  5 13:33 ./__spark_libs__/jackson-annotations-2.6.5.jar
16515638   48 -rwxr-xr-x   1 yarn     hadoop      48720 Apr  4 14:45 ./__spark_libs__/snappy-0.2.jar
16515895   84 -rwxr-xr-x   1 yarn     hadoop      82421 Apr  4 14:45 ./__spark_libs__/json4s-ast_2.11-3.2.11.jar
17172568  160 -rwxr-xr-x   1 yarn     hadoop     160519 Mar 29 19:40 ./__spark_libs__/commons-dbcp-1.4.jar
17171915   88 -rwxr-xr-x   1 yarn     hadoop      86811 Mar 29 19:40 ./__spark_libs__/spire-macros_2.11-0.7.4.jar
16515802   28 -rwxr-xr-x   1 yarn     hadoop      27084 Apr  4 14:45 ./__spark_libs__/jackson-xc-1.9.13.jar
17171944 1248 -rwxr-xr-x   1 yarn     hadoop    1277883 Mar 29 19:40 ./__spark_libs__/mesos-0.21.1-shaded-protobuf.jar
16515640  952 -rwxr-xr-x   1 yarn     hadoop     971310 Apr  4 14:45 ./__spark_libs__/jersey-guava-2.22.2.jar
16515679  404 -rwxr-xr-x   1 yarn     hadoop     412739 Apr  4 14:45 ./__spark_libs__/commons-lang3-3.3.2.jar
16515719  696 -rwxr-xr-x   1 yarn     hadoop     710492 Apr  4 14:45 ./__spark_libs__/guice-3.0.jar
16516307   28 -rwxr-xr-x   1 yarn     hadoop      26514 Apr  4 14:45 ./__spark_libs__/stax-api-1.0.1.jar
16516293  468 -rwxr-xr-x   1 yarn     hadoop     477970 Apr  4 14:45 ./__spark_libs__/lift-json_2.11-2.6.3.jar
16515904 3352 -rwxr-xr-x   1 yarn     hadoop    3431544 Apr  4 14:45 ./__spark_libs__/hadoop-common-2.7.1.jar
17172707 6716 -rwxr-xr-x   1 yarn     hadoop    6873892 Mar 29 19:40 ./__spark_libs__/spark-catalyst_2.11-2.0.1.jar
16515414   16 -rwxr-xr-x   1 yarn     hadoop      15010 Apr  4 14:45 ./__spark_libs__/xmlenc-0.52.jar
17046972  532 -rwxr-xr-x   1 yarn     hadoop     540852 Apr  5 13:33 ./__spark_libs__/mysql-connector-java-5.0.8-bin.jar
17046957 2732 -rwxr-xr-x   1 yarn     hadoop    2796935 Apr  5 13:33 ./__spark_libs__/parquet-hadoop-bundle-1.6.0.jar
16515706   12 -rwxr-xr-x   1 yarn     hadoop      10023 Apr  4 14:45 ./__spark_libs__/java-xmlbuilder-1.0.jar
16516259 1768 -rwxr-xr-x   1 yarn     hadoop    1809447 Apr  4 14:45 ./__spark_libs__/datanucleus-rdbms-3.2.9.jar
17172561 1592 -rwxr-xr-x   1 yarn     hadoop    1627065 Mar 29 19:40 ./__spark_libs__/jersey-bundle-1.19.1.jar
16910729   16 -rwxr-xr-x   1 yarn     hadoop      15305 Apr  5 13:33 ./__spark_libs__/spark-tags_2.11-2.0.1.jar
16515699  220 -rwxr-xr-x   1 yarn     hadoop     223573 Apr  4 14:45 ./__spark_libs__/chill_2.11-0.8.0.jar
17171497  184 -rwxr-xr-x   1 yarn     hadoop     185140 Mar 29 19:40 ./__spark_libs__/commons-io-2.4.jar
16516349  508 -rwxr-xr-x   1 yarn     hadoop     516127 Apr  4 14:45 ./__spark_libs__/hadoop-mapreduce-client-app-2.7.1.jar
16516256 3152 -rwxr-xr-x   1 yarn     hadoop    3224708 Apr  4 14:45 ./__spark_libs__/derby-10.12.1.1.jar
16516316   24 -rwxr-xr-x   1 yarn     hadoop      21243 Apr  4 14:45 ./__spark_libs__/parquet-generator-1.7.0.jar
16516303 1772 -rwxr-xr-x   1 yarn     hadoop    1814309 Apr  4 14:45 ./__spark_libs__/spark-hive-thriftserver_2.11-2.0.1.jar
17172647    4 -rwxr-xr-x   1 yarn     hadoop       2497 Mar 29 19:40 ./__spark_libs__/javax.inject-1.jar
16516295  120 -rwxr-xr-x   1 yarn     hadoop     118973 Apr  4 14:45 ./__spark_libs__/hk2-utils-2.4.0-b34.jar
17172688  720 -rwxr-xr-x   1 yarn     hadoop     736658 Mar 29 19:40 ./__spark_libs__/httpclient-4.5.2.jar
16516312   44 -rwxr-xr-x   1 yarn     hadoop      41123 Apr  4 14:45 ./__spark_libs__/commons-cli-1.2.jar
17046643  576 -rwxr-xr-x   1 yarn     hadoop     589462 Apr  5 13:33 ./__spark_libs__/json4s-core_2.11-3.2.11.jar
17172894  100 -rwxr-xr-x   1 yarn     hadoop     100636 Mar 29 19:40 ./__spark_libs__/jsp-api-2.1.jar
17172576  176 -rwxr-xr-x   1 yarn     hadoop     178947 Mar 29 19:40 ./__spark_libs__/hk2-api-2.4.0-b34.jar
16515664  480 -rwxr-xr-x   1 yarn     hadoop     489884 Apr  4 14:45 ./__spark_libs__/log4j-1.2.17.jar
16910654   44 -rwxr-xr-x   1 yarn     hadoop      44925 Apr  5 13:33 ./__spark_libs__/apacheds-i18n-2.0.0-M15.jar
16516202    8 -rwxr-xr-x   1 yarn     hadoop       5711 Apr  4 14:45 ./__spark_libs__/minlog-1.3.0.jar
17172860  148 -rwxr-xr-x   1 yarn     hadoop     148627 Mar 29 19:40 ./__spark_libs__/stringtemplate-3.2.1.jar
17173108   44 -rwxr-xr-x   1 yarn     hadoop      41070 Mar 29 19:40 ./__spark_libs__/spark-unsafe_2.11-2.0.1.jar
17172658  232 -rwxr-xr-x   1 yarn     hadoop     236660 Mar 29 19:40 ./__spark_libs__/ST4-4.0.4.jar
16515652  116 -rwxr-xr-x   1 yarn     hadoop     114913 Apr  4 14:45 ./__spark_libs__/py4j-0.10.3.jar
17172593  600 -rwxr-xr-x   1 yarn     hadoop     613299 Mar 29 19:40 ./__spark_libs__/janino-2.7.8.jar
16516301  400 -rwxr-xr-x   1 yarn     hadoop     409467 Apr  4 14:45 ./__spark_libs__/mx4j-3.0.2.jar
16516239  208 -rwxr-xr-x   1 yarn     hadoop     212453 Apr  4 14:45 ./__spark_libs__/commons-net-2.2.jar
16515678   72 -rwxr-xr-x   1 yarn     hadoop      70688 Apr  4 14:45 ./__spark_libs__/hadoop-auth-2.7.1.jar
17172699  256 -rwxr-xr-x   1 yarn     hadoop     258370 Mar 29 19:40 ./__spark_libs__/calcite-avatica-1.2.0-incubating.jar
16515726  700 -rwxr-xr-x   1 yarn     hadoop     714194 Apr  4 14:45 ./__spark_libs__/javassist-3.18.1-GA.jar
16516289  200 -rwxr-xr-x   1 yarn     hadoop     201928 Apr  4 14:45 ./__spark_libs__/RoaringBitmap-0.5.11.jar
16515169    8 -rwxr-xr-x   1 yarn     hadoop       4596 Apr  4 15:15 ./__spark_libs__/jul-to-slf4j-1.7.16.jar
16515396  280 -rwxr-xr-x   1 yarn     hadoop     285447 Apr  4 14:45 ./__spark_libs__/parquet-encoding-1.7.0.jar
17172673  528 -rwxr-xr-x   1 yarn     hadoop     539912 Mar 29 19:40 ./__spark_libs__/jetty-6.1.26.jar
17172804  332 -rwxr-xr-x   1 yarn     hadoop     339666 Mar 29 19:40 ./__spark_libs__/datanucleus-api-jdo-3.2.6.jar
17046852   40 -rwxr-xr-x   1 yarn     hadoop      39280 Apr  5 13:33 ./__spark_libs__/metrics-jvm-3.1.2.jar
17172716   32 -rwxr-xr-x   1 yarn     hadoop      29540 Mar 29 19:40 ./__spark_libs__/spark-sketch_2.11-2.0.1.jar
17172579  788 -rwxr-xr-x   1 yarn     hadoop     802818 Mar 29 19:40 ./__spark_libs__/scalap-2.11.8.jar
16910640   92 -rwxr-xr-x   1 yarn     hadoop      93210 Apr  5 13:33 ./__spark_libs__/super-csv-2.2.0.jar
16516206   28 -rwxr-xr-x   1 yarn     hadoop      26366 Apr  4 14:45 ./__spark_libs__/javax.annotation-api-1.2.jar
17172667    8 -rwxr-xr-x   1 yarn     hadoop       4467 Mar 29 19:40 ./__spark_libs__/aopalliance-1.0.jar
16910697   16 -rwxr-xr-x   1 yarn     hadoop      15827 Apr  5 13:33 ./__spark_libs__/metrics-json-3.1.2.jar
17171489  164 -rwxr-xr-x   1 yarn     hadoop     164368 Mar 29 19:40 ./__spark_libs__/antlr-runtime-3.4.jar
17047069   68 -rwxr-xr-x   1 yarn     hadoop      66270 Apr  5 13:33 ./__spark_libs__/jersey-container-servlet-core-2.22.2.jar
16515766   64 -rwxr-xr-x   1 yarn     hadoop      62050 Apr  4 14:45 ./__spark_libs__/commons-logging-1.1.3.jar
16910646 3440 -rwxr-xr-x   1 yarn     hadoop    3519262 Apr  5 13:33 ./__spark_libs__/calcite-core-1.2.0-incubating.jar
17172590   20 -rwxr-xr-x   1 yarn     hadoop      18098 Mar 29 19:40 ./__spark_libs__/jersey-container-servlet-2.22.2.jar
16515879  212 -rwxr-xr-x   1 yarn     hadoop     213911 Apr  4 14:45 ./__spark_libs__/jline-2.12.1.jar
16515777  232 -rwxr-xr-x   1 yarn     hadoop     236880 Apr  4 14:45 ./__spark_libs__/lz4-1.3.0.jar
17046677  188 -rwxr-xr-x   1 yarn     hadoop     188671 Apr  5 13:33 ./__spark_libs__/commons-beanutils-1.7.0.jar
16910707   48 -rwxr-xr-x   1 yarn     hadoop      45944 Apr  5 13:33 ./__spark_libs__/json-20090211.jar
13371520   44 -rwxr-xr-x   1 yarn     hadoop      41263 Mar 29 19:40 ./__spark_libs__/jackson-module-paranamer-2.6.5.jar
16515807   44 -rwxr-xr-x   1 yarn     hadoop      45015 Apr  4 14:45 ./__spark_libs__/hadoop-mapreduce-client-shuffle-2.7.1.jar
17172621 13136 -rwxr-xr-x   1 yarn     hadoop   13448966 Mar 29 19:40 ./__spark_libs__/breeze_2.11-0.11.2.jar
17172562  112 -rwxr-xr-x   1 yarn     hadoop     110600 Mar 29 19:40 ./__spark_libs__/bonecp-0.8.0.RELEASE.jar
17172630   12 -rwxr-xr-x   1 yarn     hadoop       9939 Mar 29 19:40 ./__spark_libs__/slf4j-log4j12-1.7.16.jar
16516235 1204 -rwxr-xr-x   1 yarn     hadoop    1229125 Apr  4 14:45 ./__spark_libs__/xercesImpl-2.9.1.jar
17172655   80 -rwxr-xr-x   1 yarn     hadoop      79845 Mar 29 19:40 ./__spark_libs__/compress-lzf-1.0.3.jar
16910754   40 -rwxr-xr-x   1 yarn     hadoop      40341 Apr  5 13:33 ./__spark_libs__/json4s-jackson_2.11-3.2.11.jar
16910705   40 -rwxr-xr-x   1 yarn     hadoop      40817 Apr  5 13:33 ./__spark_libs__/hive-cli-1.2.1.spark2.jar
13371506  244 -rwxr-xr-x   1 yarn     hadoop     248171 Mar 29 19:40 ./__spark_libs__/curator-recipes-2.6.0.jar
16515703  676 -rwxr-xr-x   1 yarn     hadoop     691479 Apr  4 14:45 ./__spark_libs__/apacheds-kerberos-codec-2.0.0-M15.jar
16516222 11452 -rwxr-xr-x   1 yarn     hadoop   11723537 Apr  4 14:45 ./__spark_libs__/spark-core_2.11-2.0.1.jar
16515732   20 -rwxr-xr-x   1 yarn     hadoop      18482 Apr  4 14:45 ./__spark_libs__/eigenbase-properties-1.1.5.jar
17172762   20 -rwxr-xr-x   1 yarn     hadoop      17385 Mar 29 19:40 ./__spark_libs__/hadoop-annotations-2.7.1.jar
16910711    4 -rwxr-xr-x   1 yarn     hadoop       2545 Apr  5 13:33 ./__spark_libs__/hadoop-client-2.7.1.jar
16910699   56 -rwxr-xr-x   1 yarn     hadoop      55511 Apr  5 13:33 ./__spark_libs__/spark-network-shuffle_2.11-2.0.1.jar
16515869  668 -rwxr-xr-x   1 yarn     hadoop     680106 Apr  4 14:45 ./__spark_libs__/spark-graphx_2.11-2.0.1.jar
16516034   16 -rwxr-xr-x   1 yarn     hadoop      15071 Apr  4 14:45 ./__spark_libs__/jta-1.1.jar
17046978   20 -rwxr-xr-x   1 yarn     hadoop      16993 Apr  5 13:33 ./__spark_libs__/JavaEWAH-0.3.2.jar
16910738 8068 -rwxr-xr-x   1 yarn     hadoop    8260573 Apr  5 13:33 ./__spark_libs__/hadoop-hdfs-2.7.1.jar
16515742 1032 -rwxr-xr-x   1 yarn     hadoop    1056168 Apr  4 14:45 ./__spark_libs__/snappy-java-1.1.2.6.jar
16515739  320 -rwxr-xr-x   1 yarn     hadoop     326724 Apr  4 14:45 ./__spark_libs__/httpcore-4.4.4.jar
17172636 1256 -rwxr-xr-x   1 yarn     hadoop    1282424 Mar 29 19:40 ./__spark_libs__/ivy-2.4.0.jar
16910742 2044 -rwxr-xr-x   1 yarn     hadoop    2090370 Apr  5 13:33 ./__spark_libs__/spark-streaming_2.11-2.0.1.jar
16910751   20 -rwxr-xr-x   1 yarn     hadoop      16430 Apr  5 13:33 ./__spark_libs__/jcl-over-slf4j-1.7.16.jar
16910600  204 -rwxr-xr-x   1 yarn     hadoop     206035 Apr  5 13:33 ./__spark_libs__/commons-beanutils-core-1.8.0.jar
16516262  616 -rwxr-xr-x   1 yarn     hadoop     627814 Apr  4 14:45 ./__spark_libs__/joda-time-2.9.3.jar
16516070 2140 -rwxr-xr-x   1 yarn     hadoop    2189117 Apr  4 14:45 ./__spark_libs__/guava-14.0.1.jar
16515670  100 -rwxr-xr-x   1 yarn     hadoop     100680 Apr  4 14:45 ./__spark_libs__/hive-jdbc-1.2.1.spark2.jar
17173079 5380 -rwxr-xr-x   1 yarn     hadoop    5505200 Mar 29 19:40 ./__spark_libs__/hive-metastore-1.2.1.spark2.jar
16515629  684 -rwxr-xr-x   1 yarn     hadoop     698375 Apr  4 14:45 ./__spark_libs__/jersey-common-2.22.2.jar
16516192  300 -rwxr-xr-x   1 yarn     hadoop     305001 Apr  4 14:45 ./__spark_libs__/commons-httpclient-3.1.jar
17172713   72 -rwxr-xr-x   1 yarn     hadoop      72733 Mar 29 19:40 ./__spark_libs__/jersey-media-jaxb-2.22.2.jar
17172600   92 -rwxr-xr-x   1 yarn     hadoop      93407 Mar 29 19:40 ./__spark_libs__/pyrolite-4.9.jar
17172625   68 -rwxr-xr-x   1 yarn     hadoop      65653 Mar 29 19:40 ./__spark_libs__/spark-launcher_2.11-2.0.1.jar
17172682  192 -rwxr-xr-x   1 yarn     hadoop     192993 Mar 29 19:40 ./__spark_libs__/avro-ipc-1.7.7.jar
16516345  736 -rwxr-xr-x   1 yarn     hadoop     753012 Apr  4 14:45 ./__spark_libs__/hadoop-mapreduce-client-common-2.7.1.jar
17172628   32 -rwxr-xr-x   1 yarn     hadoop      32145 Mar 29 19:40 ./__spark_libs__/hadoop-yarn-server-web-proxy-2.7.1.jar
16910748  428 -rwxr-xr-x   1 yarn     hadoop     436303 Apr  5 13:33 ./__spark_libs__/avro-1.7.7.jar
17172585  144 -rwxr-xr-x   1 yarn     hadoop     143602 Mar 29 19:40 ./__spark_libs__/commons-digester-1.8.jar
16515647  116 -rwxr-xr-x   1 yarn     hadoop     115534 Apr  4 14:45 ./__spark_libs__/javax.ws.rs-api-2.0.1.jar
17172643   52 -rwxr-xr-x   1 yarn     hadoop      50619 Mar 29 19:40 ./__spark_libs__/chill-java-0.8.0.jar
16515953 1168 -rwxr-xr-x   1 yarn     hadoop    1194003 Apr  4 14:45 ./__spark_libs__/arpack_combined_all-0.1.jar
17172588   20 -rwxr-xr-x   1 yarn     hadoop      19827 Mar 29 19:40 ./__spark_libs__/opencsv-2.3.jar
16516278 7108 -rwxr-xr-x   1 yarn     hadoop    7276083 Apr  4 14:45 ./__spark_libs__/spire_2.11-0.7.4.jar
17171485  436 -rwxr-xr-x   1 yarn     hadoop     445288 Mar 29 19:40 ./__spark_libs__/antlr-2.7.7.jar
16515710  352 -rwxr-xr-x   1 yarn     hadoop     358390 Apr  4 14:45 ./__spark_libs__/kryo-shaded-3.0.3.jar
16515649 1616 -rwxr-xr-x   1 yarn     hadoop    1654097 Apr  4 14:45 ./__spark_libs__/hadoop-yarn-common-2.7.1.jar
17172615  656 -rwxr-xr-x   1 yarn     hadoop     669589 Mar 29 19:40 ./__spark_libs__/spark-yarn_2.11-2.0.1.jar
16910591 1512 -rwxr-xr-x   1 yarn     hadoop    1544875 Apr  5 13:33 ./__spark_libs__/hadoop-mapreduce-client-core-2.7.1.jar
16515466   32 -rwxr-xr-x   1 yarn     hadoop      30595 Apr  4 14:45 ./__spark_libs__/commons-compiler-2.7.6.jar
16910756  640 -rwxr-xr-x   1 yarn     hadoop     654216 Apr  5 13:33 ./__spark_libs__/pmml-model-1.2.15.jar
16515825 5712 -rwxr-xr-x   1 yarn     hadoop    5847591 Apr  4 14:45 ./__spark_libs__/spark-mllib_2.11-2.0.1.jar
17047157  932 -rwxr-xr-x   1 yarn     hadoop     951701 Apr  5 13:33 ./__spark_libs__/jersey-server-2.22.2.jar
17047013  176 -rwxr-xr-x   1 yarn     hadoop     177832 Apr  5 13:33 ./__spark_libs__/spark-mllib-local_2.11-2.0.1.jar
16515773  512 -rwxr-xr-x   1 yarn     hadoop     521157 Apr  4 14:45 ./__spark_libs__/mail-1.4.7.jar
16515782 1024 -rwxr-xr-x   1 yarn     hadoop    1048110 Apr  4 14:45 ./__spark_libs__/parquet-jackson-1.7.0.jar
17172695  236 -rwxr-xr-x   1 yarn     hadoop     241367 Mar 29 19:40 ./__spark_libs__/commons-compress-1.4.1.jar
16516280  140 -rwxr-xr-x   1 yarn     hadoop     142631 Apr  4 14:45 ./__spark_libs__/hadoop-yarn-client-2.7.1.jar
16515790   96 -rwxr-xr-x   1 yarn     hadoop      94672 Apr  4 14:45 ./__spark_libs__/xz-1.0.jar
17172651  208 -rwxr-xr-x   1 yarn     hadoop     209622 Mar 29 19:40 ./__spark_libs__/parquet-hadoop-1.7.0.jar
16516276  420 -rwxr-xr-x   1 yarn     hadoop     427780 Apr  4 14:45 ./__spark_libs__/jodd-core-3.5.2.jar
16515752   20 -rwxr-xr-x   1 yarn     hadoop      20235 Apr  4 14:45 ./__spark_libs__/osgi-resource-locator-1.0.1.jar
16516247  136 -rwxr-xr-x   1 yarn     hadoop     138464 Apr  4 14:45 ./__spark_libs__/hive-beeline-1.2.1.spark2.jar
16910596  112 -rwxr-xr-x   1 yarn     hadoop     112558 Apr  5 13:33 ./__spark_libs__/metrics-core-3.1.2.jar
17172596  136 -rwxr-xr-x   1 yarn     hadoop     135552 Mar 29 19:40 ./__spark_libs__/breeze-macros_2.11-0.11.2.jar
16910627  180 -rwxr-xr-x   1 yarn     hadoop     180736 Apr  5 13:33 ./__spark_libs__/avro-mapred-1.7.7-hadoop2.jar
16516266  440 -rwxr-xr-x   1 yarn     hadoop     448794 Apr  4 14:45 ./__spark_libs__/apache-log4j-extras-1.2.17.jar
17046657  188 -rwxr-xr-x   1 yarn     hadoop     190432 Apr  5 13:33 ./__spark_libs__/gson-2.2.4.jar
16910733   44 -rwxr-xr-x   1 yarn     hadoop      41755 Apr  5 13:33 ./__spark_libs__/objenesis-2.1.jar
16516340 2008 -rwxr-xr-x   1 yarn     hadoop    2054931 Apr  4 14:45 ./__spark_libs__/netty-all-4.0.29.Final.jar
16515406   36 -rwxr-xr-x   1 yarn     hadoop      33015 Apr  4 14:45 ./__spark_libs__/jsr305-1.3.9.jar
16515872 1972 -rwxr-xr-x   1 yarn     hadoop    2015514 Apr  4 14:45 ./__spark_libs__/hadoop-yarn-api-2.7.1.jar
16910651   64 -rwxr-xr-x   1 yarn     hadoop      65012 Apr  5 13:33 ./__spark_libs__/guice-servlet-3.0.jar
16516352    8 -rwxr-xr-x   1 yarn     hadoop       5310 Apr  4 14:45 ./__spark_libs__/pmml-schema-1.2.15.jar
16910584   64 -rwxr-xr-x   1 yarn     hadoop      63777 Apr  5 13:33 ./__spark_libs__/validation-api-1.1.0.Final.jar
16516286  388 -rwxr-xr-x   1 yarn     hadoop     395195 Apr  4 14:45 ./__spark_libs__/javolution-5.5.1.jar
16515462  748 -rwxr-xr-x   1 yarn     hadoop     764569 Apr  4 14:45 ./__spark_libs__/jtransforms-2.4.0.jar
16515684   24 -rwxr-xr-x   1 yarn     hadoop      21575 Apr  4 14:45 ./__spark_libs__/parquet-common-1.7.0.jar
16908289  776 -rwxr-xr-x   1 yarn     hadoop     792964 Apr  5 13:33 ./__spark_libs__/zookeeper-3.4.6.jar
16515635   20 -rwxr-xr-x   1 yarn     hadoop      16560 Apr  4 14:45 ./__spark_libs__/api-asn1-api-1.0.0-M20.jar
17173117  416 -rwxr-xr-x   1 yarn     hadoop     423753 Mar 29 19:40 ./__spark_libs__/scala-parser-combinators_2.11-1.0.4.jar
17172572  284 -rwxr-xr-x   1 yarn     hadoop     290506 Mar 29 19:40 ./__spark_libs__/univocity-parsers-2.1.1.jar
16515655  292 -rwxr-xr-x   1 yarn     hadoop     298829 Apr  4 14:45 ./__spark_libs__/commons-configuration-1.6.jar
17172661 11232 -rwxr-xr-x   1 yarn     hadoop   11498852 Mar 29 19:40 ./__spark_libs__/hive-exec-1.2.1.spark2.jar
17171947 1204 -rwxr-xr-x   1 yarn     hadoop    1230201 Mar 29 19:40 ./__spark_libs__/netty-3.8.0.Final.jar
16910726   96 -rwxr-xr-x   1 yarn     hadoop      96221 Apr  5 13:33 ./__spark_libs__/commons-pool-1.5.4.jar
17172612   40 -rwxr-xr-x   1 yarn     hadoop      40509 Mar 29 19:40 ./__spark_libs__/slf4j-api-1.7.16.jar
16516331  296 -rwxr-xr-x   1 yarn     hadoop     302248 Apr  4 14:45 ./__spark_libs__/antlr4-runtime-4.5.3.jar
17172608  280 -rwxr-xr-x   1 yarn     hadoop     284184 Mar 29 19:40 ./__spark_libs__/commons-codec-1.10.jar
16516337  308 -rwxr-xr-x   1 yarn     hadoop     313686 Apr  4 14:45 ./__spark_libs__/libfb303-0.9.2.jar
17172604 1848 -rwxr-xr-x   1 yarn     hadoop    1890075 Mar 29 19:40 ./__spark_libs__/datanucleus-core-3.2.10.jar
16515723   64 -rwxr-xr-x   1 yarn     hadoop      65261 Apr  4 14:45 ./__spark_libs__/oro-2.0.8.jar
17566958 1152 -r-x------   1 yarn     hadoop    1178454 May 16 11:10 ./__app__.jar
8127778    4 drwx--x---   2 yarn     hadoop       4096 May 16 11:11 ./tmp
8127785    4 -rwx------   1 yarn     hadoop        730 May 16 11:11 ./default_container_executor.sh
17567000    4 drwx------   2 yarn     hadoop       4096 May 16 11:10 ./__spark_conf__
17567028    4 -r-x------   1 yarn     hadoop        229 May 16 11:10 ./__spark_conf__/slaves
17567027    4 -r-x------   1 yarn     hadoop       1308 May 16 11:10 ./__spark_conf__/hadoop-policy.xml
17567030    4 -r-x------   1 yarn     hadoop        951 May 16 11:10 ./__spark_conf__/mapred-env.cmd
17567035    4 -r-x------   1 yarn     hadoop        945 May 16 11:10 ./__spark_conf__/taskcontroller.cfg
17567002    8 -r-x------   1 yarn     hadoop       5467 May 16 11:10 ./__spark_conf__/hadoop-env.sh
17567022    8 -r-x------   1 yarn     hadoop       4221 May 16 11:10 ./__spark_conf__/task-log4j.properties
17567033    8 -r-x------   1 yarn     hadoop       4113 May 16 11:10 ./__spark_conf__/mapred-queues.xml.template
17567036   36 -r-x------   1 yarn     hadoop      34061 May 16 11:10 ./__spark_conf__/__spark_conf__.properties
17567009    4 -r-x------   1 yarn     hadoop       1631 May 16 11:10 ./__spark_conf__/kms-log4j.properties
17567018    8 -r-x------   1 yarn     hadoop       6572 May 16 11:10 ./__spark_conf__/hdfs-site.xml
17567005   20 -r-x------   1 yarn     hadoop      16530 May 16 11:10 ./__spark_conf__/yarn-site.xml
17567023    4 -r-x------   1 yarn     hadoop        690 May 16 11:10 ./__spark_conf__/mapred-env.sh
17567025    4 -r-x------   1 yarn     hadoop       2316 May 16 11:10 ./__spark_conf__/ssl-client.xml.example
17567017    8 -r-x------   1 yarn     hadoop       5006 May 16 11:10 ./__spark_conf__/yarn-env.sh
17567019    4 -r-x------   1 yarn     hadoop       1019 May 16 11:10 ./__spark_conf__/container-executor.cfg
17567021    4 -r-x------   1 yarn     hadoop       2490 May 16 11:10 ./__spark_conf__/hadoop-metrics.properties
17567012    8 -r-x------   1 yarn     hadoop       5404 May 16 11:10 ./__spark_conf__/capacity-scheduler.xml
17567001    8 -r-x------   1 yarn     hadoop       6836 May 16 11:10 ./__spark_conf__/mapred-site.xml
17567006    4 -r-x------   1 yarn     hadoop       3979 May 16 11:10 ./__spark_conf__/hadoop-env.cmd
17567010    4 -r-x------   1 yarn     hadoop       2250 May 16 11:10 ./__spark_conf__/yarn-env.cmd
17567014    4 -r-x------   1 yarn     hadoop        758 May 16 11:10 ./__spark_conf__/mapred-site.xml.template
17567016    4 -r-x------   1 yarn     hadoop       1335 May 16 11:10 ./__spark_conf__/configuration.xsl
17567015    4 -r-x------   1 yarn     hadoop       2358 May 16 11:10 ./__spark_conf__/topology_script.py
17567032    4 -r-x------   1 yarn     hadoop       2268 May 16 11:10 ./__spark_conf__/ssl-server.xml.example
17567013    4 -r-x------   1 yarn     hadoop       3518 May 16 11:10 ./__spark_conf__/kms-acls.xml
17567003   12 -r-x------   1 yarn     hadoop       8699 May 16 11:10 ./__spark_conf__/log4j.properties
17567007    0 -r-x------   1 yarn     hadoop          0 May 16 11:10 ./__spark_conf__/yarn.exclude
17567024    4 -r-x------   1 yarn     hadoop       1602 May 16 11:10 ./__spark_conf__/health_check
17567008    8 -r-x------   1 yarn     hadoop       4197 May 16 11:10 ./__spark_conf__/core-site.xml
17567029    4 -r-x------   1 yarn     hadoop        724 May 16 11:10 ./__spark_conf__/topology_mappings.data
17567026    4 -r-x------   1 yarn     hadoop       1527 May 16 11:10 ./__spark_conf__/kms-env.sh
17567011    4 -r-x------   1 yarn     hadoop        884 May 16 11:10 ./__spark_conf__/ssl-client.xml
17567031    4 -r-x------   1 yarn     hadoop       1000 May 16 11:10 ./__spark_conf__/ssl-server.xml
17567004    4 -r-x------   1 yarn     hadoop       2302 May 16 11:10 ./__spark_conf__/hadoop-metrics2.properties
17567034    8 -r-x------   1 yarn     hadoop       5511 May 16 11:10 ./__spark_conf__/kms-site.xml
17567020    4 -r-x------   1 yarn     hadoop       1020 May 16 11:10 ./__spark_conf__/commons-logging.properties
8127782    4 -rw-r--r--   1 yarn     hadoop        604 May 16 11:11 ./.launch_container.sh.crc
broken symlinks(find -L . -maxdepth 5 -type l -ls):
End of LogType:directory.info

LogType:launch_container.sh
Log Upload Time:Tue May 16 11:12:19 +0700 2017
LogLength:75972
Log Contents:
#!/bin/bash

export SPARK_YARN_STAGING_DIR="hdfs://c408.hadoop.gda.lo:8020/user/fairy/.sparkStaging/application_1494395298335_303847"
export LOCAL_DIRS="/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847"
export HADOOP_CONF_DIR="/usr/hdp/current/hadoop-client/conf"
export NM_HTTP_PORT="8042"
export JAVA_HOME="/usr/jdk64/jdk1.8.0_92/"
export LOG_DIRS="/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000003"
export NM_AUX_SERVICE_mapreduce_shuffle="AAA0+gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=
"
export NM_PORT="45454"
export USER="fairy"
export HADOOP_YARN_HOME="/usr/hdp/current/hadoop-yarn-nodemanager"
export CLASSPATH="$PWD:$PWD/__spark_conf__:$PWD/__spark_libs__/*:$HADOOP_CONF_DIR:/usr/hdp/current/hadoop-client/*:/usr/hdp/current/hadoop-client/lib/*:/usr/hdp/current/hadoop-hdfs-client/*:/usr/hdp/current/hadoop-hdfs-client/lib/*:/usr/hdp/current/hadoop-yarn-client/*:/usr/hdp/current/hadoop-yarn-client/lib/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.4.2.0-258/hadoop/lib/hadoop-lzo-0.6.0.2.4.2.0-258.jar:/etc/hadoop/conf/secure"
export SPARK_LOG_URL_STDOUT="http://c402.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_02_000003/fairy/stdout?start=-4096"
export NM_HOST="c402.hadoop.gda.lo"
export SPARK_YARN_MODE="true"
export HADOOP_TOKEN_FILE_LOCATION="/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_02_000003/container_tokens"
export NM_AUX_SERVICE_spark_shuffle=""
export SPARK_USER="fairy"
export SPARK_LOG_URL_STDERR="http://c402.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_02_000003/fairy/stderr?start=-4096"
export LOGNAME="fairy"
export JVM_PID="$$"
export PWD="/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_02_000003"
export HOME="/home/"
export CONTAINER_ID="container_e81_1494395298335_303847_02_000003"
export MALLOC_ARENA_MAX="4"
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372216/apache-log4j-extras-1.2.17.jar" "__spark_libs__/apache-log4j-extras-1.2.17.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370624/hk2-locator-2.4.0-b34.jar" "__spark_libs__/hk2-locator-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370640/bcprov-jdk15on-1.51.jar" "__spark_libs__/bcprov-jdk15on-1.51.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372237/jersey-client-2.22.2.jar" "__spark_libs__/jersey-client-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370617/compress-lzf-1.0.3.jar" "__spark_libs__/compress-lzf-1.0.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372199/commons-logging-1.1.3.jar" "__spark_libs__/commons-logging-1.1.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370635/protobuf-java-2.5.0.jar" "__spark_libs__/protobuf-java-2.5.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372167/apacheds-kerberos-codec-2.0.0-M15.jar" "__spark_libs__/apacheds-kerberos-codec-2.0.0-M15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370615/javax.inject-1.jar" "__spark_libs__/javax.inject-1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372217/jackson-databind-2.6.5.jar" "__spark_libs__/jackson-databind-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372147/jtransforms-2.4.0.jar" "__spark_libs__/jtransforms-2.4.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372927/gson-2.2.4.jar" "__spark_libs__/gson-2.2.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370602/datanucleus-core-3.2.10.jar" "__spark_libs__/datanucleus-core-3.2.10.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370620/hive-exec-1.2.1.spark2.jar" "__spark_libs__/hive-exec-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372910/spark-streaming_2.11-2.0.1.jar" "__spark_libs__/spark-streaming_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370649/stringtemplate-3.2.1.jar" "__spark_libs__/stringtemplate-3.2.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370646/datanucleus-api-jdo-3.2.6.jar" "__spark_libs__/datanucleus-api-jdo-3.2.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372160/hive-jdbc-1.2.1.spark2.jar" "__spark_libs__/hive-jdbc-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372164/hadoop-yarn-server-common-2.7.1.jar" "__spark_libs__/hadoop-yarn-server-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372180/api-util-1.0.0-M20.jar" "__spark_libs__/api-util-1.0.0-M20.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372211/hadoop-common-2.7.1.jar" "__spark_libs__/hadoop-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372194/snappy-java-1.1.2.6.jar" "__spark_libs__/snappy-java-1.1.2.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370603/commons-codec-1.10.jar" "__spark_libs__/commons-codec-1.10.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372202/parquet-jackson-1.7.0.jar" "__spark_libs__/parquet-jackson-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370630/calcite-avatica-1.2.0-incubating.jar" "__spark_libs__/calcite-avatica-1.2.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370595/commons-digester-1.8.jar" "__spark_libs__/commons-digester-1.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372894/apacheds-i18n-2.0.0-M15.jar" "__spark_libs__/apacheds-i18n-2.0.0-M15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372213/arpack_combined_all-0.1.jar" "__spark_libs__/arpack_combined_all-0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372170/commons-httpclient-3.1.jar" "__spark_libs__/commons-httpclient-3.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372926/JavaEWAH-0.3.2.jar" "__spark_libs__/JavaEWAH-0.3.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372925/mysql-connector-java-5.0.8-bin.jar" "__spark_libs__/mysql-connector-java-5.0.8-bin.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/usercache/fairy/filecache/33843/__spark_conf__.zip" "__spark_conf__"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372183/datanucleus-rdbms-3.2.9.jar" "__spark_libs__/datanucleus-rdbms-3.2.9.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372174/javax.annotation-api-1.2.jar" "__spark_libs__/javax.annotation-api-1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370587/curator-recipes-2.6.0.jar" "__spark_libs__/curator-recipes-2.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372239/hadoop-mapreduce-client-app-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-app-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372240/pmml-schema-1.2.15.jar" "__spark_libs__/pmml-schema-1.2.15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372892/calcite-core-1.2.0-incubating.jar" "__spark_libs__/calcite-core-1.2.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372175/spark-core_2.11-2.0.1.jar" "__spark_libs__/spark-core_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372929/spark-repl_2.11-2.0.1.jar" "__spark_libs__/spark-repl_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372922/metrics-jvm-3.1.2.jar" "__spark_libs__/metrics-jvm-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372151/jersey-guava-2.22.2.jar" "__spark_libs__/jersey-guava-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372177/commons-net-2.2.jar" "__spark_libs__/commons-net-2.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372907/objenesis-2.1.jar" "__spark_libs__/objenesis-2.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372924/paranamer-2.3.jar" "__spark_libs__/paranamer-2.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370627/calcite-linq4j-1.2.0-incubating.jar" "__spark_libs__/calcite-linq4j-1.2.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372221/jta-1.1.jar" "__spark_libs__/jta-1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372916/commons-beanutils-1.7.0.jar" "__spark_libs__/commons-beanutils-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370619/leveldbjni-all-1.8.jar" "__spark_libs__/leveldbjni-all-1.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372923/parquet-hadoop-bundle-1.6.0.jar" "__spark_libs__/parquet-hadoop-bundle-1.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370593/scalap-2.11.8.jar" "__spark_libs__/scalap-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370645/hadoop-annotations-2.7.1.jar" "__spark_libs__/hadoop-annotations-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372915/pmml-model-1.2.15.jar" "__spark_libs__/pmml-model-1.2.15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372157/parquet-format-2.3.0-incubating.jar" "__spark_libs__/parquet-format-2.3.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372156/commons-configuration-1.6.jar" "__spark_libs__/commons-configuration-1.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372196/osgi-resource-locator-1.0.1.jar" "__spark_libs__/osgi-resource-locator-1.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372200/mail-1.4.7.jar" "__spark_libs__/mail-1.4.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372154/hadoop-yarn-common-2.7.1.jar" "__spark_libs__/hadoop-yarn-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370621/aopalliance-1.0.jar" "__spark_libs__/aopalliance-1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372229/stax-api-1.0.1.jar" "__spark_libs__/stax-api-1.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372236/netty-all-4.0.29.Final.jar" "__spark_libs__/netty-all-4.0.29.Final.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370599/breeze-macros_2.11-0.11.2.jar" "__spark_libs__/breeze-macros_2.11-0.11.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372235/libfb303-0.9.2.jar" "__spark_libs__/libfb303-0.9.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370650/jsp-api-2.1.jar" "__spark_libs__/jsp-api-2.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372204/jackson-jaxrs-1.9.13.jar" "__spark_libs__/jackson-jaxrs-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370606/jackson-mapper-asl-1.9.13.jar" "__spark_libs__/jackson-mapper-asl-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372918/spark-mllib-local_2.11-2.0.1.jar" "__spark_libs__/spark-mllib-local_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372195/httpcore-4.4.4.jar" "__spark_libs__/httpcore-4.4.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370653/scala-parser-combinators_2.11-1.0.4.jar" "__spark_libs__/scala-parser-combinators_2.11-1.0.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372162/commons-lang3-3.3.2.jar" "__spark_libs__/commons-lang3-3.3.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372209/jline-2.12.1.jar" "__spark_libs__/jline-2.12.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370591/univocity-parsers-2.1.1.jar" "__spark_libs__/univocity-parsers-2.1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372214/hadoop-mapreduce-client-jobclient-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-jobclient-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372173/minlog-1.3.0.jar" "__spark_libs__/minlog-1.3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370616/parquet-hadoop-1.7.0.jar" "__spark_libs__/parquet-hadoop-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370634/spark-sketch_2.11-2.0.1.jar" "__spark_libs__/spark-sketch_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370592/hk2-api-2.4.0-b34.jar" "__spark_libs__/hk2-api-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372901/scala-xml_2.11-1.0.2.jar" "__spark_libs__/scala-xml_2.11-1.0.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372212/jackson-module-scala_2.11-2.6.5.jar" "__spark_libs__/jackson-module-scala_2.11-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370651/hive-metastore-1.2.1.spark2.jar" "__spark_libs__/hive-metastore-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372884/metrics-core-3.1.2.jar" "__spark_libs__/metrics-core-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370632/spark-catalyst_2.11-2.0.1.jar" "__spark_libs__/spark-catalyst_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372890/zookeeper-3.4.6.jar" "__spark_libs__/zookeeper-3.4.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372186/xmlenc-0.52.jar" "__spark_libs__/xmlenc-0.52.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372224/lift-json_2.11-2.6.3.jar" "__spark_libs__/lift-json_2.11-2.6.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372900/hadoop-client-2.7.1.jar" "__spark_libs__/hadoop-client-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372911/scala-compiler-2.11.8.jar" "__spark_libs__/scala-compiler-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370601/core-1.1.2.jar" "__spark_libs__/core-1.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372193/eigenbase-properties-1.1.5.jar" "__spark_libs__/eigenbase-properties-1.1.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372198/curator-framework-2.6.0.jar" "__spark_libs__/curator-framework-2.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370626/httpclient-4.5.2.jar" "__spark_libs__/httpclient-4.5.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370641/spire-macros_2.11-0.7.4.jar" "__spark_libs__/spire-macros_2.11-0.7.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372189/kryo-shaded-3.0.3.jar" "__spark_libs__/kryo-shaded-3.0.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372905/commons-pool-1.5.4.jar" "__spark_libs__/commons-pool-1.5.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370590/commons-dbcp-1.4.jar" "__spark_libs__/commons-dbcp-1.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372188/java-xmlbuilder-1.0.jar" "__spark_libs__/java-xmlbuilder-1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372219/jodd-core-3.5.2.jar" "__spark_libs__/jodd-core-3.5.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372187/commons-compiler-2.7.6.jar" "__spark_libs__/commons-compiler-2.7.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372913/jcl-over-slf4j-1.7.16.jar" "__spark_libs__/jcl-over-slf4j-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372149/api-asn1-api-1.0.0-M20.jar" "__spark_libs__/api-asn1-api-1.0.0-M20.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372201/parquet-column-1.7.0.jar" "__spark_libs__/parquet-column-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372914/json4s-jackson_2.11-3.2.11.jar" "__spark_libs__/json4s-jackson_2.11-3.2.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370625/avro-ipc-1.7.7.jar" "__spark_libs__/avro-ipc-1.7.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372158/log4j-1.2.17.jar" "__spark_libs__/log4j-1.2.17.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372161/hadoop-auth-2.7.1.jar" "__spark_libs__/hadoop-auth-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372238/hadoop-mapreduce-client-common-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372897/jackson-annotations-2.6.5.jar" "__spark_libs__/jackson-annotations-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372266/jdo-api-3.0.1.jar" "__spark_libs__/jdo-api-3.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372895/metrics-json-3.1.2.jar" "__spark_libs__/metrics-json-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372222/javolution-5.5.1.jar" "__spark_libs__/javolution-5.5.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372215/guava-14.0.1.jar" "__spark_libs__/guava-14.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372230/commons-cli-1.2.jar" "__spark_libs__/commons-cli-1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370589/jets3t-0.9.3.jar" "__spark_libs__/jets3t-0.9.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372887/commons-beanutils-core-1.8.0.jar" "__spark_libs__/commons-beanutils-core-1.8.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372182/derby-10.12.1.1.jar" "__spark_libs__/derby-10.12.1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370605/spark-yarn_2.11-2.0.1.jar" "__spark_libs__/spark-yarn_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372886/libthrift-0.9.2.jar" "__spark_libs__/libthrift-0.9.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372891/super-csv-2.2.0.jar" "__spark_libs__/super-csv-2.2.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372159/jpam-1.1.jar" "__spark_libs__/jpam-1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372920/xbean-asm5-shaded-4.4.jar" "__spark_libs__/xbean-asm5-shaded-4.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370600/pyrolite-4.9.jar" "__spark_libs__/pyrolite-4.9.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372178/spark-sql_2.11-2.0.1.jar" "__spark_libs__/spark-sql_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370598/janino-2.7.8.jar" "__spark_libs__/janino-2.7.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372181/spark-network-common_2.11-2.0.1.jar" "__spark_libs__/spark-network-common_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372912/avro-1.7.7.jar" "__spark_libs__/avro-1.7.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370629/commons-compress-1.4.1.jar" "__spark_libs__/commons-compress-1.4.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370636/antlr-2.7.7.jar" "__spark_libs__/antlr-2.7.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370648/commons-lang-2.6.jar" "__spark_libs__/commons-lang-2.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370652/spark-unsafe_2.11-2.0.1.jar" "__spark_libs__/spark-unsafe_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372192/javassist-3.18.1-GA.jar" "__spark_libs__/javassist-3.18.1-GA.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370628/scala-library-2.11.8.jar" "__spark_libs__/scala-library-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372220/hadoop-yarn-client-2.7.1.jar" "__spark_libs__/hadoop-yarn-client-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372208/hadoop-yarn-api-2.7.1.jar" "__spark_libs__/hadoop-yarn-api-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370612/bonecp-0.8.0.RELEASE.jar" "__spark_libs__/bonecp-0.8.0.RELEASE.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372904/htrace-core-3.1.0-incubating.jar" "__spark_libs__/htrace-core-3.1.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372896/spark-network-shuffle_2.11-2.0.1.jar" "__spark_libs__/spark-network-shuffle_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372163/parquet-common-1.7.0.jar" "__spark_libs__/parquet-common-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372898/hive-cli-1.2.1.spark2.jar" "__spark_libs__/hive-cli-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370614/jersey-bundle-1.19.1.jar" "__spark_libs__/jersey-bundle-1.19.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372176/xercesImpl-2.9.1.jar" "__spark_libs__/xercesImpl-2.9.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372184/joda-time-2.9.3.jar" "__spark_libs__/joda-time-2.9.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370611/ivy-2.4.0.jar" "__spark_libs__/ivy-2.4.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372190/guice-3.0.jar" "__spark_libs__/guice-3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372185/jsr305-1.3.9.jar" "__spark_libs__/jsr305-1.3.9.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372232/stream-2.7.0.jar" "__spark_libs__/stream-2.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372928/jackson-core-2.6.5.jar" "__spark_libs__/jackson-core-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372893/guice-servlet-3.0.jar" "__spark_libs__/guice-servlet-3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372146/parquet-encoding-1.7.0.jar" "__spark_libs__/parquet-encoding-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372165/commons-collections-3.2.2.jar" "__spark_libs__/commons-collections-3.2.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372206/hadoop-mapreduce-client-shuffle-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-shuffle-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372902/javax.servlet-api-3.1.0.jar" "__spark_libs__/javax.servlet-api-3.1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370610/slf4j-log4j12-1.7.16.jar" "__spark_libs__/slf4j-log4j12-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372152/stax-api-1.0-2.jar" "__spark_libs__/stax-api-1.0-2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372179/hive-beeline-1.2.1.spark2.jar" "__spark_libs__/hive-beeline-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372218/activation-1.1.1.jar" "__spark_libs__/activation-1.1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372233/antlr4-runtime-4.5.3.jar" "__spark_libs__/antlr4-runtime-4.5.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370609/hadoop-yarn-server-web-proxy-2.7.1.jar" "__spark_libs__/hadoop-yarn-server-web-proxy-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370637/antlr-runtime-3.4.jar" "__spark_libs__/antlr-runtime-3.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370597/jersey-container-servlet-2.22.2.jar" "__spark_libs__/jersey-container-servlet-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372930/jersey-server-2.22.2.jar" "__spark_libs__/jersey-server-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372231/parquet-generator-1.7.0.jar" "__spark_libs__/parquet-generator-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372903/javax.inject-2.4.0-b34.jar" "__spark_libs__/javax.inject-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372885/hadoop-mapreduce-client-core-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-core-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370638/commons-io-2.4.jar" "__spark_libs__/commons-io-2.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370647/aopalliance-repackaged-2.4.0-b34.jar" "__spark_libs__/aopalliance-repackaged-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372191/oro-2.0.8.jar" "__spark_libs__/oro-2.0.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372223/RoaringBitmap-0.5.11.jar" "__spark_libs__/RoaringBitmap-0.5.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372226/spark-hive_2.11-2.0.1.jar" "__spark_libs__/spark-hive_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372906/spark-tags_2.11-2.0.1.jar" "__spark_libs__/spark-tags_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370633/jersey-media-jaxb-2.22.2.jar" "__spark_libs__/jersey-media-jaxb-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372267/jul-to-slf4j-1.7.16.jar" "__spark_libs__/jul-to-slf4j-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370623/jetty-6.1.26.jar" "__spark_libs__/jetty-6.1.26.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372148/jersey-common-2.22.2.jar" "__spark_libs__/jersey-common-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370588/jackson-module-paranamer-2.6.5.jar" "__spark_libs__/jackson-module-paranamer-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370613/chill-java-0.8.0.jar" "__spark_libs__/chill-java-0.8.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370642/mesos-0.21.1-shaded-protobuf.jar" "__spark_libs__/mesos-0.21.1-shaded-protobuf.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370618/ST4-4.0.4.jar" "__spark_libs__/ST4-4.0.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370594/jackson-core-asl-1.9.13.jar" "__spark_libs__/jackson-core-asl-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372919/jersey-container-servlet-core-2.22.2.jar" "__spark_libs__/jersey-container-servlet-core-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370604/slf4j-api-1.7.16.jar" "__spark_libs__/slf4j-api-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370643/netty-3.8.0.Final.jar" "__spark_libs__/netty-3.8.0.Final.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372227/mx4j-3.0.2.jar" "__spark_libs__/mx4j-3.0.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372908/validation-api-1.1.0.Final.jar" "__spark_libs__/validation-api-1.1.0.Final.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372155/py4j-0.10.3.jar" "__spark_libs__/py4j-0.10.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372166/chill_2.11-0.8.0.jar" "__spark_libs__/chill_2.11-0.8.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370631/scala-reflect-2.11.8.jar" "__spark_libs__/scala-reflect-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372889/jetty-util-6.1.26.jar" "__spark_libs__/jetty-util-6.1.26.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370639/jaxb-api-2.2.2.jar" "__spark_libs__/jaxb-api-2.2.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372225/hk2-utils-2.4.0-b34.jar" "__spark_libs__/hk2-utils-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372153/javax.ws.rs-api-2.0.1.jar" "__spark_libs__/javax.ws.rs-api-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372899/json-20090211.jar" "__spark_libs__/json-20090211.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370608/spark-launcher_2.11-2.0.1.jar" "__spark_libs__/spark-launcher_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372210/json4s-ast_2.11-3.2.11.jar" "__spark_libs__/json4s-ast_2.11-3.2.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372909/hadoop-hdfs-2.7.1.jar" "__spark_libs__/hadoop-hdfs-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372168/lz4-1.3.0.jar" "__spark_libs__/lz4-1.3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372169/spark-mllib_2.11-2.0.1.jar" "__spark_libs__/spark-mllib_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372234/spire_2.11-0.7.4.jar" "__spark_libs__/spire_2.11-0.7.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372150/snappy-0.2.jar" "__spark_libs__/snappy-0.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370622/base64-2.3.8.jar" "__spark_libs__/base64-2.3.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372207/spark-graphx_2.11-2.0.1.jar" "__spark_libs__/spark-graphx_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372921/commons-math3-3.4.1.jar" "__spark_libs__/commons-math3-3.4.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370644/curator-client-2.6.0.jar" "__spark_libs__/curator-client-2.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372917/json4s-core_2.11-3.2.11.jar" "__spark_libs__/json4s-core_2.11-3.2.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372228/spark-hive-thriftserver_2.11-2.0.1.jar" "__spark_libs__/spark-hive-thriftserver_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370607/breeze_2.11-0.11.2.jar" "__spark_libs__/breeze_2.11-0.11.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/usercache/fairy/filecache/33842/stats-etlr-1.0.jar" "__app__.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372197/metrics-graphite-3.1.2.jar" "__spark_libs__/metrics-graphite-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372205/jackson-xc-1.9.13.jar" "__spark_libs__/jackson-xc-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372888/avro-mapred-1.7.7-hadoop2.jar" "__spark_libs__/avro-mapred-1.7.7-hadoop2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372203/xz-1.0.jar" "__spark_libs__/xz-1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370596/opencsv-2.3.jar" "__spark_libs__/opencsv-2.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
# Creating copy of launch script
cp "launch_container.sh" "/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000003/launch_container.sh"
chmod 640 "/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000003/launch_container.sh"
# Determining directory contents
echo "ls -l:" 1>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000003/directory.info"
ls -l 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000003/directory.info"
echo "find -L . -maxdepth 5 -ls:" 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000003/directory.info"
find -L . -maxdepth 5 -ls 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000003/directory.info"
echo "broken symlinks(find -L . -maxdepth 5 -type l -ls):" 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000003/directory.info"
find -L . -maxdepth 5 -type l -ls 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000003/directory.info"
exec /bin/bash -c "$JAVA_HOME/bin/java -server -Xmx3072m '-Dhdp.version=2.4.2.0-258' -Djava.io.tmpdir=$PWD/tmp '-Dspark.driver.port=39032' '-Dspark.history.ui.port=18080' '-Dspark.ui.port=0' -Dspark.yarn.app.container.log.dir=/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000003 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.60.43.16:39032 --executor-id 2 --hostname c402.hadoop.gda.lo --cores 1 --app-id application_1494395298335_303847 --user-class-path file:$PWD/__app__.jar 1> /hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000003/stdout 2> /hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000003/stderr"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
End of LogType:launch_container.sh

LogType:stderr
Log Upload Time:Tue May 16 11:12:19 +0700 2017
LogLength:32881
Log Contents:
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/hadoop/yarn/local/filecache/370610/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/hdp/2.4.2.0-258/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
17/05/16 11:11:04 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 8191@c402.hadoop.gda.lo
17/05/16 11:11:04 INFO util.SignalUtils: Registered signal handler for TERM
17/05/16 11:11:04 INFO util.SignalUtils: Registered signal handler for HUP
17/05/16 11:11:04 INFO util.SignalUtils: Registered signal handler for INT
17/05/16 11:11:05 INFO spark.SecurityManager: Changing view acls to: yarn,fairy
17/05/16 11:11:05 INFO spark.SecurityManager: Changing modify acls to: yarn,fairy
17/05/16 11:11:05 INFO spark.SecurityManager: Changing view acls groups to: 
17/05/16 11:11:05 INFO spark.SecurityManager: Changing modify acls groups to: 
17/05/16 11:11:05 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(yarn, fairy); groups with view permissions: Set(); users  with modify permissions: Set(yarn, fairy); groups with modify permissions: Set()
17/05/16 11:11:06 INFO client.TransportClientFactory: Successfully created connection to /10.60.43.16:39032 after 101 ms (0 ms spent in bootstraps)
17/05/16 11:11:06 INFO spark.SecurityManager: Changing view acls to: yarn,fairy
17/05/16 11:11:06 INFO spark.SecurityManager: Changing modify acls to: yarn,fairy
17/05/16 11:11:06 INFO spark.SecurityManager: Changing view acls groups to: 
17/05/16 11:11:06 INFO spark.SecurityManager: Changing modify acls groups to: 
17/05/16 11:11:06 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(yarn, fairy); groups with view permissions: Set(); users  with modify permissions: Set(yarn, fairy); groups with modify permissions: Set()
17/05/16 11:11:06 INFO client.TransportClientFactory: Successfully created connection to /10.60.43.16:39032 after 1 ms (0 ms spent in bootstraps)
17/05/16 11:11:06 INFO storage.DiskBlockManager: Created local directory at /hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/blockmgr-f969ffd4-80e8-455b-be99-ca1a4be7c808
17/05/16 11:11:07 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MB
17/05/16 11:11:07 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@10.60.43.16:39032
17/05/16 11:11:07 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver
17/05/16 11:11:07 INFO executor.Executor: Starting executor ID 2 on host c402.hadoop.gda.lo
17/05/16 11:11:07 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 20069.
17/05/16 11:11:07 INFO netty.NettyBlockTransferService: Server created on c402.hadoop.gda.lo:20069
17/05/16 11:11:07 INFO storage.BlockManager: external shuffle service port = 7337
17/05/16 11:11:07 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(2, c402.hadoop.gda.lo, 20069)
17/05/16 11:11:07 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(2, c402.hadoop.gda.lo, 20069)
17/05/16 11:11:07 INFO storage.BlockManager: Registering executor with local external shuffle service.
17/05/16 11:11:07 INFO client.TransportClientFactory: Successfully created connection to c402.hadoop.gda.lo/10.60.43.2:7337 after 2 ms (0 ms spent in bootstraps)
17/05/16 11:11:18 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 2
17/05/16 11:11:18 INFO executor.Executor: Running task 2.0 in stage 1.0 (TID 2)
17/05/16 11:11:18 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 0
17/05/16 11:11:18 INFO client.TransportClientFactory: Successfully created connection to /10.60.43.16:33694 after 3 ms (0 ms spent in bootstraps)
17/05/16 11:11:18 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.2 KB, free 1458.6 MB)
17/05/16 11:11:18 INFO broadcast.TorrentBroadcast: Reading broadcast variable 0 took 155 ms
17/05/16 11:11:18 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 30.3 KB, free 1458.6 MB)
17/05/16 11:11:18 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
17/05/16 11:11:18 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.60.43.16:39032)
17/05/16 11:11:18 INFO spark.MapOutputTrackerWorker: Got the output locations
17/05/16 11:11:18 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:18 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
17/05/16 11:11:18 INFO codegen.CodeGenerator: Code generated in 299.845777 ms
17/05/16 11:11:19 INFO codegen.CodeGenerator: Code generated in 15.681778 ms
17/05/16 11:11:19 INFO codegen.CodeGenerator: Code generated in 12.869192 ms
17/05/16 11:11:19 INFO codegen.CodeGenerator: Code generated in 9.982934 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 2.0 in stage 1.0 (TID 2). 4219 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 10
17/05/16 11:11:19 INFO executor.Executor: Running task 10.0 in stage 1.0 (TID 10)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 10.0 in stage 1.0 (TID 10). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 14
17/05/16 11:11:19 INFO executor.Executor: Running task 14.0 in stage 1.0 (TID 14)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 14.0 in stage 1.0 (TID 14). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 17
17/05/16 11:11:19 INFO executor.Executor: Running task 17.0 in stage 1.0 (TID 17)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 17.0 in stage 1.0 (TID 17). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 21
17/05/16 11:11:19 INFO executor.Executor: Running task 21.0 in stage 1.0 (TID 21)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 21.0 in stage 1.0 (TID 21). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 25
17/05/16 11:11:19 INFO executor.Executor: Running task 25.0 in stage 1.0 (TID 25)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 25.0 in stage 1.0 (TID 25). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 28
17/05/16 11:11:19 INFO executor.Executor: Running task 28.0 in stage 1.0 (TID 28)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 28.0 in stage 1.0 (TID 28). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 31
17/05/16 11:11:19 INFO executor.Executor: Running task 31.0 in stage 1.0 (TID 31)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 31.0 in stage 1.0 (TID 31). 3414 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 34
17/05/16 11:11:19 INFO executor.Executor: Running task 34.0 in stage 1.0 (TID 34)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 34.0 in stage 1.0 (TID 34). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 36
17/05/16 11:11:19 INFO executor.Executor: Running task 36.0 in stage 1.0 (TID 36)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 36.0 in stage 1.0 (TID 36). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 40
17/05/16 11:11:19 INFO executor.Executor: Running task 40.0 in stage 1.0 (TID 40)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 40.0 in stage 1.0 (TID 40). 3414 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 43
17/05/16 11:11:19 INFO executor.Executor: Running task 43.0 in stage 1.0 (TID 43)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 43.0 in stage 1.0 (TID 43). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 46
17/05/16 11:11:19 INFO executor.Executor: Running task 46.0 in stage 1.0 (TID 46)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 46.0 in stage 1.0 (TID 46). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 48
17/05/16 11:11:19 INFO executor.Executor: Running task 48.0 in stage 1.0 (TID 48)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 48.0 in stage 1.0 (TID 48). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 51
17/05/16 11:11:19 INFO executor.Executor: Running task 51.0 in stage 1.0 (TID 51)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 51.0 in stage 1.0 (TID 51). 3414 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 53
17/05/16 11:11:19 INFO executor.Executor: Running task 53.0 in stage 1.0 (TID 53)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 53.0 in stage 1.0 (TID 53). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 56
17/05/16 11:11:19 INFO executor.Executor: Running task 56.0 in stage 1.0 (TID 56)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 56.0 in stage 1.0 (TID 56). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 59
17/05/16 11:11:19 INFO executor.Executor: Running task 59.0 in stage 1.0 (TID 59)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 59.0 in stage 1.0 (TID 59). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 62
17/05/16 11:11:19 INFO executor.Executor: Running task 62.0 in stage 1.0 (TID 62)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 62.0 in stage 1.0 (TID 62). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 65
17/05/16 11:11:19 INFO executor.Executor: Running task 65.0 in stage 1.0 (TID 65)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 65.0 in stage 1.0 (TID 65). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 67
17/05/16 11:11:19 INFO executor.Executor: Running task 67.0 in stage 1.0 (TID 67)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 67.0 in stage 1.0 (TID 67). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 70
17/05/16 11:11:19 INFO executor.Executor: Running task 70.0 in stage 1.0 (TID 70)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 70.0 in stage 1.0 (TID 70). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 74
17/05/16 11:11:19 INFO executor.Executor: Running task 74.0 in stage 1.0 (TID 74)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 74.0 in stage 1.0 (TID 74). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 78
17/05/16 11:11:19 INFO executor.Executor: Running task 78.0 in stage 1.0 (TID 78)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 78.0 in stage 1.0 (TID 78). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 82
17/05/16 11:11:19 INFO executor.Executor: Running task 82.0 in stage 1.0 (TID 82)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 82.0 in stage 1.0 (TID 82). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 84
17/05/16 11:11:19 INFO executor.Executor: Running task 84.0 in stage 1.0 (TID 84)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 84.0 in stage 1.0 (TID 84). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 88
17/05/16 11:11:19 INFO executor.Executor: Running task 88.0 in stage 1.0 (TID 88)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 88.0 in stage 1.0 (TID 88). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 92
17/05/16 11:11:19 INFO executor.Executor: Running task 92.0 in stage 1.0 (TID 92)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 92.0 in stage 1.0 (TID 92). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 94
17/05/16 11:11:19 INFO executor.Executor: Running task 94.0 in stage 1.0 (TID 94)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 94.0 in stage 1.0 (TID 94). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 98
17/05/16 11:11:19 INFO executor.Executor: Running task 98.0 in stage 1.0 (TID 98)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 98.0 in stage 1.0 (TID 98). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 101
17/05/16 11:11:19 INFO executor.Executor: Running task 101.0 in stage 1.0 (TID 101)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 101.0 in stage 1.0 (TID 101). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 105
17/05/16 11:11:19 INFO executor.Executor: Running task 105.0 in stage 1.0 (TID 105)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 105.0 in stage 1.0 (TID 105). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 108
17/05/16 11:11:19 INFO executor.Executor: Running task 108.0 in stage 1.0 (TID 108)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 108.0 in stage 1.0 (TID 108). 3414 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 112
17/05/16 11:11:19 INFO executor.Executor: Running task 112.0 in stage 1.0 (TID 112)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 112.0 in stage 1.0 (TID 112). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 115
17/05/16 11:11:19 INFO executor.Executor: Running task 115.0 in stage 1.0 (TID 115)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 115.0 in stage 1.0 (TID 115). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 117
17/05/16 11:11:19 INFO executor.Executor: Running task 117.0 in stage 1.0 (TID 117)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 117.0 in stage 1.0 (TID 117). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 121
17/05/16 11:11:19 INFO executor.Executor: Running task 121.0 in stage 1.0 (TID 121)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 121.0 in stage 1.0 (TID 121). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 123
17/05/16 11:11:19 INFO executor.Executor: Running task 123.0 in stage 1.0 (TID 123)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 123.0 in stage 1.0 (TID 123). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 126
17/05/16 11:11:19 INFO executor.Executor: Running task 126.0 in stage 1.0 (TID 126)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 126.0 in stage 1.0 (TID 126). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 130
17/05/16 11:11:19 INFO executor.Executor: Running task 130.0 in stage 1.0 (TID 130)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 130.0 in stage 1.0 (TID 130). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 133
17/05/16 11:11:19 INFO executor.Executor: Running task 133.0 in stage 1.0 (TID 133)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 133.0 in stage 1.0 (TID 133). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 136
17/05/16 11:11:19 INFO executor.Executor: Running task 136.0 in stage 1.0 (TID 136)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 136.0 in stage 1.0 (TID 136). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 140
17/05/16 11:11:20 INFO executor.Executor: Running task 140.0 in stage 1.0 (TID 140)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 140.0 in stage 1.0 (TID 140). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 145
17/05/16 11:11:20 INFO executor.Executor: Running task 145.0 in stage 1.0 (TID 145)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 145.0 in stage 1.0 (TID 145). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 147
17/05/16 11:11:20 INFO executor.Executor: Running task 147.0 in stage 1.0 (TID 147)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 147.0 in stage 1.0 (TID 147). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 154
17/05/16 11:11:20 INFO executor.Executor: Running task 154.0 in stage 1.0 (TID 154)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 154.0 in stage 1.0 (TID 154). 3414 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 157
17/05/16 11:11:20 INFO executor.Executor: Running task 157.0 in stage 1.0 (TID 157)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 157.0 in stage 1.0 (TID 157). 3414 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 163
17/05/16 11:11:20 INFO executor.Executor: Running task 163.0 in stage 1.0 (TID 163)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 163.0 in stage 1.0 (TID 163). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 167
17/05/16 11:11:20 INFO executor.Executor: Running task 167.0 in stage 1.0 (TID 167)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 167.0 in stage 1.0 (TID 167). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 172
17/05/16 11:11:20 INFO executor.Executor: Running task 172.0 in stage 1.0 (TID 172)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 172.0 in stage 1.0 (TID 172). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 176
17/05/16 11:11:20 INFO executor.Executor: Running task 176.0 in stage 1.0 (TID 176)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 176.0 in stage 1.0 (TID 176). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 182
17/05/16 11:11:20 INFO executor.Executor: Running task 182.0 in stage 1.0 (TID 182)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 182.0 in stage 1.0 (TID 182). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 186
17/05/16 11:11:20 INFO executor.Executor: Running task 186.0 in stage 1.0 (TID 186)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 186.0 in stage 1.0 (TID 186). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 190
17/05/16 11:11:20 INFO executor.Executor: Running task 190.0 in stage 1.0 (TID 190)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 190.0 in stage 1.0 (TID 190). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 192
17/05/16 11:11:20 INFO executor.Executor: Running task 192.0 in stage 1.0 (TID 192)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 192.0 in stage 1.0 (TID 192). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 196
17/05/16 11:11:20 INFO executor.Executor: Running task 196.0 in stage 1.0 (TID 196)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 196.0 in stage 1.0 (TID 196). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 199
17/05/16 11:11:20 INFO executor.Executor: Running task 199.0 in stage 1.0 (TID 199)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 199.0 in stage 1.0 (TID 199). 3327 bytes result sent to driver
17/05/16 11:12:17 INFO executor.CoarseGrainedExecutorBackend: Driver commanded a shutdown
17/05/16 11:12:17 INFO memory.MemoryStore: MemoryStore cleared
17/05/16 11:12:18 INFO storage.BlockManager: BlockManager stopped
17/05/16 11:12:18 INFO executor.CoarseGrainedExecutorBackend: Driver from 10.60.43.16:39032 disconnected during shutdown
17/05/16 11:12:18 INFO executor.CoarseGrainedExecutorBackend: Driver from 10.60.43.16:39032 disconnected during shutdown
17/05/16 11:12:18 INFO util.ShutdownHookManager: Shutdown hook called
End of LogType:stderr

LogType:stdout
Log Upload Time:Tue May 16 11:12:19 +0700 2017
LogLength:0
Log Contents:
End of LogType:stdout



Container: container_e81_1494395298335_303847_01_000007 on c402.hadoop.gda.lo_45454
=====================================================================================
LogType:directory.info
Log Upload Time:Tue May 16 11:12:19 +0700 2017
LogLength:28872
Log Contents:
ls -l:
total 112
lrwxrwxrwx 1 yarn hadoop    69 May 16 11:10 __app__.jar -> /hadoop/yarn/local/usercache/fairy/filecache/33842/stats-etlr-1.0.jar
-rw-r--r-- 1 yarn hadoop    87 May 16 11:10 container_tokens
-rwx------ 1 yarn hadoop   676 May 16 11:10 default_container_executor_session.sh
-rwx------ 1 yarn hadoop   730 May 16 11:10 default_container_executor.sh
-rwx------ 1 yarn hadoop 75972 May 16 11:10 launch_container.sh
lrwxrwxrwx 1 yarn hadoop    69 May 16 11:10 __spark_conf__ -> /hadoop/yarn/local/usercache/fairy/filecache/33843/__spark_conf__.zip
drwxr-xr-x 2 yarn hadoop 12288 May 16 11:10 __spark_libs__
drwx--x--- 2 yarn hadoop  4096 May 16 11:10 tmp
find -L . -maxdepth 5 -ls:
8127316    4 drwx--x---   4 yarn     hadoop       4096 May 16 11:10 .
8127320    4 -rw-r--r--   1 yarn     hadoop         12 May 16 11:10 ./.container_tokens.crc
8127323    4 -rwx------   1 yarn     hadoop        676 May 16 11:10 ./default_container_executor_session.sh
8127321   76 -rwx------   1 yarn     hadoop      75972 May 16 11:10 ./launch_container.sh
8127319    4 -rw-r--r--   1 yarn     hadoop         87 May 16 11:10 ./container_tokens
8127324    4 -rw-r--r--   1 yarn     hadoop         16 May 16 11:10 ./.default_container_executor_session.sh.crc
8127326    4 -rw-r--r--   1 yarn     hadoop         16 May 16 11:10 ./.default_container_executor.sh.crc
8127327   12 drwxr-xr-x   2 yarn     hadoop      12288 May 16 11:10 ./__spark_libs__
17171817 2780 -rwxr-xr-x   1 yarn     hadoop    2842667 Mar 29 19:40 ./__spark_libs__/bcprov-jdk15on-1.51.jar
16910592  224 -rwxr-xr-x   1 yarn     hadoop     227712 Apr  5 13:33 ./__spark_libs__/libthrift-0.9.2.jar
16515644   24 -rwxr-xr-x   1 yarn     hadoop      23346 Apr  4 14:45 ./__spark_libs__/stax-api-1.0-2.jar
16516250   80 -rwxr-xr-x   1 yarn     hadoop      79912 Apr  4 14:45 ./__spark_libs__/api-util-1.0.0-M20.jar
16910721    8 -rwxr-xr-x   1 yarn     hadoop       5950 Apr  5 13:33 ./__spark_libs__/javax.inject-2.4.0-b34.jar
17172813  280 -rwxr-xr-x   1 yarn     hadoop     284220 Mar 29 19:40 ./__spark_libs__/commons-lang-2.6.jar
17046772  256 -rwxr-xr-x   1 yarn     hadoop     258876 Apr  5 13:33 ./__spark_libs__/jackson-core-2.6.5.jar
16910724 1444 -rwxr-xr-x   1 yarn     hadoop    1475955 Apr  5 13:33 ./__spark_libs__/htrace-core-3.1.0-incubating.jar
16516253 2304 -rwxr-xr-x   1 yarn     hadoop    2355465 Apr  4 14:45 ./__spark_libs__/spark-network-common_2.11-2.0.1.jar
17172574  164 -rwxr-xr-x   1 yarn     hadoop     164422 Mar 29 19:40 ./__spark_libs__/core-1.1.2.jar
16515658  380 -rwxr-xr-x   1 yarn     hadoop     387188 Apr  4 14:45 ./__spark_libs__/parquet-format-2.3.0-incubating.jar
16515794   20 -rwxr-xr-x   1 yarn     hadoop      18336 Apr  4 14:45 ./__spark_libs__/jackson-jaxrs-1.9.13.jar
16515159  200 -rwxr-xr-x   1 yarn     hadoop     201124 Apr  4 15:15 ./__spark_libs__/jdo-api-3.0.1.jar
16516243 6144 -rwxr-xr-x   1 yarn     hadoop    6290315 Apr  4 14:45 ./__spark_libs__/spark-sql_2.11-2.0.1.jar
17046790   64 -rwxr-xr-x   1 yarn     hadoop      63316 Apr  5 13:33 ./__spark_libs__/spark-repl_2.11-2.0.1.jar
17172670   20 -rwxr-xr-x   1 yarn     hadoop      17008 Mar 29 19:40 ./__spark_libs__/base64-2.3.8.jar
17172687  436 -rwxr-xr-x   1 yarn     hadoop     442406 Mar 29 19:40 ./__spark_libs__/calcite-linq4j-1.2.0-incubating.jar
17172811   16 -rwxr-xr-x   1 yarn     hadoop      14766 Mar 29 19:40 ./__spark_libs__/aopalliance-repackaged-2.4.0-b34.jar
16516327  172 -rwxr-xr-x   1 yarn     hadoop     174351 Apr  4 14:45 ./__spark_libs__/stream-2.7.0.jar
17172723  524 -rwxr-xr-x   1 yarn     hadoop     533455 Mar 29 19:40 ./__spark_libs__/protobuf-java-2.5.0.jar
16516273 1144 -rwxr-xr-x   1 yarn     hadoop    1171380 Apr  4 14:45 ./__spark_libs__/jackson-databind-2.6.5.jar
17046968   32 -rwxr-xr-x   1 yarn     hadoop      29555 Apr  5 13:33 ./__spark_libs__/paranamer-2.3.jar
16910717   96 -rwxr-xr-x   1 yarn     hadoop      95806 Apr  5 13:33 ./__spark_libs__/javax.servlet-api-3.1.0.jar
17172564 1996 -rwxr-xr-x   1 yarn     hadoop    2041628 Mar 29 19:40 ./__spark_libs__/jets3t-0.9.3.jar
17172704 4468 -rwxr-xr-x   1 yarn     hadoop    4573750 Mar 29 19:40 ./__spark_libs__/scala-reflect-2.11.8.jar
17171510  104 -rwxr-xr-x   1 yarn     hadoop     105134 Mar 29 19:40 ./__spark_libs__/jaxb-api-2.2.2.jar
16516343  164 -rwxr-xr-x   1 yarn     hadoop     167421 Apr  4 14:45 ./__spark_libs__/jersey-client-2.22.2.jar
16515778  896 -rwxr-xr-x   1 yarn     hadoop     917052 Apr  4 14:45 ./__spark_libs__/parquet-column-1.7.0.jar
17172692 5612 -rwxr-xr-x   1 yarn     hadoop    5744974 Mar 29 19:40 ./__spark_libs__/scala-library-2.11.8.jar
17047135  144 -rwxr-xr-x   1 yarn     hadoop     144660 Apr  5 13:33 ./__spark_libs__/xbean-asm5-shaded-4.4.jar
16516297 1024 -rwxr-xr-x   1 yarn     hadoop    1044511 Apr  4 14:45 ./__spark_libs__/spark-hive_2.11-2.0.1.jar
17172662 1024 -rwxr-xr-x   1 yarn     hadoop    1045744 Mar 29 19:40 ./__spark_libs__/leveldbjni-all-1.8.jar
16515686  356 -rwxr-xr-x   1 yarn     hadoop     363908 Apr  4 14:45 ./__spark_libs__/hadoop-yarn-server-common-2.7.1.jar
16910745 15128 -rwxr-xr-x   1 yarn     hadoop   15487351 Apr  5 13:33 ./__spark_libs__/scala-compiler-2.11.8.jar
17172733   68 -rwxr-xr-x   1 yarn     hadoop      68866 Mar 29 19:40 ./__spark_libs__/curator-client-2.6.0.jar
17046870 1988 -rwxr-xr-x   1 yarn     hadoop    2035066 Apr  5 13:33 ./__spark_libs__/commons-math3-3.4.1.jar
17172617  764 -rwxr-xr-x   1 yarn     hadoop     780664 Mar 29 19:40 ./__spark_libs__/jackson-mapper-asl-1.9.13.jar
16910703  636 -rwxr-xr-x   1 yarn     hadoop     648678 Apr  5 13:33 ./__spark_libs__/scala-xml_2.11-1.0.2.jar
16516272   68 -rwxr-xr-x   1 yarn     hadoop      69409 Apr  4 14:45 ./__spark_libs__/activation-1.1.1.jar
17172676  180 -rwxr-xr-x   1 yarn     hadoop     181271 Mar 29 19:40 ./__spark_libs__/hk2-locator-2.4.0-b34.jar
16515693  576 -rwxr-xr-x   1 yarn     hadoop     588337 Apr  4 14:45 ./__spark_libs__/commons-collections-3.2.2.jar
16516044   40 -rwxr-xr-x   1 yarn     hadoop      38134 Apr  4 14:45 ./__spark_libs__/hadoop-mapreduce-client-jobclient-2.7.1.jar
16515663   12 -rwxr-xr-x   1 yarn     hadoop      12131 Apr  4 14:45 ./__spark_libs__/jpam-1.1.jar
17172581  228 -rwxr-xr-x   1 yarn     hadoop     232248 Mar 29 19:40 ./__spark_libs__/jackson-core-asl-1.9.13.jar
16515402  184 -rwxr-xr-x   1 yarn     hadoop     185245 Apr  4 14:45 ./__spark_libs__/curator-framework-2.6.0.jar
16515757   24 -rwxr-xr-x   1 yarn     hadoop      20852 Apr  4 14:45 ./__spark_libs__/metrics-graphite-3.1.2.jar
16910628  176 -rwxr-xr-x   1 yarn     hadoop     177131 Apr  5 13:33 ./__spark_libs__/jetty-util-6.1.26.jar
16515911  504 -rwxr-xr-x   1 yarn     hadoop     515604 Apr  4 14:45 ./__spark_libs__/jackson-module-scala_2.11-2.6.5.jar
16910586   48 -rwxr-xr-x   1 yarn     hadoop      46983 Apr  5 13:33 ./__spark_libs__/jackson-annotations-2.6.5.jar
16515638   48 -rwxr-xr-x   1 yarn     hadoop      48720 Apr  4 14:45 ./__spark_libs__/snappy-0.2.jar
16515895   84 -rwxr-xr-x   1 yarn     hadoop      82421 Apr  4 14:45 ./__spark_libs__/json4s-ast_2.11-3.2.11.jar
17172568  160 -rwxr-xr-x   1 yarn     hadoop     160519 Mar 29 19:40 ./__spark_libs__/commons-dbcp-1.4.jar
17171915   88 -rwxr-xr-x   1 yarn     hadoop      86811 Mar 29 19:40 ./__spark_libs__/spire-macros_2.11-0.7.4.jar
16515802   28 -rwxr-xr-x   1 yarn     hadoop      27084 Apr  4 14:45 ./__spark_libs__/jackson-xc-1.9.13.jar
17171944 1248 -rwxr-xr-x   1 yarn     hadoop    1277883 Mar 29 19:40 ./__spark_libs__/mesos-0.21.1-shaded-protobuf.jar
16515640  952 -rwxr-xr-x   1 yarn     hadoop     971310 Apr  4 14:45 ./__spark_libs__/jersey-guava-2.22.2.jar
16515679  404 -rwxr-xr-x   1 yarn     hadoop     412739 Apr  4 14:45 ./__spark_libs__/commons-lang3-3.3.2.jar
16515719  696 -rwxr-xr-x   1 yarn     hadoop     710492 Apr  4 14:45 ./__spark_libs__/guice-3.0.jar
16516307   28 -rwxr-xr-x   1 yarn     hadoop      26514 Apr  4 14:45 ./__spark_libs__/stax-api-1.0.1.jar
16516293  468 -rwxr-xr-x   1 yarn     hadoop     477970 Apr  4 14:45 ./__spark_libs__/lift-json_2.11-2.6.3.jar
16515904 3352 -rwxr-xr-x   1 yarn     hadoop    3431544 Apr  4 14:45 ./__spark_libs__/hadoop-common-2.7.1.jar
17172707 6716 -rwxr-xr-x   1 yarn     hadoop    6873892 Mar 29 19:40 ./__spark_libs__/spark-catalyst_2.11-2.0.1.jar
16515414   16 -rwxr-xr-x   1 yarn     hadoop      15010 Apr  4 14:45 ./__spark_libs__/xmlenc-0.52.jar
17046972  532 -rwxr-xr-x   1 yarn     hadoop     540852 Apr  5 13:33 ./__spark_libs__/mysql-connector-java-5.0.8-bin.jar
17046957 2732 -rwxr-xr-x   1 yarn     hadoop    2796935 Apr  5 13:33 ./__spark_libs__/parquet-hadoop-bundle-1.6.0.jar
16515706   12 -rwxr-xr-x   1 yarn     hadoop      10023 Apr  4 14:45 ./__spark_libs__/java-xmlbuilder-1.0.jar
16516259 1768 -rwxr-xr-x   1 yarn     hadoop    1809447 Apr  4 14:45 ./__spark_libs__/datanucleus-rdbms-3.2.9.jar
17172561 1592 -rwxr-xr-x   1 yarn     hadoop    1627065 Mar 29 19:40 ./__spark_libs__/jersey-bundle-1.19.1.jar
16910729   16 -rwxr-xr-x   1 yarn     hadoop      15305 Apr  5 13:33 ./__spark_libs__/spark-tags_2.11-2.0.1.jar
16515699  220 -rwxr-xr-x   1 yarn     hadoop     223573 Apr  4 14:45 ./__spark_libs__/chill_2.11-0.8.0.jar
17171497  184 -rwxr-xr-x   1 yarn     hadoop     185140 Mar 29 19:40 ./__spark_libs__/commons-io-2.4.jar
16516349  508 -rwxr-xr-x   1 yarn     hadoop     516127 Apr  4 14:45 ./__spark_libs__/hadoop-mapreduce-client-app-2.7.1.jar
16516256 3152 -rwxr-xr-x   1 yarn     hadoop    3224708 Apr  4 14:45 ./__spark_libs__/derby-10.12.1.1.jar
16516316   24 -rwxr-xr-x   1 yarn     hadoop      21243 Apr  4 14:45 ./__spark_libs__/parquet-generator-1.7.0.jar
16516303 1772 -rwxr-xr-x   1 yarn     hadoop    1814309 Apr  4 14:45 ./__spark_libs__/spark-hive-thriftserver_2.11-2.0.1.jar
17172647    4 -rwxr-xr-x   1 yarn     hadoop       2497 Mar 29 19:40 ./__spark_libs__/javax.inject-1.jar
16516295  120 -rwxr-xr-x   1 yarn     hadoop     118973 Apr  4 14:45 ./__spark_libs__/hk2-utils-2.4.0-b34.jar
17172688  720 -rwxr-xr-x   1 yarn     hadoop     736658 Mar 29 19:40 ./__spark_libs__/httpclient-4.5.2.jar
16516312   44 -rwxr-xr-x   1 yarn     hadoop      41123 Apr  4 14:45 ./__spark_libs__/commons-cli-1.2.jar
17046643  576 -rwxr-xr-x   1 yarn     hadoop     589462 Apr  5 13:33 ./__spark_libs__/json4s-core_2.11-3.2.11.jar
17172894  100 -rwxr-xr-x   1 yarn     hadoop     100636 Mar 29 19:40 ./__spark_libs__/jsp-api-2.1.jar
17172576  176 -rwxr-xr-x   1 yarn     hadoop     178947 Mar 29 19:40 ./__spark_libs__/hk2-api-2.4.0-b34.jar
16515664  480 -rwxr-xr-x   1 yarn     hadoop     489884 Apr  4 14:45 ./__spark_libs__/log4j-1.2.17.jar
16910654   44 -rwxr-xr-x   1 yarn     hadoop      44925 Apr  5 13:33 ./__spark_libs__/apacheds-i18n-2.0.0-M15.jar
16516202    8 -rwxr-xr-x   1 yarn     hadoop       5711 Apr  4 14:45 ./__spark_libs__/minlog-1.3.0.jar
17172860  148 -rwxr-xr-x   1 yarn     hadoop     148627 Mar 29 19:40 ./__spark_libs__/stringtemplate-3.2.1.jar
17173108   44 -rwxr-xr-x   1 yarn     hadoop      41070 Mar 29 19:40 ./__spark_libs__/spark-unsafe_2.11-2.0.1.jar
17172658  232 -rwxr-xr-x   1 yarn     hadoop     236660 Mar 29 19:40 ./__spark_libs__/ST4-4.0.4.jar
16515652  116 -rwxr-xr-x   1 yarn     hadoop     114913 Apr  4 14:45 ./__spark_libs__/py4j-0.10.3.jar
17172593  600 -rwxr-xr-x   1 yarn     hadoop     613299 Mar 29 19:40 ./__spark_libs__/janino-2.7.8.jar
16516301  400 -rwxr-xr-x   1 yarn     hadoop     409467 Apr  4 14:45 ./__spark_libs__/mx4j-3.0.2.jar
16516239  208 -rwxr-xr-x   1 yarn     hadoop     212453 Apr  4 14:45 ./__spark_libs__/commons-net-2.2.jar
16515678   72 -rwxr-xr-x   1 yarn     hadoop      70688 Apr  4 14:45 ./__spark_libs__/hadoop-auth-2.7.1.jar
17172699  256 -rwxr-xr-x   1 yarn     hadoop     258370 Mar 29 19:40 ./__spark_libs__/calcite-avatica-1.2.0-incubating.jar
16515726  700 -rwxr-xr-x   1 yarn     hadoop     714194 Apr  4 14:45 ./__spark_libs__/javassist-3.18.1-GA.jar
16516289  200 -rwxr-xr-x   1 yarn     hadoop     201928 Apr  4 14:45 ./__spark_libs__/RoaringBitmap-0.5.11.jar
16515169    8 -rwxr-xr-x   1 yarn     hadoop       4596 Apr  4 15:15 ./__spark_libs__/jul-to-slf4j-1.7.16.jar
16515396  280 -rwxr-xr-x   1 yarn     hadoop     285447 Apr  4 14:45 ./__spark_libs__/parquet-encoding-1.7.0.jar
17172673  528 -rwxr-xr-x   1 yarn     hadoop     539912 Mar 29 19:40 ./__spark_libs__/jetty-6.1.26.jar
17172804  332 -rwxr-xr-x   1 yarn     hadoop     339666 Mar 29 19:40 ./__spark_libs__/datanucleus-api-jdo-3.2.6.jar
17046852   40 -rwxr-xr-x   1 yarn     hadoop      39280 Apr  5 13:33 ./__spark_libs__/metrics-jvm-3.1.2.jar
17172716   32 -rwxr-xr-x   1 yarn     hadoop      29540 Mar 29 19:40 ./__spark_libs__/spark-sketch_2.11-2.0.1.jar
17172579  788 -rwxr-xr-x   1 yarn     hadoop     802818 Mar 29 19:40 ./__spark_libs__/scalap-2.11.8.jar
16910640   92 -rwxr-xr-x   1 yarn     hadoop      93210 Apr  5 13:33 ./__spark_libs__/super-csv-2.2.0.jar
16516206   28 -rwxr-xr-x   1 yarn     hadoop      26366 Apr  4 14:45 ./__spark_libs__/javax.annotation-api-1.2.jar
17172667    8 -rwxr-xr-x   1 yarn     hadoop       4467 Mar 29 19:40 ./__spark_libs__/aopalliance-1.0.jar
16910697   16 -rwxr-xr-x   1 yarn     hadoop      15827 Apr  5 13:33 ./__spark_libs__/metrics-json-3.1.2.jar
17171489  164 -rwxr-xr-x   1 yarn     hadoop     164368 Mar 29 19:40 ./__spark_libs__/antlr-runtime-3.4.jar
17047069   68 -rwxr-xr-x   1 yarn     hadoop      66270 Apr  5 13:33 ./__spark_libs__/jersey-container-servlet-core-2.22.2.jar
16515766   64 -rwxr-xr-x   1 yarn     hadoop      62050 Apr  4 14:45 ./__spark_libs__/commons-logging-1.1.3.jar
16910646 3440 -rwxr-xr-x   1 yarn     hadoop    3519262 Apr  5 13:33 ./__spark_libs__/calcite-core-1.2.0-incubating.jar
17172590   20 -rwxr-xr-x   1 yarn     hadoop      18098 Mar 29 19:40 ./__spark_libs__/jersey-container-servlet-2.22.2.jar
16515879  212 -rwxr-xr-x   1 yarn     hadoop     213911 Apr  4 14:45 ./__spark_libs__/jline-2.12.1.jar
16515777  232 -rwxr-xr-x   1 yarn     hadoop     236880 Apr  4 14:45 ./__spark_libs__/lz4-1.3.0.jar
17046677  188 -rwxr-xr-x   1 yarn     hadoop     188671 Apr  5 13:33 ./__spark_libs__/commons-beanutils-1.7.0.jar
16910707   48 -rwxr-xr-x   1 yarn     hadoop      45944 Apr  5 13:33 ./__spark_libs__/json-20090211.jar
13371520   44 -rwxr-xr-x   1 yarn     hadoop      41263 Mar 29 19:40 ./__spark_libs__/jackson-module-paranamer-2.6.5.jar
16515807   44 -rwxr-xr-x   1 yarn     hadoop      45015 Apr  4 14:45 ./__spark_libs__/hadoop-mapreduce-client-shuffle-2.7.1.jar
17172621 13136 -rwxr-xr-x   1 yarn     hadoop   13448966 Mar 29 19:40 ./__spark_libs__/breeze_2.11-0.11.2.jar
17172562  112 -rwxr-xr-x   1 yarn     hadoop     110600 Mar 29 19:40 ./__spark_libs__/bonecp-0.8.0.RELEASE.jar
17172630   12 -rwxr-xr-x   1 yarn     hadoop       9939 Mar 29 19:40 ./__spark_libs__/slf4j-log4j12-1.7.16.jar
16516235 1204 -rwxr-xr-x   1 yarn     hadoop    1229125 Apr  4 14:45 ./__spark_libs__/xercesImpl-2.9.1.jar
17172655   80 -rwxr-xr-x   1 yarn     hadoop      79845 Mar 29 19:40 ./__spark_libs__/compress-lzf-1.0.3.jar
16910754   40 -rwxr-xr-x   1 yarn     hadoop      40341 Apr  5 13:33 ./__spark_libs__/json4s-jackson_2.11-3.2.11.jar
16910705   40 -rwxr-xr-x   1 yarn     hadoop      40817 Apr  5 13:33 ./__spark_libs__/hive-cli-1.2.1.spark2.jar
13371506  244 -rwxr-xr-x   1 yarn     hadoop     248171 Mar 29 19:40 ./__spark_libs__/curator-recipes-2.6.0.jar
16515703  676 -rwxr-xr-x   1 yarn     hadoop     691479 Apr  4 14:45 ./__spark_libs__/apacheds-kerberos-codec-2.0.0-M15.jar
16516222 11452 -rwxr-xr-x   1 yarn     hadoop   11723537 Apr  4 14:45 ./__spark_libs__/spark-core_2.11-2.0.1.jar
16515732   20 -rwxr-xr-x   1 yarn     hadoop      18482 Apr  4 14:45 ./__spark_libs__/eigenbase-properties-1.1.5.jar
17172762   20 -rwxr-xr-x   1 yarn     hadoop      17385 Mar 29 19:40 ./__spark_libs__/hadoop-annotations-2.7.1.jar
16910711    4 -rwxr-xr-x   1 yarn     hadoop       2545 Apr  5 13:33 ./__spark_libs__/hadoop-client-2.7.1.jar
16910699   56 -rwxr-xr-x   1 yarn     hadoop      55511 Apr  5 13:33 ./__spark_libs__/spark-network-shuffle_2.11-2.0.1.jar
16515869  668 -rwxr-xr-x   1 yarn     hadoop     680106 Apr  4 14:45 ./__spark_libs__/spark-graphx_2.11-2.0.1.jar
16516034   16 -rwxr-xr-x   1 yarn     hadoop      15071 Apr  4 14:45 ./__spark_libs__/jta-1.1.jar
17046978   20 -rwxr-xr-x   1 yarn     hadoop      16993 Apr  5 13:33 ./__spark_libs__/JavaEWAH-0.3.2.jar
16910738 8068 -rwxr-xr-x   1 yarn     hadoop    8260573 Apr  5 13:33 ./__spark_libs__/hadoop-hdfs-2.7.1.jar
16515742 1032 -rwxr-xr-x   1 yarn     hadoop    1056168 Apr  4 14:45 ./__spark_libs__/snappy-java-1.1.2.6.jar
16515739  320 -rwxr-xr-x   1 yarn     hadoop     326724 Apr  4 14:45 ./__spark_libs__/httpcore-4.4.4.jar
17172636 1256 -rwxr-xr-x   1 yarn     hadoop    1282424 Mar 29 19:40 ./__spark_libs__/ivy-2.4.0.jar
16910742 2044 -rwxr-xr-x   1 yarn     hadoop    2090370 Apr  5 13:33 ./__spark_libs__/spark-streaming_2.11-2.0.1.jar
16910751   20 -rwxr-xr-x   1 yarn     hadoop      16430 Apr  5 13:33 ./__spark_libs__/jcl-over-slf4j-1.7.16.jar
16910600  204 -rwxr-xr-x   1 yarn     hadoop     206035 Apr  5 13:33 ./__spark_libs__/commons-beanutils-core-1.8.0.jar
16516262  616 -rwxr-xr-x   1 yarn     hadoop     627814 Apr  4 14:45 ./__spark_libs__/joda-time-2.9.3.jar
16516070 2140 -rwxr-xr-x   1 yarn     hadoop    2189117 Apr  4 14:45 ./__spark_libs__/guava-14.0.1.jar
16515670  100 -rwxr-xr-x   1 yarn     hadoop     100680 Apr  4 14:45 ./__spark_libs__/hive-jdbc-1.2.1.spark2.jar
17173079 5380 -rwxr-xr-x   1 yarn     hadoop    5505200 Mar 29 19:40 ./__spark_libs__/hive-metastore-1.2.1.spark2.jar
16515629  684 -rwxr-xr-x   1 yarn     hadoop     698375 Apr  4 14:45 ./__spark_libs__/jersey-common-2.22.2.jar
16516192  300 -rwxr-xr-x   1 yarn     hadoop     305001 Apr  4 14:45 ./__spark_libs__/commons-httpclient-3.1.jar
17172713   72 -rwxr-xr-x   1 yarn     hadoop      72733 Mar 29 19:40 ./__spark_libs__/jersey-media-jaxb-2.22.2.jar
17172600   92 -rwxr-xr-x   1 yarn     hadoop      93407 Mar 29 19:40 ./__spark_libs__/pyrolite-4.9.jar
17172625   68 -rwxr-xr-x   1 yarn     hadoop      65653 Mar 29 19:40 ./__spark_libs__/spark-launcher_2.11-2.0.1.jar
17172682  192 -rwxr-xr-x   1 yarn     hadoop     192993 Mar 29 19:40 ./__spark_libs__/avro-ipc-1.7.7.jar
16516345  736 -rwxr-xr-x   1 yarn     hadoop     753012 Apr  4 14:45 ./__spark_libs__/hadoop-mapreduce-client-common-2.7.1.jar
17172628   32 -rwxr-xr-x   1 yarn     hadoop      32145 Mar 29 19:40 ./__spark_libs__/hadoop-yarn-server-web-proxy-2.7.1.jar
16910748  428 -rwxr-xr-x   1 yarn     hadoop     436303 Apr  5 13:33 ./__spark_libs__/avro-1.7.7.jar
17172585  144 -rwxr-xr-x   1 yarn     hadoop     143602 Mar 29 19:40 ./__spark_libs__/commons-digester-1.8.jar
16515647  116 -rwxr-xr-x   1 yarn     hadoop     115534 Apr  4 14:45 ./__spark_libs__/javax.ws.rs-api-2.0.1.jar
17172643   52 -rwxr-xr-x   1 yarn     hadoop      50619 Mar 29 19:40 ./__spark_libs__/chill-java-0.8.0.jar
16515953 1168 -rwxr-xr-x   1 yarn     hadoop    1194003 Apr  4 14:45 ./__spark_libs__/arpack_combined_all-0.1.jar
17172588   20 -rwxr-xr-x   1 yarn     hadoop      19827 Mar 29 19:40 ./__spark_libs__/opencsv-2.3.jar
16516278 7108 -rwxr-xr-x   1 yarn     hadoop    7276083 Apr  4 14:45 ./__spark_libs__/spire_2.11-0.7.4.jar
17171485  436 -rwxr-xr-x   1 yarn     hadoop     445288 Mar 29 19:40 ./__spark_libs__/antlr-2.7.7.jar
16515710  352 -rwxr-xr-x   1 yarn     hadoop     358390 Apr  4 14:45 ./__spark_libs__/kryo-shaded-3.0.3.jar
16515649 1616 -rwxr-xr-x   1 yarn     hadoop    1654097 Apr  4 14:45 ./__spark_libs__/hadoop-yarn-common-2.7.1.jar
17172615  656 -rwxr-xr-x   1 yarn     hadoop     669589 Mar 29 19:40 ./__spark_libs__/spark-yarn_2.11-2.0.1.jar
16910591 1512 -rwxr-xr-x   1 yarn     hadoop    1544875 Apr  5 13:33 ./__spark_libs__/hadoop-mapreduce-client-core-2.7.1.jar
16515466   32 -rwxr-xr-x   1 yarn     hadoop      30595 Apr  4 14:45 ./__spark_libs__/commons-compiler-2.7.6.jar
16910756  640 -rwxr-xr-x   1 yarn     hadoop     654216 Apr  5 13:33 ./__spark_libs__/pmml-model-1.2.15.jar
16515825 5712 -rwxr-xr-x   1 yarn     hadoop    5847591 Apr  4 14:45 ./__spark_libs__/spark-mllib_2.11-2.0.1.jar
17047157  932 -rwxr-xr-x   1 yarn     hadoop     951701 Apr  5 13:33 ./__spark_libs__/jersey-server-2.22.2.jar
17047013  176 -rwxr-xr-x   1 yarn     hadoop     177832 Apr  5 13:33 ./__spark_libs__/spark-mllib-local_2.11-2.0.1.jar
16515773  512 -rwxr-xr-x   1 yarn     hadoop     521157 Apr  4 14:45 ./__spark_libs__/mail-1.4.7.jar
16515782 1024 -rwxr-xr-x   1 yarn     hadoop    1048110 Apr  4 14:45 ./__spark_libs__/parquet-jackson-1.7.0.jar
17172695  236 -rwxr-xr-x   1 yarn     hadoop     241367 Mar 29 19:40 ./__spark_libs__/commons-compress-1.4.1.jar
16516280  140 -rwxr-xr-x   1 yarn     hadoop     142631 Apr  4 14:45 ./__spark_libs__/hadoop-yarn-client-2.7.1.jar
16515790   96 -rwxr-xr-x   1 yarn     hadoop      94672 Apr  4 14:45 ./__spark_libs__/xz-1.0.jar
17172651  208 -rwxr-xr-x   1 yarn     hadoop     209622 Mar 29 19:40 ./__spark_libs__/parquet-hadoop-1.7.0.jar
16516276  420 -rwxr-xr-x   1 yarn     hadoop     427780 Apr  4 14:45 ./__spark_libs__/jodd-core-3.5.2.jar
16515752   20 -rwxr-xr-x   1 yarn     hadoop      20235 Apr  4 14:45 ./__spark_libs__/osgi-resource-locator-1.0.1.jar
16516247  136 -rwxr-xr-x   1 yarn     hadoop     138464 Apr  4 14:45 ./__spark_libs__/hive-beeline-1.2.1.spark2.jar
16910596  112 -rwxr-xr-x   1 yarn     hadoop     112558 Apr  5 13:33 ./__spark_libs__/metrics-core-3.1.2.jar
17172596  136 -rwxr-xr-x   1 yarn     hadoop     135552 Mar 29 19:40 ./__spark_libs__/breeze-macros_2.11-0.11.2.jar
16910627  180 -rwxr-xr-x   1 yarn     hadoop     180736 Apr  5 13:33 ./__spark_libs__/avro-mapred-1.7.7-hadoop2.jar
16516266  440 -rwxr-xr-x   1 yarn     hadoop     448794 Apr  4 14:45 ./__spark_libs__/apache-log4j-extras-1.2.17.jar
17046657  188 -rwxr-xr-x   1 yarn     hadoop     190432 Apr  5 13:33 ./__spark_libs__/gson-2.2.4.jar
16910733   44 -rwxr-xr-x   1 yarn     hadoop      41755 Apr  5 13:33 ./__spark_libs__/objenesis-2.1.jar
16516340 2008 -rwxr-xr-x   1 yarn     hadoop    2054931 Apr  4 14:45 ./__spark_libs__/netty-all-4.0.29.Final.jar
16515406   36 -rwxr-xr-x   1 yarn     hadoop      33015 Apr  4 14:45 ./__spark_libs__/jsr305-1.3.9.jar
16515872 1972 -rwxr-xr-x   1 yarn     hadoop    2015514 Apr  4 14:45 ./__spark_libs__/hadoop-yarn-api-2.7.1.jar
16910651   64 -rwxr-xr-x   1 yarn     hadoop      65012 Apr  5 13:33 ./__spark_libs__/guice-servlet-3.0.jar
16516352    8 -rwxr-xr-x   1 yarn     hadoop       5310 Apr  4 14:45 ./__spark_libs__/pmml-schema-1.2.15.jar
16910584   64 -rwxr-xr-x   1 yarn     hadoop      63777 Apr  5 13:33 ./__spark_libs__/validation-api-1.1.0.Final.jar
16516286  388 -rwxr-xr-x   1 yarn     hadoop     395195 Apr  4 14:45 ./__spark_libs__/javolution-5.5.1.jar
16515462  748 -rwxr-xr-x   1 yarn     hadoop     764569 Apr  4 14:45 ./__spark_libs__/jtransforms-2.4.0.jar
16515684   24 -rwxr-xr-x   1 yarn     hadoop      21575 Apr  4 14:45 ./__spark_libs__/parquet-common-1.7.0.jar
16908289  776 -rwxr-xr-x   1 yarn     hadoop     792964 Apr  5 13:33 ./__spark_libs__/zookeeper-3.4.6.jar
16515635   20 -rwxr-xr-x   1 yarn     hadoop      16560 Apr  4 14:45 ./__spark_libs__/api-asn1-api-1.0.0-M20.jar
17173117  416 -rwxr-xr-x   1 yarn     hadoop     423753 Mar 29 19:40 ./__spark_libs__/scala-parser-combinators_2.11-1.0.4.jar
17172572  284 -rwxr-xr-x   1 yarn     hadoop     290506 Mar 29 19:40 ./__spark_libs__/univocity-parsers-2.1.1.jar
16515655  292 -rwxr-xr-x   1 yarn     hadoop     298829 Apr  4 14:45 ./__spark_libs__/commons-configuration-1.6.jar
17172661 11232 -rwxr-xr-x   1 yarn     hadoop   11498852 Mar 29 19:40 ./__spark_libs__/hive-exec-1.2.1.spark2.jar
17171947 1204 -rwxr-xr-x   1 yarn     hadoop    1230201 Mar 29 19:40 ./__spark_libs__/netty-3.8.0.Final.jar
16910726   96 -rwxr-xr-x   1 yarn     hadoop      96221 Apr  5 13:33 ./__spark_libs__/commons-pool-1.5.4.jar
17172612   40 -rwxr-xr-x   1 yarn     hadoop      40509 Mar 29 19:40 ./__spark_libs__/slf4j-api-1.7.16.jar
16516331  296 -rwxr-xr-x   1 yarn     hadoop     302248 Apr  4 14:45 ./__spark_libs__/antlr4-runtime-4.5.3.jar
17172608  280 -rwxr-xr-x   1 yarn     hadoop     284184 Mar 29 19:40 ./__spark_libs__/commons-codec-1.10.jar
16516337  308 -rwxr-xr-x   1 yarn     hadoop     313686 Apr  4 14:45 ./__spark_libs__/libfb303-0.9.2.jar
17172604 1848 -rwxr-xr-x   1 yarn     hadoop    1890075 Mar 29 19:40 ./__spark_libs__/datanucleus-core-3.2.10.jar
16515723   64 -rwxr-xr-x   1 yarn     hadoop      65261 Apr  4 14:45 ./__spark_libs__/oro-2.0.8.jar
17566958 1152 -r-x------   1 yarn     hadoop    1178454 May 16 11:10 ./__app__.jar
8127317    4 drwx--x---   2 yarn     hadoop       4096 May 16 11:10 ./tmp
8127325    4 -rwx------   1 yarn     hadoop        730 May 16 11:10 ./default_container_executor.sh
17567000    4 drwx------   2 yarn     hadoop       4096 May 16 11:10 ./__spark_conf__
17567028    4 -r-x------   1 yarn     hadoop        229 May 16 11:10 ./__spark_conf__/slaves
17567027    4 -r-x------   1 yarn     hadoop       1308 May 16 11:10 ./__spark_conf__/hadoop-policy.xml
17567030    4 -r-x------   1 yarn     hadoop        951 May 16 11:10 ./__spark_conf__/mapred-env.cmd
17567035    4 -r-x------   1 yarn     hadoop        945 May 16 11:10 ./__spark_conf__/taskcontroller.cfg
17567002    8 -r-x------   1 yarn     hadoop       5467 May 16 11:10 ./__spark_conf__/hadoop-env.sh
17567022    8 -r-x------   1 yarn     hadoop       4221 May 16 11:10 ./__spark_conf__/task-log4j.properties
17567033    8 -r-x------   1 yarn     hadoop       4113 May 16 11:10 ./__spark_conf__/mapred-queues.xml.template
17567036   36 -r-x------   1 yarn     hadoop      34061 May 16 11:10 ./__spark_conf__/__spark_conf__.properties
17567009    4 -r-x------   1 yarn     hadoop       1631 May 16 11:10 ./__spark_conf__/kms-log4j.properties
17567018    8 -r-x------   1 yarn     hadoop       6572 May 16 11:10 ./__spark_conf__/hdfs-site.xml
17567005   20 -r-x------   1 yarn     hadoop      16530 May 16 11:10 ./__spark_conf__/yarn-site.xml
17567023    4 -r-x------   1 yarn     hadoop        690 May 16 11:10 ./__spark_conf__/mapred-env.sh
17567025    4 -r-x------   1 yarn     hadoop       2316 May 16 11:10 ./__spark_conf__/ssl-client.xml.example
17567017    8 -r-x------   1 yarn     hadoop       5006 May 16 11:10 ./__spark_conf__/yarn-env.sh
17567019    4 -r-x------   1 yarn     hadoop       1019 May 16 11:10 ./__spark_conf__/container-executor.cfg
17567021    4 -r-x------   1 yarn     hadoop       2490 May 16 11:10 ./__spark_conf__/hadoop-metrics.properties
17567012    8 -r-x------   1 yarn     hadoop       5404 May 16 11:10 ./__spark_conf__/capacity-scheduler.xml
17567001    8 -r-x------   1 yarn     hadoop       6836 May 16 11:10 ./__spark_conf__/mapred-site.xml
17567006    4 -r-x------   1 yarn     hadoop       3979 May 16 11:10 ./__spark_conf__/hadoop-env.cmd
17567010    4 -r-x------   1 yarn     hadoop       2250 May 16 11:10 ./__spark_conf__/yarn-env.cmd
17567014    4 -r-x------   1 yarn     hadoop        758 May 16 11:10 ./__spark_conf__/mapred-site.xml.template
17567016    4 -r-x------   1 yarn     hadoop       1335 May 16 11:10 ./__spark_conf__/configuration.xsl
17567015    4 -r-x------   1 yarn     hadoop       2358 May 16 11:10 ./__spark_conf__/topology_script.py
17567032    4 -r-x------   1 yarn     hadoop       2268 May 16 11:10 ./__spark_conf__/ssl-server.xml.example
17567013    4 -r-x------   1 yarn     hadoop       3518 May 16 11:10 ./__spark_conf__/kms-acls.xml
17567003   12 -r-x------   1 yarn     hadoop       8699 May 16 11:10 ./__spark_conf__/log4j.properties
17567007    0 -r-x------   1 yarn     hadoop          0 May 16 11:10 ./__spark_conf__/yarn.exclude
17567024    4 -r-x------   1 yarn     hadoop       1602 May 16 11:10 ./__spark_conf__/health_check
17567008    8 -r-x------   1 yarn     hadoop       4197 May 16 11:10 ./__spark_conf__/core-site.xml
17567029    4 -r-x------   1 yarn     hadoop        724 May 16 11:10 ./__spark_conf__/topology_mappings.data
17567026    4 -r-x------   1 yarn     hadoop       1527 May 16 11:10 ./__spark_conf__/kms-env.sh
17567011    4 -r-x------   1 yarn     hadoop        884 May 16 11:10 ./__spark_conf__/ssl-client.xml
17567031    4 -r-x------   1 yarn     hadoop       1000 May 16 11:10 ./__spark_conf__/ssl-server.xml
17567004    4 -r-x------   1 yarn     hadoop       2302 May 16 11:10 ./__spark_conf__/hadoop-metrics2.properties
17567034    8 -r-x------   1 yarn     hadoop       5511 May 16 11:10 ./__spark_conf__/kms-site.xml
17567020    4 -r-x------   1 yarn     hadoop       1020 May 16 11:10 ./__spark_conf__/commons-logging.properties
8127322    4 -rw-r--r--   1 yarn     hadoop        604 May 16 11:10 ./.launch_container.sh.crc
broken symlinks(find -L . -maxdepth 5 -type l -ls):
End of LogType:directory.info

LogType:launch_container.sh
Log Upload Time:Tue May 16 11:12:19 +0700 2017
LogLength:75972
Log Contents:
#!/bin/bash

export SPARK_YARN_STAGING_DIR="hdfs://c408.hadoop.gda.lo:8020/user/fairy/.sparkStaging/application_1494395298335_303847"
export LOCAL_DIRS="/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847"
export HADOOP_CONF_DIR="/usr/hdp/current/hadoop-client/conf"
export NM_HTTP_PORT="8042"
export JAVA_HOME="/usr/jdk64/jdk1.8.0_92/"
export LOG_DIRS="/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000007"
export NM_AUX_SERVICE_mapreduce_shuffle="AAA0+gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=
"
export NM_PORT="45454"
export USER="fairy"
export HADOOP_YARN_HOME="/usr/hdp/current/hadoop-yarn-nodemanager"
export CLASSPATH="$PWD:$PWD/__spark_conf__:$PWD/__spark_libs__/*:$HADOOP_CONF_DIR:/usr/hdp/current/hadoop-client/*:/usr/hdp/current/hadoop-client/lib/*:/usr/hdp/current/hadoop-hdfs-client/*:/usr/hdp/current/hadoop-hdfs-client/lib/*:/usr/hdp/current/hadoop-yarn-client/*:/usr/hdp/current/hadoop-yarn-client/lib/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.4.2.0-258/hadoop/lib/hadoop-lzo-0.6.0.2.4.2.0-258.jar:/etc/hadoop/conf/secure"
export SPARK_LOG_URL_STDOUT="http://c402.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_01_000007/fairy/stdout?start=-4096"
export NM_HOST="c402.hadoop.gda.lo"
export SPARK_YARN_MODE="true"
export HADOOP_TOKEN_FILE_LOCATION="/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_01_000007/container_tokens"
export NM_AUX_SERVICE_spark_shuffle=""
export SPARK_USER="fairy"
export SPARK_LOG_URL_STDERR="http://c402.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_01_000007/fairy/stderr?start=-4096"
export LOGNAME="fairy"
export JVM_PID="$$"
export PWD="/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_01_000007"
export HOME="/home/"
export CONTAINER_ID="container_e81_1494395298335_303847_01_000007"
export MALLOC_ARENA_MAX="4"
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372216/apache-log4j-extras-1.2.17.jar" "__spark_libs__/apache-log4j-extras-1.2.17.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370624/hk2-locator-2.4.0-b34.jar" "__spark_libs__/hk2-locator-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370640/bcprov-jdk15on-1.51.jar" "__spark_libs__/bcprov-jdk15on-1.51.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372237/jersey-client-2.22.2.jar" "__spark_libs__/jersey-client-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370617/compress-lzf-1.0.3.jar" "__spark_libs__/compress-lzf-1.0.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372199/commons-logging-1.1.3.jar" "__spark_libs__/commons-logging-1.1.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370635/protobuf-java-2.5.0.jar" "__spark_libs__/protobuf-java-2.5.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372167/apacheds-kerberos-codec-2.0.0-M15.jar" "__spark_libs__/apacheds-kerberos-codec-2.0.0-M15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370615/javax.inject-1.jar" "__spark_libs__/javax.inject-1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372217/jackson-databind-2.6.5.jar" "__spark_libs__/jackson-databind-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372147/jtransforms-2.4.0.jar" "__spark_libs__/jtransforms-2.4.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372927/gson-2.2.4.jar" "__spark_libs__/gson-2.2.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370602/datanucleus-core-3.2.10.jar" "__spark_libs__/datanucleus-core-3.2.10.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370620/hive-exec-1.2.1.spark2.jar" "__spark_libs__/hive-exec-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372910/spark-streaming_2.11-2.0.1.jar" "__spark_libs__/spark-streaming_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370649/stringtemplate-3.2.1.jar" "__spark_libs__/stringtemplate-3.2.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370646/datanucleus-api-jdo-3.2.6.jar" "__spark_libs__/datanucleus-api-jdo-3.2.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372160/hive-jdbc-1.2.1.spark2.jar" "__spark_libs__/hive-jdbc-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372164/hadoop-yarn-server-common-2.7.1.jar" "__spark_libs__/hadoop-yarn-server-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372180/api-util-1.0.0-M20.jar" "__spark_libs__/api-util-1.0.0-M20.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372211/hadoop-common-2.7.1.jar" "__spark_libs__/hadoop-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372194/snappy-java-1.1.2.6.jar" "__spark_libs__/snappy-java-1.1.2.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370603/commons-codec-1.10.jar" "__spark_libs__/commons-codec-1.10.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372202/parquet-jackson-1.7.0.jar" "__spark_libs__/parquet-jackson-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370630/calcite-avatica-1.2.0-incubating.jar" "__spark_libs__/calcite-avatica-1.2.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370595/commons-digester-1.8.jar" "__spark_libs__/commons-digester-1.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372894/apacheds-i18n-2.0.0-M15.jar" "__spark_libs__/apacheds-i18n-2.0.0-M15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372213/arpack_combined_all-0.1.jar" "__spark_libs__/arpack_combined_all-0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372170/commons-httpclient-3.1.jar" "__spark_libs__/commons-httpclient-3.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372926/JavaEWAH-0.3.2.jar" "__spark_libs__/JavaEWAH-0.3.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372925/mysql-connector-java-5.0.8-bin.jar" "__spark_libs__/mysql-connector-java-5.0.8-bin.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/usercache/fairy/filecache/33843/__spark_conf__.zip" "__spark_conf__"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372183/datanucleus-rdbms-3.2.9.jar" "__spark_libs__/datanucleus-rdbms-3.2.9.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372174/javax.annotation-api-1.2.jar" "__spark_libs__/javax.annotation-api-1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370587/curator-recipes-2.6.0.jar" "__spark_libs__/curator-recipes-2.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372239/hadoop-mapreduce-client-app-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-app-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372240/pmml-schema-1.2.15.jar" "__spark_libs__/pmml-schema-1.2.15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372892/calcite-core-1.2.0-incubating.jar" "__spark_libs__/calcite-core-1.2.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372175/spark-core_2.11-2.0.1.jar" "__spark_libs__/spark-core_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372929/spark-repl_2.11-2.0.1.jar" "__spark_libs__/spark-repl_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372922/metrics-jvm-3.1.2.jar" "__spark_libs__/metrics-jvm-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372151/jersey-guava-2.22.2.jar" "__spark_libs__/jersey-guava-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372177/commons-net-2.2.jar" "__spark_libs__/commons-net-2.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372907/objenesis-2.1.jar" "__spark_libs__/objenesis-2.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372924/paranamer-2.3.jar" "__spark_libs__/paranamer-2.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370627/calcite-linq4j-1.2.0-incubating.jar" "__spark_libs__/calcite-linq4j-1.2.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372221/jta-1.1.jar" "__spark_libs__/jta-1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372916/commons-beanutils-1.7.0.jar" "__spark_libs__/commons-beanutils-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370619/leveldbjni-all-1.8.jar" "__spark_libs__/leveldbjni-all-1.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372923/parquet-hadoop-bundle-1.6.0.jar" "__spark_libs__/parquet-hadoop-bundle-1.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370593/scalap-2.11.8.jar" "__spark_libs__/scalap-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370645/hadoop-annotations-2.7.1.jar" "__spark_libs__/hadoop-annotations-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372915/pmml-model-1.2.15.jar" "__spark_libs__/pmml-model-1.2.15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372157/parquet-format-2.3.0-incubating.jar" "__spark_libs__/parquet-format-2.3.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372156/commons-configuration-1.6.jar" "__spark_libs__/commons-configuration-1.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372196/osgi-resource-locator-1.0.1.jar" "__spark_libs__/osgi-resource-locator-1.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372200/mail-1.4.7.jar" "__spark_libs__/mail-1.4.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372154/hadoop-yarn-common-2.7.1.jar" "__spark_libs__/hadoop-yarn-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370621/aopalliance-1.0.jar" "__spark_libs__/aopalliance-1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372229/stax-api-1.0.1.jar" "__spark_libs__/stax-api-1.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372236/netty-all-4.0.29.Final.jar" "__spark_libs__/netty-all-4.0.29.Final.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370599/breeze-macros_2.11-0.11.2.jar" "__spark_libs__/breeze-macros_2.11-0.11.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372235/libfb303-0.9.2.jar" "__spark_libs__/libfb303-0.9.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370650/jsp-api-2.1.jar" "__spark_libs__/jsp-api-2.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372204/jackson-jaxrs-1.9.13.jar" "__spark_libs__/jackson-jaxrs-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370606/jackson-mapper-asl-1.9.13.jar" "__spark_libs__/jackson-mapper-asl-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372918/spark-mllib-local_2.11-2.0.1.jar" "__spark_libs__/spark-mllib-local_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372195/httpcore-4.4.4.jar" "__spark_libs__/httpcore-4.4.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370653/scala-parser-combinators_2.11-1.0.4.jar" "__spark_libs__/scala-parser-combinators_2.11-1.0.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372162/commons-lang3-3.3.2.jar" "__spark_libs__/commons-lang3-3.3.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372209/jline-2.12.1.jar" "__spark_libs__/jline-2.12.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370591/univocity-parsers-2.1.1.jar" "__spark_libs__/univocity-parsers-2.1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372214/hadoop-mapreduce-client-jobclient-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-jobclient-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372173/minlog-1.3.0.jar" "__spark_libs__/minlog-1.3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370616/parquet-hadoop-1.7.0.jar" "__spark_libs__/parquet-hadoop-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370634/spark-sketch_2.11-2.0.1.jar" "__spark_libs__/spark-sketch_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370592/hk2-api-2.4.0-b34.jar" "__spark_libs__/hk2-api-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372901/scala-xml_2.11-1.0.2.jar" "__spark_libs__/scala-xml_2.11-1.0.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372212/jackson-module-scala_2.11-2.6.5.jar" "__spark_libs__/jackson-module-scala_2.11-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370651/hive-metastore-1.2.1.spark2.jar" "__spark_libs__/hive-metastore-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372884/metrics-core-3.1.2.jar" "__spark_libs__/metrics-core-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370632/spark-catalyst_2.11-2.0.1.jar" "__spark_libs__/spark-catalyst_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372890/zookeeper-3.4.6.jar" "__spark_libs__/zookeeper-3.4.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372186/xmlenc-0.52.jar" "__spark_libs__/xmlenc-0.52.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372224/lift-json_2.11-2.6.3.jar" "__spark_libs__/lift-json_2.11-2.6.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372900/hadoop-client-2.7.1.jar" "__spark_libs__/hadoop-client-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372911/scala-compiler-2.11.8.jar" "__spark_libs__/scala-compiler-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370601/core-1.1.2.jar" "__spark_libs__/core-1.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372193/eigenbase-properties-1.1.5.jar" "__spark_libs__/eigenbase-properties-1.1.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372198/curator-framework-2.6.0.jar" "__spark_libs__/curator-framework-2.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370626/httpclient-4.5.2.jar" "__spark_libs__/httpclient-4.5.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370641/spire-macros_2.11-0.7.4.jar" "__spark_libs__/spire-macros_2.11-0.7.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372189/kryo-shaded-3.0.3.jar" "__spark_libs__/kryo-shaded-3.0.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372905/commons-pool-1.5.4.jar" "__spark_libs__/commons-pool-1.5.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370590/commons-dbcp-1.4.jar" "__spark_libs__/commons-dbcp-1.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372188/java-xmlbuilder-1.0.jar" "__spark_libs__/java-xmlbuilder-1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372219/jodd-core-3.5.2.jar" "__spark_libs__/jodd-core-3.5.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372187/commons-compiler-2.7.6.jar" "__spark_libs__/commons-compiler-2.7.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372913/jcl-over-slf4j-1.7.16.jar" "__spark_libs__/jcl-over-slf4j-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372149/api-asn1-api-1.0.0-M20.jar" "__spark_libs__/api-asn1-api-1.0.0-M20.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372201/parquet-column-1.7.0.jar" "__spark_libs__/parquet-column-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372914/json4s-jackson_2.11-3.2.11.jar" "__spark_libs__/json4s-jackson_2.11-3.2.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370625/avro-ipc-1.7.7.jar" "__spark_libs__/avro-ipc-1.7.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372158/log4j-1.2.17.jar" "__spark_libs__/log4j-1.2.17.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372161/hadoop-auth-2.7.1.jar" "__spark_libs__/hadoop-auth-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372238/hadoop-mapreduce-client-common-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372897/jackson-annotations-2.6.5.jar" "__spark_libs__/jackson-annotations-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372266/jdo-api-3.0.1.jar" "__spark_libs__/jdo-api-3.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372895/metrics-json-3.1.2.jar" "__spark_libs__/metrics-json-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372222/javolution-5.5.1.jar" "__spark_libs__/javolution-5.5.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372215/guava-14.0.1.jar" "__spark_libs__/guava-14.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372230/commons-cli-1.2.jar" "__spark_libs__/commons-cli-1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370589/jets3t-0.9.3.jar" "__spark_libs__/jets3t-0.9.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372887/commons-beanutils-core-1.8.0.jar" "__spark_libs__/commons-beanutils-core-1.8.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372182/derby-10.12.1.1.jar" "__spark_libs__/derby-10.12.1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370605/spark-yarn_2.11-2.0.1.jar" "__spark_libs__/spark-yarn_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372886/libthrift-0.9.2.jar" "__spark_libs__/libthrift-0.9.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372891/super-csv-2.2.0.jar" "__spark_libs__/super-csv-2.2.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372159/jpam-1.1.jar" "__spark_libs__/jpam-1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372920/xbean-asm5-shaded-4.4.jar" "__spark_libs__/xbean-asm5-shaded-4.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370600/pyrolite-4.9.jar" "__spark_libs__/pyrolite-4.9.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372178/spark-sql_2.11-2.0.1.jar" "__spark_libs__/spark-sql_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370598/janino-2.7.8.jar" "__spark_libs__/janino-2.7.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372181/spark-network-common_2.11-2.0.1.jar" "__spark_libs__/spark-network-common_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372912/avro-1.7.7.jar" "__spark_libs__/avro-1.7.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370629/commons-compress-1.4.1.jar" "__spark_libs__/commons-compress-1.4.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370636/antlr-2.7.7.jar" "__spark_libs__/antlr-2.7.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370648/commons-lang-2.6.jar" "__spark_libs__/commons-lang-2.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370652/spark-unsafe_2.11-2.0.1.jar" "__spark_libs__/spark-unsafe_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372192/javassist-3.18.1-GA.jar" "__spark_libs__/javassist-3.18.1-GA.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370628/scala-library-2.11.8.jar" "__spark_libs__/scala-library-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372220/hadoop-yarn-client-2.7.1.jar" "__spark_libs__/hadoop-yarn-client-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372208/hadoop-yarn-api-2.7.1.jar" "__spark_libs__/hadoop-yarn-api-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370612/bonecp-0.8.0.RELEASE.jar" "__spark_libs__/bonecp-0.8.0.RELEASE.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372904/htrace-core-3.1.0-incubating.jar" "__spark_libs__/htrace-core-3.1.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372896/spark-network-shuffle_2.11-2.0.1.jar" "__spark_libs__/spark-network-shuffle_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372163/parquet-common-1.7.0.jar" "__spark_libs__/parquet-common-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372898/hive-cli-1.2.1.spark2.jar" "__spark_libs__/hive-cli-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370614/jersey-bundle-1.19.1.jar" "__spark_libs__/jersey-bundle-1.19.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372176/xercesImpl-2.9.1.jar" "__spark_libs__/xercesImpl-2.9.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372184/joda-time-2.9.3.jar" "__spark_libs__/joda-time-2.9.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370611/ivy-2.4.0.jar" "__spark_libs__/ivy-2.4.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372190/guice-3.0.jar" "__spark_libs__/guice-3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372185/jsr305-1.3.9.jar" "__spark_libs__/jsr305-1.3.9.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372232/stream-2.7.0.jar" "__spark_libs__/stream-2.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372928/jackson-core-2.6.5.jar" "__spark_libs__/jackson-core-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372893/guice-servlet-3.0.jar" "__spark_libs__/guice-servlet-3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372146/parquet-encoding-1.7.0.jar" "__spark_libs__/parquet-encoding-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372165/commons-collections-3.2.2.jar" "__spark_libs__/commons-collections-3.2.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372206/hadoop-mapreduce-client-shuffle-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-shuffle-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372902/javax.servlet-api-3.1.0.jar" "__spark_libs__/javax.servlet-api-3.1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370610/slf4j-log4j12-1.7.16.jar" "__spark_libs__/slf4j-log4j12-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372152/stax-api-1.0-2.jar" "__spark_libs__/stax-api-1.0-2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372179/hive-beeline-1.2.1.spark2.jar" "__spark_libs__/hive-beeline-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372218/activation-1.1.1.jar" "__spark_libs__/activation-1.1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372233/antlr4-runtime-4.5.3.jar" "__spark_libs__/antlr4-runtime-4.5.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370609/hadoop-yarn-server-web-proxy-2.7.1.jar" "__spark_libs__/hadoop-yarn-server-web-proxy-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370637/antlr-runtime-3.4.jar" "__spark_libs__/antlr-runtime-3.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370597/jersey-container-servlet-2.22.2.jar" "__spark_libs__/jersey-container-servlet-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372930/jersey-server-2.22.2.jar" "__spark_libs__/jersey-server-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372231/parquet-generator-1.7.0.jar" "__spark_libs__/parquet-generator-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372903/javax.inject-2.4.0-b34.jar" "__spark_libs__/javax.inject-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372885/hadoop-mapreduce-client-core-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-core-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370638/commons-io-2.4.jar" "__spark_libs__/commons-io-2.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370647/aopalliance-repackaged-2.4.0-b34.jar" "__spark_libs__/aopalliance-repackaged-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372191/oro-2.0.8.jar" "__spark_libs__/oro-2.0.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372223/RoaringBitmap-0.5.11.jar" "__spark_libs__/RoaringBitmap-0.5.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372226/spark-hive_2.11-2.0.1.jar" "__spark_libs__/spark-hive_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372906/spark-tags_2.11-2.0.1.jar" "__spark_libs__/spark-tags_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370633/jersey-media-jaxb-2.22.2.jar" "__spark_libs__/jersey-media-jaxb-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372267/jul-to-slf4j-1.7.16.jar" "__spark_libs__/jul-to-slf4j-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370623/jetty-6.1.26.jar" "__spark_libs__/jetty-6.1.26.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372148/jersey-common-2.22.2.jar" "__spark_libs__/jersey-common-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370588/jackson-module-paranamer-2.6.5.jar" "__spark_libs__/jackson-module-paranamer-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370613/chill-java-0.8.0.jar" "__spark_libs__/chill-java-0.8.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370642/mesos-0.21.1-shaded-protobuf.jar" "__spark_libs__/mesos-0.21.1-shaded-protobuf.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370618/ST4-4.0.4.jar" "__spark_libs__/ST4-4.0.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370594/jackson-core-asl-1.9.13.jar" "__spark_libs__/jackson-core-asl-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372919/jersey-container-servlet-core-2.22.2.jar" "__spark_libs__/jersey-container-servlet-core-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370604/slf4j-api-1.7.16.jar" "__spark_libs__/slf4j-api-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370643/netty-3.8.0.Final.jar" "__spark_libs__/netty-3.8.0.Final.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372227/mx4j-3.0.2.jar" "__spark_libs__/mx4j-3.0.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372908/validation-api-1.1.0.Final.jar" "__spark_libs__/validation-api-1.1.0.Final.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372155/py4j-0.10.3.jar" "__spark_libs__/py4j-0.10.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372166/chill_2.11-0.8.0.jar" "__spark_libs__/chill_2.11-0.8.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370631/scala-reflect-2.11.8.jar" "__spark_libs__/scala-reflect-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372889/jetty-util-6.1.26.jar" "__spark_libs__/jetty-util-6.1.26.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370639/jaxb-api-2.2.2.jar" "__spark_libs__/jaxb-api-2.2.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372225/hk2-utils-2.4.0-b34.jar" "__spark_libs__/hk2-utils-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372153/javax.ws.rs-api-2.0.1.jar" "__spark_libs__/javax.ws.rs-api-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372899/json-20090211.jar" "__spark_libs__/json-20090211.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370608/spark-launcher_2.11-2.0.1.jar" "__spark_libs__/spark-launcher_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372210/json4s-ast_2.11-3.2.11.jar" "__spark_libs__/json4s-ast_2.11-3.2.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372909/hadoop-hdfs-2.7.1.jar" "__spark_libs__/hadoop-hdfs-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372168/lz4-1.3.0.jar" "__spark_libs__/lz4-1.3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372169/spark-mllib_2.11-2.0.1.jar" "__spark_libs__/spark-mllib_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372234/spire_2.11-0.7.4.jar" "__spark_libs__/spire_2.11-0.7.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372150/snappy-0.2.jar" "__spark_libs__/snappy-0.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370622/base64-2.3.8.jar" "__spark_libs__/base64-2.3.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372207/spark-graphx_2.11-2.0.1.jar" "__spark_libs__/spark-graphx_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372921/commons-math3-3.4.1.jar" "__spark_libs__/commons-math3-3.4.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370644/curator-client-2.6.0.jar" "__spark_libs__/curator-client-2.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372917/json4s-core_2.11-3.2.11.jar" "__spark_libs__/json4s-core_2.11-3.2.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372228/spark-hive-thriftserver_2.11-2.0.1.jar" "__spark_libs__/spark-hive-thriftserver_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370607/breeze_2.11-0.11.2.jar" "__spark_libs__/breeze_2.11-0.11.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/usercache/fairy/filecache/33842/stats-etlr-1.0.jar" "__app__.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372197/metrics-graphite-3.1.2.jar" "__spark_libs__/metrics-graphite-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372205/jackson-xc-1.9.13.jar" "__spark_libs__/jackson-xc-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372888/avro-mapred-1.7.7-hadoop2.jar" "__spark_libs__/avro-mapred-1.7.7-hadoop2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/372203/xz-1.0.jar" "__spark_libs__/xz-1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/370596/opencsv-2.3.jar" "__spark_libs__/opencsv-2.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
# Creating copy of launch script
cp "launch_container.sh" "/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000007/launch_container.sh"
chmod 640 "/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000007/launch_container.sh"
# Determining directory contents
echo "ls -l:" 1>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000007/directory.info"
ls -l 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000007/directory.info"
echo "find -L . -maxdepth 5 -ls:" 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000007/directory.info"
find -L . -maxdepth 5 -ls 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000007/directory.info"
echo "broken symlinks(find -L . -maxdepth 5 -type l -ls):" 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000007/directory.info"
find -L . -maxdepth 5 -type l -ls 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000007/directory.info"
exec /bin/bash -c "$JAVA_HOME/bin/java -server -Xmx3072m '-Dhdp.version=2.4.2.0-258' -Djava.io.tmpdir=$PWD/tmp '-Dspark.history.ui.port=18080' '-Dspark.ui.port=0' '-Dspark.driver.port=19210' -Dspark.yarn.app.container.log.dir=/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000007 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.60.43.33:19210 --executor-id 6 --hostname c402.hadoop.gda.lo --cores 1 --app-id application_1494395298335_303847 --user-class-path file:$PWD/__app__.jar 1> /hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000007/stdout 2> /hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000007/stderr"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
End of LogType:launch_container.sh

LogType:stderr
Log Upload Time:Tue May 16 11:12:19 +0700 2017
LogLength:902
Log Contents:
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/hadoop/yarn/local/filecache/370610/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/hdp/2.4.2.0-258/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
17/05/16 11:10:54 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 6188@c402.hadoop.gda.lo
17/05/16 11:10:54 INFO util.SignalUtils: Registered signal handler for TERM
17/05/16 11:10:54 INFO util.SignalUtils: Registered signal handler for HUP
17/05/16 11:10:54 INFO util.SignalUtils: Registered signal handler for INT
17/05/16 11:10:55 ERROR executor.CoarseGrainedExecutorBackend: RECEIVED SIGNAL TERM
End of LogType:stderr

LogType:stdout
Log Upload Time:Tue May 16 11:12:19 +0700 2017
LogLength:0
Log Contents:
End of LogType:stdout



Container: container_e81_1494395298335_303847_02_000004 on c411.hadoop.gda.lo_45454
=====================================================================================
LogType:directory.info
Log Upload Time:Tue May 16 11:12:19 +0700 2017
LogLength:28678
Log Contents:
ls -l:
total 112
lrwxrwxrwx 1 yarn hadoop    69 May 16 11:11 __app__.jar -> /hadoop/yarn/local/usercache/fairy/filecache/34841/stats-etlr-1.0.jar
-rw-r--r-- 1 yarn hadoop    87 May 16 11:11 container_tokens
-rwx------ 1 yarn hadoop   676 May 16 11:11 default_container_executor_session.sh
-rwx------ 1 yarn hadoop   730 May 16 11:11 default_container_executor.sh
-rwx------ 1 yarn hadoop 75763 May 16 11:11 launch_container.sh
lrwxrwxrwx 1 yarn hadoop    69 May 16 11:11 __spark_conf__ -> /hadoop/yarn/local/usercache/fairy/filecache/34842/__spark_conf__.zip
drwxr-xr-x 2 yarn hadoop 12288 May 16 11:11 __spark_libs__
drwx--x--- 2 yarn hadoop  4096 May 16 11:11 tmp
find -L . -maxdepth 5 -ls:
786443    4 drwx--x---   4 yarn     hadoop       4096 May 16 11:11 .
786449    4 -rw-r--r--   1 yarn     hadoop        600 May 16 11:11 ./.launch_container.sh.crc
13767516    4 drwx------   2 yarn     hadoop       4096 May 16 11:10 ./__spark_conf__
13767548    4 -r-x------   1 yarn     hadoop       2268 May 16 11:10 ./__spark_conf__/ssl-server.xml.example
13767549    8 -r-x------   1 yarn     hadoop       4113 May 16 11:10 ./__spark_conf__/mapred-queues.xml.template
13767531    4 -r-x------   1 yarn     hadoop       2358 May 16 11:10 ./__spark_conf__/topology_script.py
13767546    4 -r-x------   1 yarn     hadoop        951 May 16 11:10 ./__spark_conf__/mapred-env.cmd
13767523    0 -r-x------   1 yarn     hadoop          0 May 16 11:10 ./__spark_conf__/yarn.exclude
13767528    8 -r-x------   1 yarn     hadoop       5404 May 16 11:10 ./__spark_conf__/capacity-scheduler.xml
13767536    4 -r-x------   1 yarn     hadoop       1020 May 16 11:10 ./__spark_conf__/commons-logging.properties
13767541    4 -r-x------   1 yarn     hadoop       2316 May 16 11:10 ./__spark_conf__/ssl-client.xml.example
13767526    4 -r-x------   1 yarn     hadoop       2250 May 16 11:10 ./__spark_conf__/yarn-env.cmd
13767524    8 -r-x------   1 yarn     hadoop       4197 May 16 11:10 ./__spark_conf__/core-site.xml
13767535    4 -r-x------   1 yarn     hadoop       1019 May 16 11:10 ./__spark_conf__/container-executor.cfg
13767527    4 -r-x------   1 yarn     hadoop        884 May 16 11:10 ./__spark_conf__/ssl-client.xml
13767539    4 -r-x------   1 yarn     hadoop        690 May 16 11:10 ./__spark_conf__/mapred-env.sh
13767537    4 -r-x------   1 yarn     hadoop       2490 May 16 11:10 ./__spark_conf__/hadoop-metrics.properties
13767520    4 -r-x------   1 yarn     hadoop       2302 May 16 11:10 ./__spark_conf__/hadoop-metrics2.properties
13767544    4 -r-x------   1 yarn     hadoop        229 May 16 11:10 ./__spark_conf__/slaves
13767540    4 -r-x------   1 yarn     hadoop       1602 May 16 11:10 ./__spark_conf__/health_check
13767519   12 -r-x------   1 yarn     hadoop       8699 May 16 11:10 ./__spark_conf__/log4j.properties
13767542    4 -r-x------   1 yarn     hadoop       1527 May 16 11:10 ./__spark_conf__/kms-env.sh
13767533    8 -r-x------   1 yarn     hadoop       5006 May 16 11:10 ./__spark_conf__/yarn-env.sh
13767547    4 -r-x------   1 yarn     hadoop       1000 May 16 11:10 ./__spark_conf__/ssl-server.xml
13767551    4 -r-x------   1 yarn     hadoop        945 May 16 11:10 ./__spark_conf__/taskcontroller.cfg
13767543    4 -r-x------   1 yarn     hadoop       1308 May 16 11:10 ./__spark_conf__/hadoop-policy.xml
13767538    8 -r-x------   1 yarn     hadoop       4221 May 16 11:10 ./__spark_conf__/task-log4j.properties
13767534    8 -r-x------   1 yarn     hadoop       6572 May 16 11:10 ./__spark_conf__/hdfs-site.xml
13767530    4 -r-x------   1 yarn     hadoop        758 May 16 11:10 ./__spark_conf__/mapred-site.xml.template
13767552   36 -r-x------   1 yarn     hadoop      34061 May 16 11:10 ./__spark_conf__/__spark_conf__.properties
13767518    8 -r-x------   1 yarn     hadoop       5467 May 16 11:10 ./__spark_conf__/hadoop-env.sh
13767550    8 -r-x------   1 yarn     hadoop       5511 May 16 11:10 ./__spark_conf__/kms-site.xml
13767529    4 -r-x------   1 yarn     hadoop       3518 May 16 11:10 ./__spark_conf__/kms-acls.xml
13767545    4 -r-x------   1 yarn     hadoop        724 May 16 11:10 ./__spark_conf__/topology_mappings.data
13767525    4 -r-x------   1 yarn     hadoop       1631 May 16 11:10 ./__spark_conf__/kms-log4j.properties
13767532    4 -r-x------   1 yarn     hadoop       1335 May 16 11:10 ./__spark_conf__/configuration.xsl
13767521   20 -r-x------   1 yarn     hadoop      16530 May 16 11:10 ./__spark_conf__/yarn-site.xml
13767522    4 -r-x------   1 yarn     hadoop       3979 May 16 11:10 ./__spark_conf__/hadoop-env.cmd
13767517    8 -r-x------   1 yarn     hadoop       6836 May 16 11:10 ./__spark_conf__/mapred-site.xml
786451    4 -rw-r--r--   1 yarn     hadoop         16 May 16 11:11 ./.default_container_executor_session.sh.crc
786447    4 -rw-r--r--   1 yarn     hadoop         12 May 16 11:11 ./.container_tokens.crc
13767436 1152 -r-x------   1 yarn     hadoop    1178454 May 16 11:10 ./__app__.jar
786446    4 -rw-r--r--   1 yarn     hadoop         87 May 16 11:11 ./container_tokens
786453    4 -rw-r--r--   1 yarn     hadoop         16 May 16 11:11 ./.default_container_executor.sh.crc
786450    4 -rwx------   1 yarn     hadoop        676 May 16 11:11 ./default_container_executor_session.sh
786455   12 drwxr-xr-x   2 yarn     hadoop      12288 May 16 11:11 ./__spark_libs__
4325815  244 -rwxr-xr-x   1 yarn     hadoop     248171 Mar 27 00:50 ./__spark_libs__/curator-recipes-2.6.0.jar
4325941  524 -rwxr-xr-x   1 yarn     hadoop     533455 Mar 27 00:50 ./__spark_libs__/protobuf-java-2.5.0.jar
4326095    8 -rwxr-xr-x   1 yarn     hadoop       4596 Mar 27 00:50 ./__spark_libs__/jul-to-slf4j-1.7.16.jar
4326035   48 -rwxr-xr-x   1 yarn     hadoop      48720 Mar 27 00:50 ./__spark_libs__/snappy-0.2.jar
4325947  236 -rwxr-xr-x   1 yarn     hadoop     241367 Mar 27 00:50 ./__spark_libs__/commons-compress-1.4.1.jar
4325858   92 -rwxr-xr-x   1 yarn     hadoop      93407 Mar 27 00:50 ./__spark_libs__/pyrolite-4.9.jar
4326131 3152 -rwxr-xr-x   1 yarn     hadoop    3224708 Mar 27 00:50 ./__spark_libs__/derby-10.12.1.1.jar
11272636  204 -rwxr-xr-x   1 yarn     hadoop     206035 Apr  4 16:11 ./__spark_libs__/commons-beanutils-core-1.8.0.jar
4326129   36 -rwxr-xr-x   1 yarn     hadoop      33015 Mar 27 00:50 ./__spark_libs__/jsr305-1.3.9.jar
11272774    4 -rwxr-xr-x   1 yarn     hadoop       2545 Apr  4 16:11 ./__spark_libs__/hadoop-client-2.7.1.jar
4326237  140 -rwxr-xr-x   1 yarn     hadoop     142631 Mar 27 00:50 ./__spark_libs__/hadoop-yarn-client-2.7.1.jar
4325827  284 -rwxr-xr-x   1 yarn     hadoop     290506 Mar 27 00:50 ./__spark_libs__/univocity-parsers-2.1.1.jar
4325842  228 -rwxr-xr-x   1 yarn     hadoop     232248 Mar 27 00:50 ./__spark_libs__/jackson-core-asl-1.9.13.jar
11272631  224 -rwxr-xr-x   1 yarn     hadoop     227712 Apr  4 16:11 ./__spark_libs__/libthrift-0.9.2.jar
4326208   84 -rwxr-xr-x   1 yarn     hadoop      82421 Mar 27 00:50 ./__spark_libs__/json4s-ast_2.11-3.2.11.jar
4325938  256 -rwxr-xr-x   1 yarn     hadoop     258370 Mar 27 00:50 ./__spark_libs__/calcite-avatica-1.2.0-incubating.jar
4326217 3352 -rwxr-xr-x   1 yarn     hadoop    3431544 Mar 27 00:50 ./__spark_libs__/hadoop-common-2.7.1.jar
4326156  696 -rwxr-xr-x   1 yarn     hadoop     710492 Mar 27 00:50 ./__spark_libs__/guice-3.0.jar
11272770   40 -rwxr-xr-x   1 yarn     hadoop      40817 Apr  4 16:11 ./__spark_libs__/hive-cli-1.2.1.spark2.jar
4325884   68 -rwxr-xr-x   1 yarn     hadoop      65653 Mar 27 00:50 ./__spark_libs__/spark-launcher_2.11-2.0.1.jar
4326087  232 -rwxr-xr-x   1 yarn     hadoop     236880 Mar 27 00:50 ./__spark_libs__/lz4-1.3.0.jar
4325784  144 -rwxr-xr-x   1 yarn     hadoop     144660 Mar 27 00:50 ./__spark_libs__/xbean-asm5-shaded-4.4.jar
4326016   44 -rwxr-xr-x   1 yarn     hadoop      41070 Mar 27 00:50 ./__spark_libs__/spark-unsafe_2.11-2.0.1.jar
4326092 5712 -rwxr-xr-x   1 yarn     hadoop    5847591 Mar 27 00:50 ./__spark_libs__/spark-mllib_2.11-2.0.1.jar
4326145   32 -rwxr-xr-x   1 yarn     hadoop      30595 Mar 27 00:50 ./__spark_libs__/commons-compiler-2.7.6.jar
11272694   92 -rwxr-xr-x   1 yarn     hadoop      93210 Apr  4 16:11 ./__spark_libs__/super-csv-2.2.0.jar
4325988 1248 -rwxr-xr-x   1 yarn     hadoop    1277883 Mar 27 00:50 ./__spark_libs__/mesos-0.21.1-shaded-protobuf.jar
4325908  208 -rwxr-xr-x   1 yarn     hadoop     209622 Mar 27 00:50 ./__spark_libs__/parquet-hadoop-1.7.0.jar
4325882 13136 -rwxr-xr-x   1 yarn     hadoop   13448966 Mar 27 00:50 ./__spark_libs__/breeze_2.11-0.11.2.jar
4326204  668 -rwxr-xr-x   1 yarn     hadoop     680106 Mar 27 00:50 ./__spark_libs__/spark-graphx_2.11-2.0.1.jar
4326272   44 -rwxr-xr-x   1 yarn     hadoop      41123 Mar 27 00:50 ./__spark_libs__/commons-cli-1.2.jar
4325829  160 -rwxr-xr-x   1 yarn     hadoop     160519 Mar 27 00:50 ./__spark_libs__/commons-dbcp-1.4.jar
4326160  700 -rwxr-xr-x   1 yarn     hadoop     714194 Mar 27 00:50 ./__spark_libs__/javassist-3.18.1-GA.jar
4325982 2780 -rwxr-xr-x   1 yarn     hadoop    2842667 Mar 27 00:50 ./__spark_libs__/bcprov-jdk15on-1.51.jar
4326005  280 -rwxr-xr-x   1 yarn     hadoop     284220 Mar 27 00:50 ./__spark_libs__/commons-lang-2.6.jar
4326119 6144 -rwxr-xr-x   1 yarn     hadoop    6290315 Mar 27 00:50 ./__spark_libs__/spark-sql_2.11-2.0.1.jar
4326068   72 -rwxr-xr-x   1 yarn     hadoop      70688 Mar 27 00:50 ./__spark_libs__/hadoop-auth-2.7.1.jar
4325834  144 -rwxr-xr-x   1 yarn     hadoop     143602 Mar 27 00:50 ./__spark_libs__/commons-digester-1.8.jar
4326101    8 -rwxr-xr-x   1 yarn     hadoop       5711 Mar 27 00:50 ./__spark_libs__/minlog-1.3.0.jar
4326194   20 -rwxr-xr-x   1 yarn     hadoop      18336 Mar 27 00:50 ./__spark_libs__/jackson-jaxrs-1.9.13.jar
4325848   20 -rwxr-xr-x   1 yarn     hadoop      19827 Mar 27 00:50 ./__spark_libs__/opencsv-2.3.jar
11272775   48 -rwxr-xr-x   1 yarn     hadoop      45944 Apr  4 16:11 ./__spark_libs__/json-20090211.jar
4326084  220 -rwxr-xr-x   1 yarn     hadoop     223573 Mar 27 00:50 ./__spark_libs__/chill_2.11-0.8.0.jar
4325822   44 -rwxr-xr-x   1 yarn     hadoop      41263 Mar 27 00:50 ./__spark_libs__/jackson-module-paranamer-2.6.5.jar
4325782  176 -rwxr-xr-x   1 yarn     hadoop     177832 Mar 27 00:50 ./__spark_libs__/spark-mllib-local_2.11-2.0.1.jar
4326149   12 -rwxr-xr-x   1 yarn     hadoop      10023 Mar 27 00:50 ./__spark_libs__/java-xmlbuilder-1.0.jar
11272728 3440 -rwxr-xr-x   1 yarn     hadoop    3519262 Apr  4 16:11 ./__spark_libs__/calcite-core-1.2.0-incubating.jar
4325880   32 -rwxr-xr-x   1 yarn     hadoop      32145 Mar 27 00:50 ./__spark_libs__/hadoop-yarn-server-web-proxy-2.7.1.jar
4325984   88 -rwxr-xr-x   1 yarn     hadoop      86811 Mar 27 00:50 ./__spark_libs__/spire-macros_2.11-0.7.4.jar
4325804  188 -rwxr-xr-x   1 yarn     hadoop     190432 Mar 27 00:50 ./__spark_libs__/gson-2.2.4.jar
4325957 6716 -rwxr-xr-x   1 yarn     hadoop    6873892 Mar 27 00:50 ./__spark_libs__/spark-catalyst_2.11-2.0.1.jar
4326260  120 -rwxr-xr-x   1 yarn     hadoop     118973 Mar 27 00:50 ./__spark_libs__/hk2-utils-2.4.0-b34.jar
4325916  232 -rwxr-xr-x   1 yarn     hadoop     236660 Mar 27 00:50 ./__spark_libs__/ST4-4.0.4.jar
4326109  136 -rwxr-xr-x   1 yarn     hadoop     138464 Mar 27 00:50 ./__spark_libs__/hive-beeline-1.2.1.spark2.jar
4325865 1848 -rwxr-xr-x   1 yarn     hadoop    1890075 Mar 27 00:50 ./__spark_libs__/datanucleus-core-3.2.10.jar
4325755  576 -rwxr-xr-x   1 yarn     hadoop     589462 Mar 27 00:50 ./__spark_libs__/json4s-core_2.11-3.2.11.jar
4326086  676 -rwxr-xr-x   1 yarn     hadoop     691479 Mar 27 00:50 ./__spark_libs__/apacheds-kerberos-codec-2.0.0-M15.jar
4325900   52 -rwxr-xr-x   1 yarn     hadoop      50619 Mar 27 00:50 ./__spark_libs__/chill-java-0.8.0.jar
4325934  720 -rwxr-xr-x   1 yarn     hadoop     736658 Mar 27 00:50 ./__spark_libs__/httpclient-4.5.2.jar
4325896  112 -rwxr-xr-x   1 yarn     hadoop     110600 Mar 27 00:50 ./__spark_libs__/bonecp-0.8.0.RELEASE.jar
4326134 1768 -rwxr-xr-x   1 yarn     hadoop    1809447 Mar 27 00:50 ./__spark_libs__/datanucleus-rdbms-3.2.9.jar
4326233   68 -rwxr-xr-x   1 yarn     hadoop      69409 Mar 27 00:50 ./__spark_libs__/activation-1.1.1.jar
4326115  208 -rwxr-xr-x   1 yarn     hadoop     212453 Mar 27 00:50 ./__spark_libs__/commons-net-2.2.jar
4326121   80 -rwxr-xr-x   1 yarn     hadoop      79912 Mar 27 00:50 ./__spark_libs__/api-util-1.0.0-M20.jar
4325952  184 -rwxr-xr-x   1 yarn     hadoop     185140 Mar 27 00:50 ./__spark_libs__/commons-io-2.4.jar
4325931  528 -rwxr-xr-x   1 yarn     hadoop     539912 Mar 27 00:50 ./__spark_libs__/jetty-6.1.26.jar
4326163   20 -rwxr-xr-x   1 yarn     hadoop      18482 Mar 27 00:50 ./__spark_libs__/eigenbase-properties-1.1.5.jar
4325969  436 -rwxr-xr-x   1 yarn     hadoop     445288 Mar 27 00:50 ./__spark_libs__/antlr-2.7.7.jar
4325918 1024 -rwxr-xr-x   1 yarn     hadoop    1045744 Mar 27 00:50 ./__spark_libs__/leveldbjni-all-1.8.jar
4326076  356 -rwxr-xr-x   1 yarn     hadoop     363908 Mar 27 00:50 ./__spark_libs__/hadoop-yarn-server-common-2.7.1.jar
4326165   64 -rwxr-xr-x   1 yarn     hadoop      62050 Mar 27 00:50 ./__spark_libs__/commons-logging-1.1.3.jar
4326055  480 -rwxr-xr-x   1 yarn     hadoop     489884 Mar 27 00:50 ./__spark_libs__/log4j-1.2.17.jar
4326022  280 -rwxr-xr-x   1 yarn     hadoop     285447 Mar 27 00:50 ./__spark_libs__/parquet-encoding-1.7.0.jar
11272626 1512 -rwxr-xr-x   1 yarn     hadoop    1544875 Apr  4 16:11 ./__spark_libs__/hadoop-mapreduce-client-core-2.7.1.jar
4325876  764 -rwxr-xr-x   1 yarn     hadoop     780664 Mar 27 00:50 ./__spark_libs__/jackson-mapper-asl-1.9.13.jar
4326173   20 -rwxr-xr-x   1 yarn     hadoop      20235 Mar 27 00:50 ./__spark_libs__/osgi-resource-locator-1.0.1.jar
4326078  300 -rwxr-xr-x   1 yarn     hadoop     305001 Mar 27 00:50 ./__spark_libs__/commons-httpclient-3.1.jar
4325997  352 -rwxr-xr-x   1 yarn     hadoop     358390 Mar 27 00:50 ./__spark_libs__/kryo-shaded-3.0.3.jar
4326142   16 -rwxr-xr-x   1 yarn     hadoop      15010 Mar 27 00:50 ./__spark_libs__/xmlenc-0.52.jar
4326275   24 -rwxr-xr-x   1 yarn     hadoop      21243 Mar 27 00:50 ./__spark_libs__/parquet-generator-1.7.0.jar
4326107 11452 -rwxr-xr-x   1 yarn     hadoop   11723537 Mar 27 00:50 ./__spark_libs__/spark-core_2.11-2.0.1.jar
4325869  280 -rwxr-xr-x   1 yarn     hadoop     284184 Mar 27 00:50 ./__spark_libs__/commons-codec-1.10.jar
4326123 2304 -rwxr-xr-x   1 yarn     hadoop    2355465 Mar 27 00:50 ./__spark_libs__/spark-network-common_2.11-2.0.1.jar
4325943  436 -rwxr-xr-x   1 yarn     hadoop     442406 Mar 27 00:50 ./__spark_libs__/calcite-linq4j-1.2.0-incubating.jar
4326044  116 -rwxr-xr-x   1 yarn     hadoop     115534 Mar 27 00:50 ./__spark_libs__/javax.ws.rs-api-2.0.1.jar
4326097  200 -rwxr-xr-x   1 yarn     hadoop     201124 Mar 27 00:50 ./__spark_libs__/jdo-api-3.0.1.jar
4326295 2008 -rwxr-xr-x   1 yarn     hadoop    2054931 Mar 27 00:50 ./__spark_libs__/netty-all-4.0.29.Final.jar
4326304    8 -rwxr-xr-x   1 yarn     hadoop       5310 Mar 27 00:50 ./__spark_libs__/pmml-schema-1.2.15.jar
4326111 1204 -rwxr-xr-x   1 yarn     hadoop    1229125 Mar 27 00:50 ./__spark_libs__/xercesImpl-2.9.1.jar
4326256  468 -rwxr-xr-x   1 yarn     hadoop     477970 Mar 27 00:50 ./__spark_libs__/lift-json_2.11-2.6.3.jar
4326175   24 -rwxr-xr-x   1 yarn     hadoop      20852 Mar 27 00:50 ./__spark_libs__/metrics-graphite-3.1.2.jar
4325793   40 -rwxr-xr-x   1 yarn     hadoop      39280 Mar 27 00:50 ./__spark_libs__/metrics-jvm-3.1.2.jar
4326057  380 -rwxr-xr-x   1 yarn     hadoop     387188 Mar 27 00:50 ./__spark_libs__/parquet-format-2.3.0-incubating.jar
4326247   16 -rwxr-xr-x   1 yarn     hadoop      15071 Mar 27 00:50 ./__spark_libs__/jta-1.1.jar
4326029  684 -rwxr-xr-x   1 yarn     hadoop     698375 Mar 27 00:50 ./__spark_libs__/jersey-common-2.22.2.jar
11272651  176 -rwxr-xr-x   1 yarn     hadoop     177131 Apr  4 16:11 ./__spark_libs__/jetty-util-6.1.26.jar
4326235 1144 -rwxr-xr-x   1 yarn     hadoop    1171380 Mar 27 00:50 ./__spark_libs__/jackson-databind-2.6.5.jar
4326284  308 -rwxr-xr-x   1 yarn     hadoop     313686 Mar 27 00:50 ./__spark_libs__/libfb303-0.9.2.jar
4325836  176 -rwxr-xr-x   1 yarn     hadoop     178947 Mar 27 00:50 ./__spark_libs__/hk2-api-2.4.0-b34.jar
4325831  788 -rwxr-xr-x   1 yarn     hadoop     802818 Mar 27 00:50 ./__spark_libs__/scalap-2.11.8.jar
4326178  184 -rwxr-xr-x   1 yarn     hadoop     185245 Mar 27 00:50 ./__spark_libs__/curator-framework-2.6.0.jar
4325872   40 -rwxr-xr-x   1 yarn     hadoop      40509 Mar 27 00:50 ./__spark_libs__/slf4j-api-1.7.16.jar
4326007  148 -rwxr-xr-x   1 yarn     hadoop     148627 Mar 27 00:50 ./__spark_libs__/stringtemplate-3.2.1.jar
4326020  416 -rwxr-xr-x   1 yarn     hadoop     423753 Mar 27 00:50 ./__spark_libs__/scala-parser-combinators_2.11-1.0.4.jar
4325960   72 -rwxr-xr-x   1 yarn     hadoop      72733 Mar 27 00:50 ./__spark_libs__/jersey-media-jaxb-2.22.2.jar
4325817  932 -rwxr-xr-x   1 yarn     hadoop     951701 Mar 27 00:50 ./__spark_libs__/jersey-server-2.22.2.jar
4325986   20 -rwxr-xr-x   1 yarn     hadoop      17385 Mar 27 00:50 ./__spark_libs__/hadoop-annotations-2.7.1.jar
4325978  104 -rwxr-xr-x   1 yarn     hadoop     105134 Mar 27 00:50 ./__spark_libs__/jaxb-api-2.2.2.jar
4326200   44 -rwxr-xr-x   1 yarn     hadoop      45015 Mar 27 00:50 ./__spark_libs__/hadoop-mapreduce-client-shuffle-2.7.1.jar
4326080  576 -rwxr-xr-x   1 yarn     hadoop     588337 Mar 27 00:50 ./__spark_libs__/commons-collections-3.2.2.jar
4325878  192 -rwxr-xr-x   1 yarn     hadoop     192993 Mar 27 00:50 ./__spark_libs__/avro-ipc-1.7.7.jar
4325795 2732 -rwxr-xr-x   1 yarn     hadoop    2796935 Mar 27 00:50 ./__spark_libs__/parquet-hadoop-bundle-1.6.0.jar
4326062  404 -rwxr-xr-x   1 yarn     hadoop     412739 Mar 27 00:50 ./__spark_libs__/commons-lang3-3.3.2.jar
4326197   28 -rwxr-xr-x   1 yarn     hadoop      27084 Mar 27 00:50 ./__spark_libs__/jackson-xc-1.9.13.jar
4326060   12 -rwxr-xr-x   1 yarn     hadoop      12131 Mar 27 00:50 ./__spark_libs__/jpam-1.1.jar
4325802  532 -rwxr-xr-x   1 yarn     hadoop     540852 Mar 27 00:50 ./__spark_libs__/mysql-connector-java-5.0.8-bin.jar
4326270   28 -rwxr-xr-x   1 yarn     hadoop      26514 Mar 27 00:50 ./__spark_libs__/stax-api-1.0.1.jar
4326254 1024 -rwxr-xr-x   1 yarn     hadoop    1044511 Mar 27 00:50 ./__spark_libs__/spark-hive_2.11-2.0.1.jar
4325728  600 -rwxr-xr-x   1 yarn     hadoop     613299 Mar 27 00:50 ./__spark_libs__/janino-2.7.8.jar
11272751   16 -rwxr-xr-x   1 yarn     hadoop      15827 Apr  4 16:11 ./__spark_libs__/metrics-json-3.1.2.jar
4326298  508 -rwxr-xr-x   1 yarn     hadoop     516127 Mar 27 00:50 ./__spark_libs__/hadoop-mapreduce-client-app-2.7.1.jar
4326229  440 -rwxr-xr-x   1 yarn     hadoop     448794 Mar 27 00:50 ./__spark_libs__/apache-log4j-extras-1.2.17.jar
4326104   28 -rwxr-xr-x   1 yarn     hadoop      26366 Mar 27 00:50 ./__spark_libs__/javax.annotation-api-1.2.jar
4326191   96 -rwxr-xr-x   1 yarn     hadoop      94672 Mar 27 00:50 ./__spark_libs__/xz-1.0.jar
4326041   24 -rwxr-xr-x   1 yarn     hadoop      23346 Mar 27 00:50 ./__spark_libs__/stax-api-1.0-2.jar
4326221   40 -rwxr-xr-x   1 yarn     hadoop      38134 Mar 27 00:50 ./__spark_libs__/hadoop-mapreduce-client-jobclient-2.7.1.jar
11272793   96 -rwxr-xr-x   1 yarn     hadoop      96221 Apr  4 16:11 ./__spark_libs__/commons-pool-1.5.4.jar
4325759   40 -rwxr-xr-x   1 yarn     hadoop      40341 Mar 27 00:50 ./__spark_libs__/json4s-jackson_2.11-3.2.11.jar
4326185  896 -rwxr-xr-x   1 yarn     hadoop     917052 Mar 27 00:50 ./__spark_libs__/parquet-column-1.7.0.jar
4326278  172 -rwxr-xr-x   1 yarn     hadoop     174351 Mar 27 00:50 ./__spark_libs__/stream-2.7.0.jar
4325890   12 -rwxr-xr-x   1 yarn     hadoop       9939 Mar 27 00:50 ./__spark_libs__/slf4j-log4j12-1.7.16.jar
11272748   44 -rwxr-xr-x   1 yarn     hadoop      44925 Apr  4 16:11 ./__spark_libs__/apacheds-i18n-2.0.0-M15.jar
11272785    8 -rwxr-xr-x   1 yarn     hadoop       5950 Apr  4 16:11 ./__spark_libs__/javax.inject-2.4.0-b34.jar
11272788 1444 -rwxr-xr-x   1 yarn     hadoop    1475955 Apr  4 16:11 ./__spark_libs__/htrace-core-3.1.0-incubating.jar
4325813   64 -rwxr-xr-x   1 yarn     hadoop      63316 Mar 27 00:50 ./__spark_libs__/spark-repl_2.11-2.0.1.jar
11272784   96 -rwxr-xr-x   1 yarn     hadoop      95806 Apr  4 16:11 ./__spark_libs__/javax.servlet-api-3.1.0.jar
4325768   20 -rwxr-xr-x   1 yarn     hadoop      16430 Mar 27 00:50 ./__spark_libs__/jcl-over-slf4j-1.7.16.jar
4326137  616 -rwxr-xr-x   1 yarn     hadoop     627814 Mar 27 00:50 ./__spark_libs__/joda-time-2.9.3.jar
4325791 1988 -rwxr-xr-x   1 yarn     hadoop    2035066 Mar 27 00:50 ./__spark_libs__/commons-math3-3.4.1.jar
4326052  292 -rwxr-xr-x   1 yarn     hadoop     298829 Mar 27 00:50 ./__spark_libs__/commons-configuration-1.6.jar
4325925    8 -rwxr-xr-x   1 yarn     hadoop       4467 Mar 27 00:50 ./__spark_libs__/aopalliance-1.0.jar
4325966  164 -rwxr-xr-x   1 yarn     hadoop     164368 Mar 27 00:50 ./__spark_libs__/antlr-runtime-3.4.jar
4325999  332 -rwxr-xr-x   1 yarn     hadoop     339666 Mar 27 00:50 ./__spark_libs__/datanucleus-api-jdo-3.2.6.jar
4325949  116 -rwxr-xr-x   1 yarn     hadoop     114913 Mar 27 00:50 ./__spark_libs__/py4j-0.10.3.jar
4325862  164 -rwxr-xr-x   1 yarn     hadoop     164422 Mar 27 00:50 ./__spark_libs__/core-1.1.2.jar
4326025  748 -rwxr-xr-x   1 yarn     hadoop     764569 Mar 27 00:50 ./__spark_libs__/jtransforms-2.4.0.jar
4326064  100 -rwxr-xr-x   1 yarn     hadoop     100680 Mar 27 00:50 ./__spark_libs__/hive-jdbc-1.2.1.spark2.jar
4325757 2044 -rwxr-xr-x   1 yarn     hadoop    2090370 Mar 27 00:50 ./__spark_libs__/spark-streaming_2.11-2.0.1.jar
4325860 1256 -rwxr-xr-x   1 yarn     hadoop    1282424 Mar 27 00:50 ./__spark_libs__/ivy-2.4.0.jar
4326002   16 -rwxr-xr-x   1 yarn     hadoop      14766 Mar 27 00:50 ./__spark_libs__/aopalliance-repackaged-2.4.0-b34.jar
4326040  952 -rwxr-xr-x   1 yarn     hadoop     971310 Mar 27 00:50 ./__spark_libs__/jersey-guava-2.22.2.jar
4325775  640 -rwxr-xr-x   1 yarn     hadoop     654216 Mar 27 00:50 ./__spark_libs__/pmml-model-1.2.15.jar
11272763   48 -rwxr-xr-x   1 yarn     hadoop      46983 Apr  4 16:11 ./__spark_libs__/jackson-annotations-2.6.5.jar
4326302  736 -rwxr-xr-x   1 yarn     hadoop     753012 Mar 27 00:50 ./__spark_libs__/hadoop-mapreduce-client-common-2.7.1.jar
11272849 8068 -rwxr-xr-x   1 yarn     hadoop    8260573 Apr  4 16:11 ./__spark_libs__/hadoop-hdfs-2.7.1.jar
11272797   16 -rwxr-xr-x   1 yarn     hadoop      15305 Apr  4 16:11 ./__spark_libs__/spark-tags_2.11-2.0.1.jar
11272823   64 -rwxr-xr-x   1 yarn     hadoop      63777 Apr  4 16:11 ./__spark_libs__/validation-api-1.1.0.Final.jar
4326294  164 -rwxr-xr-x   1 yarn     hadoop     167421 Mar 27 00:50 ./__spark_libs__/jersey-client-2.22.2.jar
4325856  136 -rwxr-xr-x   1 yarn     hadoop     135552 Mar 27 00:50 ./__spark_libs__/breeze-macros_2.11-0.11.2.jar
4326249  388 -rwxr-xr-x   1 yarn     hadoop     395195 Mar 27 00:50 ./__spark_libs__/javolution-5.5.1.jar
4325920   20 -rwxr-xr-x   1 yarn     hadoop      17008 Mar 27 00:50 ./__spark_libs__/base64-2.3.8.jar
4326169  320 -rwxr-xr-x   1 yarn     hadoop     326724 Mar 27 00:50 ./__spark_libs__/httpcore-4.4.4.jar
4326293 7108 -rwxr-xr-x   1 yarn     hadoop    7276083 Mar 27 00:50 ./__spark_libs__/spire_2.11-0.7.4.jar
4325892  100 -rwxr-xr-x   1 yarn     hadoop     100636 Mar 27 00:50 ./__spark_libs__/jsp-api-2.1.jar
11272779  636 -rwxr-xr-x   1 yarn     hadoop     648678 Apr  4 16:11 ./__spark_libs__/scala-xml_2.11-1.0.2.jar
4326189 1024 -rwxr-xr-x   1 yarn     hadoop    1048110 Mar 27 00:50 ./__spark_libs__/parquet-jackson-1.7.0.jar
11272761   56 -rwxr-xr-x   1 yarn     hadoop      55511 Apr  4 16:11 ./__spark_libs__/spark-network-shuffle_2.11-2.0.1.jar
4326219  504 -rwxr-xr-x   1 yarn     hadoop     515604 Mar 27 00:50 ./__spark_libs__/jackson-module-scala_2.11-2.6.5.jar
4325928  180 -rwxr-xr-x   1 yarn     hadoop     181271 Mar 27 00:50 ./__spark_libs__/hk2-locator-2.4.0-b34.jar
4325846   20 -rwxr-xr-x   1 yarn     hadoop      18098 Mar 27 00:50 ./__spark_libs__/jersey-container-servlet-2.22.2.jar
4326126   64 -rwxr-xr-x   1 yarn     hadoop      65261 Mar 27 00:50 ./__spark_libs__/oro-2.0.8.jar
11272622  112 -rwxr-xr-x   1 yarn     hadoop     112558 Apr  4 16:11 ./__spark_libs__/metrics-core-3.1.2.jar
4325825 1996 -rwxr-xr-x   1 yarn     hadoop    2041628 Mar 27 00:50 ./__spark_libs__/jets3t-0.9.3.jar
4326264  400 -rwxr-xr-x   1 yarn     hadoop     409467 Mar 27 00:50 ./__spark_libs__/mx4j-3.0.2.jar
4326252  200 -rwxr-xr-x   1 yarn     hadoop     201928 Mar 27 00:50 ./__spark_libs__/RoaringBitmap-0.5.11.jar
4325777  188 -rwxr-xr-x   1 yarn     hadoop     188671 Mar 27 00:50 ./__spark_libs__/commons-beanutils-1.7.0.jar
4326167 1032 -rwxr-xr-x   1 yarn     hadoop    1056168 Mar 27 00:50 ./__spark_libs__/snappy-java-1.1.2.6.jar
4325954 4468 -rwxr-xr-x   1 yarn     hadoop    4573750 Mar 27 00:50 ./__spark_libs__/scala-reflect-2.11.8.jar
4326046 1616 -rwxr-xr-x   1 yarn     hadoop    1654097 Mar 27 00:50 ./__spark_libs__/hadoop-yarn-common-2.7.1.jar
4326182  512 -rwxr-xr-x   1 yarn     hadoop     521157 Mar 27 00:50 ./__spark_libs__/mail-1.4.7.jar
11272639  180 -rwxr-xr-x   1 yarn     hadoop     180736 Apr  4 16:11 ./__spark_libs__/avro-mapred-1.7.7-hadoop2.jar
4325990 1204 -rwxr-xr-x   1 yarn     hadoop    1230201 Mar 27 00:50 ./__spark_libs__/netty-3.8.0.Final.jar
4325907    4 -rwxr-xr-x   1 yarn     hadoop       2497 Mar 27 00:50 ./__spark_libs__/javax.inject-1.jar
11272713   44 -rwxr-xr-x   1 yarn     hadoop      41755 Apr  4 16:11 ./__spark_libs__/objenesis-2.1.jar
4325799   32 -rwxr-xr-x   1 yarn     hadoop      29555 Mar 27 00:50 ./__spark_libs__/paranamer-2.3.jar
4325806   20 -rwxr-xr-x   1 yarn     hadoop      16993 Mar 27 00:50 ./__spark_libs__/JavaEWAH-0.3.2.jar
4325867  656 -rwxr-xr-x   1 yarn     hadoop     669589 Mar 27 00:50 ./__spark_libs__/spark-yarn_2.11-2.0.1.jar
4325912   80 -rwxr-xr-x   1 yarn     hadoop      79845 Mar 27 00:50 ./__spark_libs__/compress-lzf-1.0.3.jar
4326215  296 -rwxr-xr-x   1 yarn     hadoop     302248 Mar 27 00:50 ./__spark_libs__/antlr4-runtime-4.5.3.jar
4326117 1168 -rwxr-xr-x   1 yarn     hadoop    1194003 Mar 27 00:50 ./__spark_libs__/arpack_combined_all-0.1.jar
4325963   32 -rwxr-xr-x   1 yarn     hadoop      29540 Mar 27 00:50 ./__spark_libs__/spark-sketch_2.11-2.0.1.jar
4325761 15128 -rwxr-xr-x   1 yarn     hadoop   15487351 Mar 27 00:50 ./__spark_libs__/scala-compiler-2.11.8.jar
4325898 1592 -rwxr-xr-x   1 yarn     hadoop    1627065 Mar 27 00:50 ./__spark_libs__/jersey-bundle-1.19.1.jar
4326231 2140 -rwxr-xr-x   1 yarn     hadoop    2189117 Mar 27 00:50 ./__spark_libs__/guava-14.0.1.jar
4326267 1772 -rwxr-xr-x   1 yarn     hadoop    1814309 Mar 27 00:50 ./__spark_libs__/spark-hive-thriftserver_2.11-2.0.1.jar
4325923 11232 -rwxr-xr-x   1 yarn     hadoop   11498852 Mar 27 00:50 ./__spark_libs__/hive-exec-1.2.1.spark2.jar
4325786   68 -rwxr-xr-x   1 yarn     hadoop      66270 Mar 27 00:50 ./__spark_libs__/jersey-container-servlet-core-2.22.2.jar
4325765  428 -rwxr-xr-x   1 yarn     hadoop     436303 Mar 27 00:50 ./__spark_libs__/avro-1.7.7.jar
4325945 5612 -rwxr-xr-x   1 yarn     hadoop    5744974 Mar 27 00:50 ./__spark_libs__/scala-library-2.11.8.jar
4326241  420 -rwxr-xr-x   1 yarn     hadoop     427780 Mar 27 00:50 ./__spark_libs__/jodd-core-3.5.2.jar
4326206 1972 -rwxr-xr-x   1 yarn     hadoop    2015514 Mar 27 00:50 ./__spark_libs__/hadoop-yarn-api-2.7.1.jar
11272682  776 -rwxr-xr-x   1 yarn     hadoop     792964 Apr  4 16:11 ./__spark_libs__/zookeeper-3.4.6.jar
4326031   20 -rwxr-xr-x   1 yarn     hadoop      16560 Mar 27 00:50 ./__spark_libs__/api-asn1-api-1.0.0-M20.jar
4326013 5380 -rwxr-xr-x   1 yarn     hadoop    5505200 Mar 27 00:50 ./__spark_libs__/hive-metastore-1.2.1.spark2.jar
4325973   68 -rwxr-xr-x   1 yarn     hadoop      68866 Mar 27 00:50 ./__spark_libs__/curator-client-2.6.0.jar
4326213  212 -rwxr-xr-x   1 yarn     hadoop     213911 Mar 27 00:50 ./__spark_libs__/jline-2.12.1.jar
4326074   24 -rwxr-xr-x   1 yarn     hadoop      21575 Mar 27 00:50 ./__spark_libs__/parquet-common-1.7.0.jar
11272735   64 -rwxr-xr-x   1 yarn     hadoop      65012 Apr  4 16:11 ./__spark_libs__/guice-servlet-3.0.jar
4325808  256 -rwxr-xr-x   1 yarn     hadoop     258876 Mar 27 00:50 ./__spark_libs__/jackson-core-2.6.5.jar
786448   76 -rwx------   1 yarn     hadoop      75763 May 16 11:11 ./launch_container.sh
786452    4 -rwx------   1 yarn     hadoop        730 May 16 11:11 ./default_container_executor.sh
786445    4 drwx--x---   2 yarn     hadoop       4096 May 16 11:11 ./tmp
broken symlinks(find -L . -maxdepth 5 -type l -ls):
End of LogType:directory.info

LogType:launch_container.sh
Log Upload Time:Tue May 16 11:12:19 +0700 2017
LogLength:75763
Log Contents:
#!/bin/bash

export SPARK_YARN_STAGING_DIR="hdfs://c408.hadoop.gda.lo:8020/user/fairy/.sparkStaging/application_1494395298335_303847"
export LOCAL_DIRS="/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847"
export HADOOP_CONF_DIR="/usr/hdp/current/hadoop-client/conf"
export NM_HTTP_PORT="8042"
export JAVA_HOME="/usr/jdk64/jdk1.8.0_92/"
export LOG_DIRS="/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000004"
export NM_AUX_SERVICE_mapreduce_shuffle="AAA0+gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=
"
export NM_PORT="45454"
export USER="fairy"
export HADOOP_YARN_HOME="/usr/hdp/current/hadoop-yarn-nodemanager"
export CLASSPATH="$PWD:$PWD/__spark_conf__:$PWD/__spark_libs__/*:$HADOOP_CONF_DIR:/usr/hdp/current/hadoop-client/*:/usr/hdp/current/hadoop-client/lib/*:/usr/hdp/current/hadoop-hdfs-client/*:/usr/hdp/current/hadoop-hdfs-client/lib/*:/usr/hdp/current/hadoop-yarn-client/*:/usr/hdp/current/hadoop-yarn-client/lib/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.4.2.0-258/hadoop/lib/hadoop-lzo-0.6.0.2.4.2.0-258.jar:/etc/hadoop/conf/secure"
export SPARK_LOG_URL_STDOUT="http://c411.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_02_000004/fairy/stdout?start=-4096"
export NM_HOST="c411.hadoop.gda.lo"
export SPARK_YARN_MODE="true"
export HADOOP_TOKEN_FILE_LOCATION="/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_02_000004/container_tokens"
export NM_AUX_SERVICE_spark_shuffle=""
export SPARK_USER="fairy"
export SPARK_LOG_URL_STDERR="http://c411.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_02_000004/fairy/stderr?start=-4096"
export LOGNAME="fairy"
export JVM_PID="$$"
export PWD="/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_02_000004"
export HOME="/home/"
export CONTAINER_ID="container_e81_1494395298335_303847_02_000004"
export MALLOC_ARENA_MAX="4"
ln -sf "/hadoop/yarn/local/usercache/fairy/filecache/34842/__spark_conf__.zip" "__spark_conf__"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65905/xz-1.0.jar" "__spark_libs__/xz-1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65886/joda-time-2.9.3.jar" "__spark_libs__/joda-time-2.9.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65812/ST4-4.0.4.jar" "__spark_libs__/ST4-4.0.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65817/jetty-6.1.26.jar" "__spark_libs__/jetty-6.1.26.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65939/jersey-client-2.22.2.jar" "__spark_libs__/jersey-client-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65834/bcprov-jdk15on-1.51.jar" "__spark_libs__/bcprov-jdk15on-1.51.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65784/commons-dbcp-1.4.jar" "__spark_libs__/commons-dbcp-1.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65832/commons-io-2.4.jar" "__spark_libs__/commons-io-2.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65880/spark-sql_2.11-2.0.1.jar" "__spark_libs__/spark-sql_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65849/jtransforms-2.4.0.jar" "__spark_libs__/jtransforms-2.4.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65860/parquet-format-2.3.0-incubating.jar" "__spark_libs__/parquet-format-2.3.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65871/spark-mllib_2.11-2.0.1.jar" "__spark_libs__/spark-mllib_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65778/jackson-core-2.6.5.jar" "__spark_libs__/jackson-core-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69768/calcite-core-1.2.0-incubating.jar" "__spark_libs__/calcite-core-1.2.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69777/scala-xml_2.11-1.0.2.jar" "__spark_libs__/scala-xml_2.11-1.0.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65913/hadoop-common-2.7.1.jar" "__spark_libs__/hadoop-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65833/jaxb-api-2.2.2.jar" "__spark_libs__/jaxb-api-2.2.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65842/commons-lang-2.6.jar" "__spark_libs__/commons-lang-2.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69782/spark-tags_2.11-2.0.1.jar" "__spark_libs__/spark-tags_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65806/bonecp-0.8.0.RELEASE.jar" "__spark_libs__/bonecp-0.8.0.RELEASE.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65822/scala-library-2.11.8.jar" "__spark_libs__/scala-library-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65890/java-xmlbuilder-1.0.jar" "__spark_libs__/java-xmlbuilder-1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65896/snappy-java-1.1.2.6.jar" "__spark_libs__/snappy-java-1.1.2.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65768/spark-mllib-local_2.11-2.0.1.jar" "__spark_libs__/spark-mllib-local_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65780/jersey-server-2.22.2.jar" "__spark_libs__/jersey-server-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65864/commons-lang3-3.3.2.jar" "__spark_libs__/commons-lang3-3.3.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65776/JavaEWAH-0.3.2.jar" "__spark_libs__/JavaEWAH-0.3.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65830/antlr-2.7.7.jar" "__spark_libs__/antlr-2.7.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65912/jline-2.12.1.jar" "__spark_libs__/jline-2.12.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65829/protobuf-java-2.5.0.jar" "__spark_libs__/protobuf-java-2.5.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65888/xmlenc-0.52.jar" "__spark_libs__/xmlenc-0.52.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69767/super-csv-2.2.0.jar" "__spark_libs__/super-csv-2.2.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65762/avro-1.7.7.jar" "__spark_libs__/avro-1.7.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65802/spark-launcher_2.11-2.0.1.jar" "__spark_libs__/spark-launcher_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65887/jsr305-1.3.9.jar" "__spark_libs__/jsr305-1.3.9.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65932/commons-cli-1.2.jar" "__spark_libs__/commons-cli-1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69781/commons-pool-1.5.4.jar" "__spark_libs__/commons-pool-1.5.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65920/activation-1.1.1.jar" "__spark_libs__/activation-1.1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65804/slf4j-log4j12-1.7.16.jar" "__spark_libs__/slf4j-log4j12-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65809/javax.inject-1.jar" "__spark_libs__/javax.inject-1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65869/apacheds-kerberos-codec-2.0.0-M15.jar" "__spark_libs__/apacheds-kerberos-codec-2.0.0-M15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69779/javax.inject-2.4.0-b34.jar" "__spark_libs__/javax.inject-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65885/datanucleus-rdbms-3.2.9.jar" "__spark_libs__/datanucleus-rdbms-3.2.9.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65884/derby-10.12.1.1.jar" "__spark_libs__/derby-10.12.1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69760/metrics-core-3.1.2.jar" "__spark_libs__/metrics-core-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65799/spark-yarn_2.11-2.0.1.jar" "__spark_libs__/spark-yarn_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65915/arpack_combined_all-0.1.jar" "__spark_libs__/arpack_combined_all-0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69762/libthrift-0.9.2.jar" "__spark_libs__/libthrift-0.9.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65767/json4s-core_2.11-3.2.11.jar" "__spark_libs__/json4s-core_2.11-3.2.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65790/opencsv-2.3.jar" "__spark_libs__/opencsv-2.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/usercache/fairy/filecache/34841/stats-etlr-1.0.jar" "__app__.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65936/libfb303-0.9.2.jar" "__spark_libs__/libfb303-0.9.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65847/scala-parser-combinators_2.11-1.0.4.jar" "__spark_libs__/scala-parser-combinators_2.11-1.0.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65771/commons-math3-3.4.1.jar" "__spark_libs__/commons-math3-3.4.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65825/scala-reflect-2.11.8.jar" "__spark_libs__/scala-reflect-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65785/univocity-parsers-2.1.1.jar" "__spark_libs__/univocity-parsers-2.1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65786/hk2-api-2.4.0-b34.jar" "__spark_libs__/hk2-api-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69784/validation-api-1.1.0.Final.jar" "__spark_libs__/validation-api-1.1.0.Final.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65846/spark-unsafe_2.11-2.0.1.jar" "__spark_libs__/spark-unsafe_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65908/hadoop-mapreduce-client-shuffle-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-shuffle-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65927/hk2-utils-2.4.0-b34.jar" "__spark_libs__/hk2-utils-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65916/hadoop-mapreduce-client-jobclient-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-jobclient-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65838/curator-client-2.6.0.jar" "__spark_libs__/curator-client-2.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65914/jackson-module-scala_2.11-2.6.5.jar" "__spark_libs__/jackson-module-scala_2.11-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69770/apacheds-i18n-2.0.0-M15.jar" "__spark_libs__/apacheds-i18n-2.0.0-M15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65917/apache-log4j-extras-1.2.17.jar" "__spark_libs__/apache-log4j-extras-1.2.17.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65800/jackson-mapper-asl-1.9.13.jar" "__spark_libs__/jackson-mapper-asl-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65902/mail-1.4.7.jar" "__spark_libs__/mail-1.4.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65933/parquet-generator-1.7.0.jar" "__spark_libs__/parquet-generator-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65937/spire_2.11-0.7.4.jar" "__spark_libs__/spire_2.11-0.7.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65892/guice-3.0.jar" "__spark_libs__/guice-3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65873/jdo-api-3.0.1.jar" "__spark_libs__/jdo-api-3.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65779/spark-repl_2.11-2.0.1.jar" "__spark_libs__/spark-repl_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65823/commons-compress-1.4.1.jar" "__spark_libs__/commons-compress-1.4.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65761/scala-compiler-2.11.8.jar" "__spark_libs__/scala-compiler-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65883/spark-network-common_2.11-2.0.1.jar" "__spark_libs__/spark-network-common_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65764/json4s-jackson_2.11-3.2.11.jar" "__spark_libs__/json4s-jackson_2.11-3.2.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65901/commons-logging-1.1.3.jar" "__spark_libs__/commons-logging-1.1.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69764/avro-mapred-1.7.7-hadoop2.jar" "__spark_libs__/avro-mapred-1.7.7-hadoop2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65808/jersey-bundle-1.19.1.jar" "__spark_libs__/jersey-bundle-1.19.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65877/spark-core_2.11-2.0.1.jar" "__spark_libs__/spark-core_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65795/core-1.1.2.jar" "__spark_libs__/core-1.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65774/paranamer-2.3.jar" "__spark_libs__/paranamer-2.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65796/datanucleus-core-3.2.10.jar" "__spark_libs__/datanucleus-core-3.2.10.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65811/compress-lzf-1.0.3.jar" "__spark_libs__/compress-lzf-1.0.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65882/hive-beeline-1.2.1.spark2.jar" "__spark_libs__/hive-beeline-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65942/pmml-schema-1.2.15.jar" "__spark_libs__/pmml-schema-1.2.15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65857/py4j-0.10.3.jar" "__spark_libs__/py4j-0.10.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65793/breeze-macros_2.11-0.11.2.jar" "__spark_libs__/breeze-macros_2.11-0.11.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69774/hive-cli-1.2.1.spark2.jar" "__spark_libs__/hive-cli-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65845/hive-metastore-1.2.1.spark2.jar" "__spark_libs__/hive-metastore-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65854/stax-api-1.0-2.jar" "__spark_libs__/stax-api-1.0-2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65926/lift-json_2.11-2.6.3.jar" "__spark_libs__/lift-json_2.11-2.6.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65760/spark-streaming_2.11-2.0.1.jar" "__spark_libs__/spark-streaming_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65904/parquet-jackson-1.7.0.jar" "__spark_libs__/parquet-jackson-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65894/javassist-3.18.1-GA.jar" "__spark_libs__/javassist-3.18.1-GA.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65906/jackson-jaxrs-1.9.13.jar" "__spark_libs__/jackson-jaxrs-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65866/hadoop-yarn-server-common-2.7.1.jar" "__spark_libs__/hadoop-yarn-server-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65895/eigenbase-properties-1.1.5.jar" "__spark_libs__/eigenbase-properties-1.1.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65909/spark-graphx_2.11-2.0.1.jar" "__spark_libs__/spark-graphx_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65875/minlog-1.3.0.jar" "__spark_libs__/minlog-1.3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65848/parquet-encoding-1.7.0.jar" "__spark_libs__/parquet-encoding-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65815/aopalliance-1.0.jar" "__spark_libs__/aopalliance-1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65930/spark-hive-thriftserver_2.11-2.0.1.jar" "__spark_libs__/spark-hive-thriftserver_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65766/commons-beanutils-1.7.0.jar" "__spark_libs__/commons-beanutils-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65881/api-util-1.0.0-M20.jar" "__spark_libs__/api-util-1.0.0-M20.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65837/netty-3.8.0.Final.jar" "__spark_libs__/netty-3.8.0.Final.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65899/metrics-graphite-3.1.2.jar" "__spark_libs__/metrics-graphite-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65770/xbean-asm5-shaded-4.4.jar" "__spark_libs__/xbean-asm5-shaded-4.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69778/javax.servlet-api-3.1.0.jar" "__spark_libs__/javax.servlet-api-3.1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65789/commons-digester-1.8.jar" "__spark_libs__/commons-digester-1.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65828/spark-sketch_2.11-2.0.1.jar" "__spark_libs__/spark-sketch_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65879/commons-net-2.2.jar" "__spark_libs__/commons-net-2.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65772/metrics-jvm-3.1.2.jar" "__spark_libs__/metrics-jvm-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65928/spark-hive_2.11-2.0.1.jar" "__spark_libs__/spark-hive_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65874/jul-to-slf4j-1.7.16.jar" "__spark_libs__/jul-to-slf4j-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65911/hadoop-yarn-api-2.7.1.jar" "__spark_libs__/hadoop-yarn-api-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65840/datanucleus-api-jdo-3.2.6.jar" "__spark_libs__/datanucleus-api-jdo-3.2.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65863/hadoop-auth-2.7.1.jar" "__spark_libs__/hadoop-auth-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65876/javax.annotation-api-1.2.jar" "__spark_libs__/javax.annotation-api-1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65893/oro-2.0.8.jar" "__spark_libs__/oro-2.0.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65868/chill_2.11-0.8.0.jar" "__spark_libs__/chill_2.11-0.8.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65844/jsp-api-2.1.jar" "__spark_libs__/jsp-api-2.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65816/base64-2.3.8.jar" "__spark_libs__/base64-2.3.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65781/curator-recipes-2.6.0.jar" "__spark_libs__/curator-recipes-2.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65918/guava-14.0.1.jar" "__spark_libs__/guava-14.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65787/scalap-2.11.8.jar" "__spark_libs__/scalap-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65941/hadoop-mapreduce-client-common-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65858/commons-configuration-1.6.jar" "__spark_libs__/commons-configuration-1.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65921/jodd-core-3.5.2.jar" "__spark_libs__/jodd-core-3.5.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65862/hive-jdbc-1.2.1.spark2.jar" "__spark_libs__/hive-jdbc-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65819/avro-ipc-1.7.7.jar" "__spark_libs__/avro-ipc-1.7.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65783/jets3t-0.9.3.jar" "__spark_libs__/jets3t-0.9.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65835/spire-macros_2.11-0.7.4.jar" "__spark_libs__/spire-macros_2.11-0.7.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69783/objenesis-2.1.jar" "__spark_libs__/objenesis-2.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65865/parquet-common-1.7.0.jar" "__spark_libs__/parquet-common-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65775/mysql-connector-java-5.0.8-bin.jar" "__spark_libs__/mysql-connector-java-5.0.8-bin.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69769/guice-servlet-3.0.jar" "__spark_libs__/guice-servlet-3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69773/jackson-annotations-2.6.5.jar" "__spark_libs__/jackson-annotations-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65836/mesos-0.21.1-shaded-protobuf.jar" "__spark_libs__/mesos-0.21.1-shaded-protobuf.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65839/hadoop-annotations-2.7.1.jar" "__spark_libs__/hadoop-annotations-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69763/commons-beanutils-core-1.8.0.jar" "__spark_libs__/commons-beanutils-core-1.8.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65818/hk2-locator-2.4.0-b34.jar" "__spark_libs__/hk2-locator-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65821/calcite-linq4j-1.2.0-incubating.jar" "__spark_libs__/calcite-linq4j-1.2.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65891/kryo-shaded-3.0.3.jar" "__spark_libs__/kryo-shaded-3.0.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65782/jackson-module-paranamer-2.6.5.jar" "__spark_libs__/jackson-module-paranamer-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65938/netty-all-4.0.29.Final.jar" "__spark_libs__/netty-all-4.0.29.Final.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65803/hadoop-yarn-server-web-proxy-2.7.1.jar" "__spark_libs__/hadoop-yarn-server-web-proxy-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65841/aopalliance-repackaged-2.4.0-b34.jar" "__spark_libs__/aopalliance-repackaged-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65898/osgi-resource-locator-1.0.1.jar" "__spark_libs__/osgi-resource-locator-1.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65850/jersey-common-2.22.2.jar" "__spark_libs__/jersey-common-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65931/stax-api-1.0.1.jar" "__spark_libs__/stax-api-1.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65855/javax.ws.rs-api-2.0.1.jar" "__spark_libs__/javax.ws.rs-api-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65820/httpclient-4.5.2.jar" "__spark_libs__/httpclient-4.5.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65773/parquet-hadoop-bundle-1.6.0.jar" "__spark_libs__/parquet-hadoop-bundle-1.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65788/jackson-core-asl-1.9.13.jar" "__spark_libs__/jackson-core-asl-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65900/curator-framework-2.6.0.jar" "__spark_libs__/curator-framework-2.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65777/gson-2.2.4.jar" "__spark_libs__/gson-2.2.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65935/antlr4-runtime-4.5.3.jar" "__spark_libs__/antlr4-runtime-4.5.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65807/chill-java-0.8.0.jar" "__spark_libs__/chill-java-0.8.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69761/hadoop-mapreduce-client-core-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-core-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65826/spark-catalyst_2.11-2.0.1.jar" "__spark_libs__/spark-catalyst_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65831/antlr-runtime-3.4.jar" "__spark_libs__/antlr-runtime-3.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65878/xercesImpl-2.9.1.jar" "__spark_libs__/xercesImpl-2.9.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65924/javolution-5.5.1.jar" "__spark_libs__/javolution-5.5.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65907/jackson-xc-1.9.13.jar" "__spark_libs__/jackson-xc-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65814/hive-exec-1.2.1.spark2.jar" "__spark_libs__/hive-exec-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65852/snappy-0.2.jar" "__spark_libs__/snappy-0.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65794/pyrolite-4.9.jar" "__spark_libs__/pyrolite-4.9.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65843/stringtemplate-3.2.1.jar" "__spark_libs__/stringtemplate-3.2.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65872/commons-httpclient-3.1.jar" "__spark_libs__/commons-httpclient-3.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65929/mx4j-3.0.2.jar" "__spark_libs__/mx4j-3.0.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65925/RoaringBitmap-0.5.11.jar" "__spark_libs__/RoaringBitmap-0.5.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65813/leveldbjni-all-1.8.jar" "__spark_libs__/leveldbjni-all-1.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65922/hadoop-yarn-client-2.7.1.jar" "__spark_libs__/hadoop-yarn-client-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65897/httpcore-4.4.4.jar" "__spark_libs__/httpcore-4.4.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69785/hadoop-hdfs-2.7.1.jar" "__spark_libs__/hadoop-hdfs-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65765/pmml-model-1.2.15.jar" "__spark_libs__/pmml-model-1.2.15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65798/slf4j-api-1.7.16.jar" "__spark_libs__/slf4j-api-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65769/jersey-container-servlet-core-2.22.2.jar" "__spark_libs__/jersey-container-servlet-core-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65919/jackson-databind-2.6.5.jar" "__spark_libs__/jackson-databind-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65940/hadoop-mapreduce-client-app-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-app-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69772/spark-network-shuffle_2.11-2.0.1.jar" "__spark_libs__/spark-network-shuffle_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65763/jcl-over-slf4j-1.7.16.jar" "__spark_libs__/jcl-over-slf4j-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69775/json-20090211.jar" "__spark_libs__/json-20090211.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65934/stream-2.7.0.jar" "__spark_libs__/stream-2.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65791/jersey-container-servlet-2.22.2.jar" "__spark_libs__/jersey-container-servlet-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69766/zookeeper-3.4.6.jar" "__spark_libs__/zookeeper-3.4.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69771/metrics-json-3.1.2.jar" "__spark_libs__/metrics-json-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65889/commons-compiler-2.7.6.jar" "__spark_libs__/commons-compiler-2.7.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65853/jersey-guava-2.22.2.jar" "__spark_libs__/jersey-guava-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65805/ivy-2.4.0.jar" "__spark_libs__/ivy-2.4.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65797/commons-codec-1.10.jar" "__spark_libs__/commons-codec-1.10.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65810/parquet-hadoop-1.7.0.jar" "__spark_libs__/parquet-hadoop-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65867/commons-collections-3.2.2.jar" "__spark_libs__/commons-collections-3.2.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65824/calcite-avatica-1.2.0-incubating.jar" "__spark_libs__/calcite-avatica-1.2.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65827/jersey-media-jaxb-2.22.2.jar" "__spark_libs__/jersey-media-jaxb-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69765/jetty-util-6.1.26.jar" "__spark_libs__/jetty-util-6.1.26.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69780/htrace-core-3.1.0-incubating.jar" "__spark_libs__/htrace-core-3.1.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65792/janino-2.7.8.jar" "__spark_libs__/janino-2.7.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65903/parquet-column-1.7.0.jar" "__spark_libs__/parquet-column-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65856/hadoop-yarn-common-2.7.1.jar" "__spark_libs__/hadoop-yarn-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65870/lz4-1.3.0.jar" "__spark_libs__/lz4-1.3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65851/api-asn1-api-1.0.0-M20.jar" "__spark_libs__/api-asn1-api-1.0.0-M20.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69776/hadoop-client-2.7.1.jar" "__spark_libs__/hadoop-client-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65801/breeze_2.11-0.11.2.jar" "__spark_libs__/breeze_2.11-0.11.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65910/json4s-ast_2.11-3.2.11.jar" "__spark_libs__/json4s-ast_2.11-3.2.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65923/jta-1.1.jar" "__spark_libs__/jta-1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65861/jpam-1.1.jar" "__spark_libs__/jpam-1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65859/log4j-1.2.17.jar" "__spark_libs__/log4j-1.2.17.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
# Creating copy of launch script
cp "launch_container.sh" "/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000004/launch_container.sh"
chmod 640 "/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000004/launch_container.sh"
# Determining directory contents
echo "ls -l:" 1>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000004/directory.info"
ls -l 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000004/directory.info"
echo "find -L . -maxdepth 5 -ls:" 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000004/directory.info"
find -L . -maxdepth 5 -ls 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000004/directory.info"
echo "broken symlinks(find -L . -maxdepth 5 -type l -ls):" 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000004/directory.info"
find -L . -maxdepth 5 -type l -ls 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000004/directory.info"
exec /bin/bash -c "$JAVA_HOME/bin/java -server -Xmx3072m '-Dhdp.version=2.4.2.0-258' -Djava.io.tmpdir=$PWD/tmp '-Dspark.driver.port=39032' '-Dspark.history.ui.port=18080' '-Dspark.ui.port=0' -Dspark.yarn.app.container.log.dir=/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000004 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.60.43.16:39032 --executor-id 3 --hostname c411.hadoop.gda.lo --cores 1 --app-id application_1494395298335_303847 --user-class-path file:$PWD/__app__.jar 1> /hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000004/stdout 2> /hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000004/stderr"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
End of LogType:launch_container.sh

LogType:stderr
Log Upload Time:Tue May 16 11:12:19 +0700 2017
LogLength:21371
Log Contents:
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/hadoop/yarn/local/filecache/65804/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/hdp/2.4.2.0-258/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
17/05/16 11:11:04 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 65096@c411.hadoop.gda.lo
17/05/16 11:11:04 INFO util.SignalUtils: Registered signal handler for TERM
17/05/16 11:11:04 INFO util.SignalUtils: Registered signal handler for HUP
17/05/16 11:11:04 INFO util.SignalUtils: Registered signal handler for INT
17/05/16 11:11:04 INFO spark.SecurityManager: Changing view acls to: yarn,fairy
17/05/16 11:11:04 INFO spark.SecurityManager: Changing modify acls to: yarn,fairy
17/05/16 11:11:04 INFO spark.SecurityManager: Changing view acls groups to: 
17/05/16 11:11:04 INFO spark.SecurityManager: Changing modify acls groups to: 
17/05/16 11:11:04 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(yarn, fairy); groups with view permissions: Set(); users  with modify permissions: Set(yarn, fairy); groups with modify permissions: Set()
17/05/16 11:11:05 INFO client.TransportClientFactory: Successfully created connection to /10.60.43.16:39032 after 145 ms (0 ms spent in bootstraps)
17/05/16 11:11:05 INFO spark.SecurityManager: Changing view acls to: yarn,fairy
17/05/16 11:11:05 INFO spark.SecurityManager: Changing modify acls to: yarn,fairy
17/05/16 11:11:05 INFO spark.SecurityManager: Changing view acls groups to: 
17/05/16 11:11:05 INFO spark.SecurityManager: Changing modify acls groups to: 
17/05/16 11:11:05 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(yarn, fairy); groups with view permissions: Set(); users  with modify permissions: Set(yarn, fairy); groups with modify permissions: Set()
17/05/16 11:11:05 INFO client.TransportClientFactory: Successfully created connection to /10.60.43.16:39032 after 2 ms (0 ms spent in bootstraps)
17/05/16 11:11:05 INFO storage.DiskBlockManager: Created local directory at /hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/blockmgr-f12db1ce-5c0b-4e70-9375-cc89074d6c7c
17/05/16 11:11:05 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MB
17/05/16 11:11:06 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@10.60.43.16:39032
17/05/16 11:11:06 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver
17/05/16 11:11:06 INFO executor.Executor: Starting executor ID 3 on host c411.hadoop.gda.lo
17/05/16 11:11:06 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 19196.
17/05/16 11:11:06 INFO netty.NettyBlockTransferService: Server created on c411.hadoop.gda.lo:19196
17/05/16 11:11:06 INFO storage.BlockManager: external shuffle service port = 7337
17/05/16 11:11:06 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(3, c411.hadoop.gda.lo, 19196)
17/05/16 11:11:06 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(3, c411.hadoop.gda.lo, 19196)
17/05/16 11:11:06 INFO storage.BlockManager: Registering executor with local external shuffle service.
17/05/16 11:11:06 INFO client.TransportClientFactory: Successfully created connection to c411.hadoop.gda.lo/10.60.43.11:7337 after 1 ms (0 ms spent in bootstraps)
17/05/16 11:11:18 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 3
17/05/16 11:11:18 INFO executor.Executor: Running task 3.0 in stage 1.0 (TID 3)
17/05/16 11:11:18 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 0
17/05/16 11:11:18 INFO client.TransportClientFactory: Successfully created connection to /10.60.43.16:33694 after 2 ms (0 ms spent in bootstraps)
17/05/16 11:11:18 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.2 KB, free 1458.6 MB)
17/05/16 11:11:18 INFO broadcast.TorrentBroadcast: Reading broadcast variable 0 took 175 ms
17/05/16 11:11:18 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 30.3 KB, free 1458.6 MB)
17/05/16 11:11:18 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
17/05/16 11:11:18 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.60.43.16:39032)
17/05/16 11:11:18 INFO spark.MapOutputTrackerWorker: Got the output locations
17/05/16 11:11:18 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:18 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
17/05/16 11:11:19 INFO codegen.CodeGenerator: Code generated in 460.148842 ms
17/05/16 11:11:19 INFO codegen.CodeGenerator: Code generated in 18.022329 ms
17/05/16 11:11:19 INFO codegen.CodeGenerator: Code generated in 20.427674 ms
17/05/16 11:11:19 INFO codegen.CodeGenerator: Code generated in 16.213029 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 3.0 in stage 1.0 (TID 3). 4292 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 44
17/05/16 11:11:19 INFO executor.Executor: Running task 44.0 in stage 1.0 (TID 44)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 44.0 in stage 1.0 (TID 44). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 50
17/05/16 11:11:19 INFO executor.Executor: Running task 50.0 in stage 1.0 (TID 50)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 50.0 in stage 1.0 (TID 50). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 55
17/05/16 11:11:19 INFO executor.Executor: Running task 55.0 in stage 1.0 (TID 55)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 55.0 in stage 1.0 (TID 55). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 58
17/05/16 11:11:19 INFO executor.Executor: Running task 58.0 in stage 1.0 (TID 58)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 58.0 in stage 1.0 (TID 58). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 61
17/05/16 11:11:19 INFO executor.Executor: Running task 61.0 in stage 1.0 (TID 61)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 61.0 in stage 1.0 (TID 61). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 64
17/05/16 11:11:19 INFO executor.Executor: Running task 64.0 in stage 1.0 (TID 64)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 64.0 in stage 1.0 (TID 64). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 68
17/05/16 11:11:19 INFO executor.Executor: Running task 68.0 in stage 1.0 (TID 68)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 68.0 in stage 1.0 (TID 68). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 71
17/05/16 11:11:19 INFO executor.Executor: Running task 71.0 in stage 1.0 (TID 71)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 71.0 in stage 1.0 (TID 71). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 75
17/05/16 11:11:19 INFO executor.Executor: Running task 75.0 in stage 1.0 (TID 75)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 75.0 in stage 1.0 (TID 75). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 81
17/05/16 11:11:19 INFO executor.Executor: Running task 81.0 in stage 1.0 (TID 81)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 81.0 in stage 1.0 (TID 81). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 86
17/05/16 11:11:19 INFO executor.Executor: Running task 86.0 in stage 1.0 (TID 86)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 86.0 in stage 1.0 (TID 86). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 90
17/05/16 11:11:19 INFO executor.Executor: Running task 90.0 in stage 1.0 (TID 90)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 90.0 in stage 1.0 (TID 90). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 96
17/05/16 11:11:19 INFO executor.Executor: Running task 96.0 in stage 1.0 (TID 96)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 96.0 in stage 1.0 (TID 96). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 102
17/05/16 11:11:19 INFO executor.Executor: Running task 102.0 in stage 1.0 (TID 102)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 102.0 in stage 1.0 (TID 102). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 110
17/05/16 11:11:19 INFO executor.Executor: Running task 110.0 in stage 1.0 (TID 110)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 110.0 in stage 1.0 (TID 110). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 119
17/05/16 11:11:19 INFO executor.Executor: Running task 119.0 in stage 1.0 (TID 119)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 119.0 in stage 1.0 (TID 119). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 124
17/05/16 11:11:19 INFO executor.Executor: Running task 124.0 in stage 1.0 (TID 124)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 124.0 in stage 1.0 (TID 124). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 129
17/05/16 11:11:19 INFO executor.Executor: Running task 129.0 in stage 1.0 (TID 129)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 129.0 in stage 1.0 (TID 129). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 135
17/05/16 11:11:19 INFO executor.Executor: Running task 135.0 in stage 1.0 (TID 135)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 135.0 in stage 1.0 (TID 135). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 139
17/05/16 11:11:20 INFO executor.Executor: Running task 139.0 in stage 1.0 (TID 139)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 139.0 in stage 1.0 (TID 139). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 143
17/05/16 11:11:20 INFO executor.Executor: Running task 143.0 in stage 1.0 (TID 143)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 143.0 in stage 1.0 (TID 143). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 148
17/05/16 11:11:20 INFO executor.Executor: Running task 148.0 in stage 1.0 (TID 148)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 148.0 in stage 1.0 (TID 148). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 153
17/05/16 11:11:20 INFO executor.Executor: Running task 153.0 in stage 1.0 (TID 153)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 153.0 in stage 1.0 (TID 153). 3414 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 160
17/05/16 11:11:20 INFO executor.Executor: Running task 160.0 in stage 1.0 (TID 160)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 160.0 in stage 1.0 (TID 160). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 164
17/05/16 11:11:20 INFO executor.Executor: Running task 164.0 in stage 1.0 (TID 164)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 164.0 in stage 1.0 (TID 164). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 170
17/05/16 11:11:20 INFO executor.Executor: Running task 170.0 in stage 1.0 (TID 170)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 170.0 in stage 1.0 (TID 170). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 175
17/05/16 11:11:20 INFO executor.Executor: Running task 175.0 in stage 1.0 (TID 175)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 175.0 in stage 1.0 (TID 175). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 179
17/05/16 11:11:20 INFO executor.Executor: Running task 179.0 in stage 1.0 (TID 179)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 179.0 in stage 1.0 (TID 179). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 185
17/05/16 11:11:20 INFO executor.Executor: Running task 185.0 in stage 1.0 (TID 185)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 185.0 in stage 1.0 (TID 185). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 188
17/05/16 11:11:20 INFO executor.Executor: Running task 188.0 in stage 1.0 (TID 188)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 188.0 in stage 1.0 (TID 188). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 193
17/05/16 11:11:20 INFO executor.Executor: Running task 193.0 in stage 1.0 (TID 193)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 193.0 in stage 1.0 (TID 193). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 197
17/05/16 11:11:20 INFO executor.Executor: Running task 197.0 in stage 1.0 (TID 197)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 197.0 in stage 1.0 (TID 197). 3327 bytes result sent to driver
17/05/16 11:12:17 INFO executor.CoarseGrainedExecutorBackend: Driver commanded a shutdown
17/05/16 11:12:17 INFO memory.MemoryStore: MemoryStore cleared
17/05/16 11:12:17 INFO storage.BlockManager: BlockManager stopped
17/05/16 11:12:17 INFO executor.CoarseGrainedExecutorBackend: Driver from 10.60.43.16:39032 disconnected during shutdown
17/05/16 11:12:17 INFO executor.CoarseGrainedExecutorBackend: Driver from 10.60.43.16:39032 disconnected during shutdown
17/05/16 11:12:17 INFO util.ShutdownHookManager: Shutdown hook called
End of LogType:stderr

LogType:stdout
Log Upload Time:Tue May 16 11:12:19 +0700 2017
LogLength:0
Log Contents:
End of LogType:stdout



Container: container_e81_1494395298335_303847_01_000004 on c411.hadoop.gda.lo_45454
=====================================================================================
LogType:directory.info
Log Upload Time:Tue May 16 11:12:19 +0700 2017
LogLength:28678
Log Contents:
ls -l:
total 112
lrwxrwxrwx 1 yarn hadoop    69 May 16 11:10 __app__.jar -> /hadoop/yarn/local/usercache/fairy/filecache/34841/stats-etlr-1.0.jar
-rw-r--r-- 1 yarn hadoop    87 May 16 11:10 container_tokens
-rwx------ 1 yarn hadoop   676 May 16 11:10 default_container_executor_session.sh
-rwx------ 1 yarn hadoop   730 May 16 11:10 default_container_executor.sh
-rwx------ 1 yarn hadoop 75763 May 16 11:10 launch_container.sh
lrwxrwxrwx 1 yarn hadoop    69 May 16 11:10 __spark_conf__ -> /hadoop/yarn/local/usercache/fairy/filecache/34842/__spark_conf__.zip
drwxr-xr-x 2 yarn hadoop 12288 May 16 11:10 __spark_libs__
drwx--x--- 2 yarn hadoop  4096 May 16 11:10 tmp
find -L . -maxdepth 5 -ls:
786442    4 drwx--x---   4 yarn     hadoop       4096 May 16 11:10 .
786448    4 -rw-r--r--   1 yarn     hadoop        600 May 16 11:10 ./.launch_container.sh.crc
13767516    4 drwx------   2 yarn     hadoop       4096 May 16 11:10 ./__spark_conf__
13767548    4 -r-x------   1 yarn     hadoop       2268 May 16 11:10 ./__spark_conf__/ssl-server.xml.example
13767549    8 -r-x------   1 yarn     hadoop       4113 May 16 11:10 ./__spark_conf__/mapred-queues.xml.template
13767531    4 -r-x------   1 yarn     hadoop       2358 May 16 11:10 ./__spark_conf__/topology_script.py
13767546    4 -r-x------   1 yarn     hadoop        951 May 16 11:10 ./__spark_conf__/mapred-env.cmd
13767523    0 -r-x------   1 yarn     hadoop          0 May 16 11:10 ./__spark_conf__/yarn.exclude
13767528    8 -r-x------   1 yarn     hadoop       5404 May 16 11:10 ./__spark_conf__/capacity-scheduler.xml
13767536    4 -r-x------   1 yarn     hadoop       1020 May 16 11:10 ./__spark_conf__/commons-logging.properties
13767541    4 -r-x------   1 yarn     hadoop       2316 May 16 11:10 ./__spark_conf__/ssl-client.xml.example
13767526    4 -r-x------   1 yarn     hadoop       2250 May 16 11:10 ./__spark_conf__/yarn-env.cmd
13767524    8 -r-x------   1 yarn     hadoop       4197 May 16 11:10 ./__spark_conf__/core-site.xml
13767535    4 -r-x------   1 yarn     hadoop       1019 May 16 11:10 ./__spark_conf__/container-executor.cfg
13767527    4 -r-x------   1 yarn     hadoop        884 May 16 11:10 ./__spark_conf__/ssl-client.xml
13767539    4 -r-x------   1 yarn     hadoop        690 May 16 11:10 ./__spark_conf__/mapred-env.sh
13767537    4 -r-x------   1 yarn     hadoop       2490 May 16 11:10 ./__spark_conf__/hadoop-metrics.properties
13767520    4 -r-x------   1 yarn     hadoop       2302 May 16 11:10 ./__spark_conf__/hadoop-metrics2.properties
13767544    4 -r-x------   1 yarn     hadoop        229 May 16 11:10 ./__spark_conf__/slaves
13767540    4 -r-x------   1 yarn     hadoop       1602 May 16 11:10 ./__spark_conf__/health_check
13767519   12 -r-x------   1 yarn     hadoop       8699 May 16 11:10 ./__spark_conf__/log4j.properties
13767542    4 -r-x------   1 yarn     hadoop       1527 May 16 11:10 ./__spark_conf__/kms-env.sh
13767533    8 -r-x------   1 yarn     hadoop       5006 May 16 11:10 ./__spark_conf__/yarn-env.sh
13767547    4 -r-x------   1 yarn     hadoop       1000 May 16 11:10 ./__spark_conf__/ssl-server.xml
13767551    4 -r-x------   1 yarn     hadoop        945 May 16 11:10 ./__spark_conf__/taskcontroller.cfg
13767543    4 -r-x------   1 yarn     hadoop       1308 May 16 11:10 ./__spark_conf__/hadoop-policy.xml
13767538    8 -r-x------   1 yarn     hadoop       4221 May 16 11:10 ./__spark_conf__/task-log4j.properties
13767534    8 -r-x------   1 yarn     hadoop       6572 May 16 11:10 ./__spark_conf__/hdfs-site.xml
13767530    4 -r-x------   1 yarn     hadoop        758 May 16 11:10 ./__spark_conf__/mapred-site.xml.template
13767552   36 -r-x------   1 yarn     hadoop      34061 May 16 11:10 ./__spark_conf__/__spark_conf__.properties
13767518    8 -r-x------   1 yarn     hadoop       5467 May 16 11:10 ./__spark_conf__/hadoop-env.sh
13767550    8 -r-x------   1 yarn     hadoop       5511 May 16 11:10 ./__spark_conf__/kms-site.xml
13767529    4 -r-x------   1 yarn     hadoop       3518 May 16 11:10 ./__spark_conf__/kms-acls.xml
13767545    4 -r-x------   1 yarn     hadoop        724 May 16 11:10 ./__spark_conf__/topology_mappings.data
13767525    4 -r-x------   1 yarn     hadoop       1631 May 16 11:10 ./__spark_conf__/kms-log4j.properties
13767532    4 -r-x------   1 yarn     hadoop       1335 May 16 11:10 ./__spark_conf__/configuration.xsl
13767521   20 -r-x------   1 yarn     hadoop      16530 May 16 11:10 ./__spark_conf__/yarn-site.xml
13767522    4 -r-x------   1 yarn     hadoop       3979 May 16 11:10 ./__spark_conf__/hadoop-env.cmd
13767517    8 -r-x------   1 yarn     hadoop       6836 May 16 11:10 ./__spark_conf__/mapred-site.xml
786450    4 -rw-r--r--   1 yarn     hadoop         16 May 16 11:10 ./.default_container_executor_session.sh.crc
786446    4 -rw-r--r--   1 yarn     hadoop         12 May 16 11:10 ./.container_tokens.crc
13767436 1152 -r-x------   1 yarn     hadoop    1178454 May 16 11:10 ./__app__.jar
786445    4 -rw-r--r--   1 yarn     hadoop         87 May 16 11:10 ./container_tokens
786452    4 -rw-r--r--   1 yarn     hadoop         16 May 16 11:10 ./.default_container_executor.sh.crc
786449    4 -rwx------   1 yarn     hadoop        676 May 16 11:10 ./default_container_executor_session.sh
786454   12 drwxr-xr-x   2 yarn     hadoop      12288 May 16 11:10 ./__spark_libs__
4325815  244 -rwxr-xr-x   1 yarn     hadoop     248171 Mar 27 00:50 ./__spark_libs__/curator-recipes-2.6.0.jar
4325941  524 -rwxr-xr-x   1 yarn     hadoop     533455 Mar 27 00:50 ./__spark_libs__/protobuf-java-2.5.0.jar
4326095    8 -rwxr-xr-x   1 yarn     hadoop       4596 Mar 27 00:50 ./__spark_libs__/jul-to-slf4j-1.7.16.jar
4326035   48 -rwxr-xr-x   1 yarn     hadoop      48720 Mar 27 00:50 ./__spark_libs__/snappy-0.2.jar
4325947  236 -rwxr-xr-x   1 yarn     hadoop     241367 Mar 27 00:50 ./__spark_libs__/commons-compress-1.4.1.jar
4325858   92 -rwxr-xr-x   1 yarn     hadoop      93407 Mar 27 00:50 ./__spark_libs__/pyrolite-4.9.jar
4326131 3152 -rwxr-xr-x   1 yarn     hadoop    3224708 Mar 27 00:50 ./__spark_libs__/derby-10.12.1.1.jar
11272636  204 -rwxr-xr-x   1 yarn     hadoop     206035 Apr  4 16:11 ./__spark_libs__/commons-beanutils-core-1.8.0.jar
4326129   36 -rwxr-xr-x   1 yarn     hadoop      33015 Mar 27 00:50 ./__spark_libs__/jsr305-1.3.9.jar
11272774    4 -rwxr-xr-x   1 yarn     hadoop       2545 Apr  4 16:11 ./__spark_libs__/hadoop-client-2.7.1.jar
4326237  140 -rwxr-xr-x   1 yarn     hadoop     142631 Mar 27 00:50 ./__spark_libs__/hadoop-yarn-client-2.7.1.jar
4325827  284 -rwxr-xr-x   1 yarn     hadoop     290506 Mar 27 00:50 ./__spark_libs__/univocity-parsers-2.1.1.jar
4325842  228 -rwxr-xr-x   1 yarn     hadoop     232248 Mar 27 00:50 ./__spark_libs__/jackson-core-asl-1.9.13.jar
11272631  224 -rwxr-xr-x   1 yarn     hadoop     227712 Apr  4 16:11 ./__spark_libs__/libthrift-0.9.2.jar
4326208   84 -rwxr-xr-x   1 yarn     hadoop      82421 Mar 27 00:50 ./__spark_libs__/json4s-ast_2.11-3.2.11.jar
4325938  256 -rwxr-xr-x   1 yarn     hadoop     258370 Mar 27 00:50 ./__spark_libs__/calcite-avatica-1.2.0-incubating.jar
4326217 3352 -rwxr-xr-x   1 yarn     hadoop    3431544 Mar 27 00:50 ./__spark_libs__/hadoop-common-2.7.1.jar
4326156  696 -rwxr-xr-x   1 yarn     hadoop     710492 Mar 27 00:50 ./__spark_libs__/guice-3.0.jar
11272770   40 -rwxr-xr-x   1 yarn     hadoop      40817 Apr  4 16:11 ./__spark_libs__/hive-cli-1.2.1.spark2.jar
4325884   68 -rwxr-xr-x   1 yarn     hadoop      65653 Mar 27 00:50 ./__spark_libs__/spark-launcher_2.11-2.0.1.jar
4326087  232 -rwxr-xr-x   1 yarn     hadoop     236880 Mar 27 00:50 ./__spark_libs__/lz4-1.3.0.jar
4325784  144 -rwxr-xr-x   1 yarn     hadoop     144660 Mar 27 00:50 ./__spark_libs__/xbean-asm5-shaded-4.4.jar
4326016   44 -rwxr-xr-x   1 yarn     hadoop      41070 Mar 27 00:50 ./__spark_libs__/spark-unsafe_2.11-2.0.1.jar
4326092 5712 -rwxr-xr-x   1 yarn     hadoop    5847591 Mar 27 00:50 ./__spark_libs__/spark-mllib_2.11-2.0.1.jar
4326145   32 -rwxr-xr-x   1 yarn     hadoop      30595 Mar 27 00:50 ./__spark_libs__/commons-compiler-2.7.6.jar
11272694   92 -rwxr-xr-x   1 yarn     hadoop      93210 Apr  4 16:11 ./__spark_libs__/super-csv-2.2.0.jar
4325988 1248 -rwxr-xr-x   1 yarn     hadoop    1277883 Mar 27 00:50 ./__spark_libs__/mesos-0.21.1-shaded-protobuf.jar
4325908  208 -rwxr-xr-x   1 yarn     hadoop     209622 Mar 27 00:50 ./__spark_libs__/parquet-hadoop-1.7.0.jar
4325882 13136 -rwxr-xr-x   1 yarn     hadoop   13448966 Mar 27 00:50 ./__spark_libs__/breeze_2.11-0.11.2.jar
4326204  668 -rwxr-xr-x   1 yarn     hadoop     680106 Mar 27 00:50 ./__spark_libs__/spark-graphx_2.11-2.0.1.jar
4326272   44 -rwxr-xr-x   1 yarn     hadoop      41123 Mar 27 00:50 ./__spark_libs__/commons-cli-1.2.jar
4325829  160 -rwxr-xr-x   1 yarn     hadoop     160519 Mar 27 00:50 ./__spark_libs__/commons-dbcp-1.4.jar
4326160  700 -rwxr-xr-x   1 yarn     hadoop     714194 Mar 27 00:50 ./__spark_libs__/javassist-3.18.1-GA.jar
4325982 2780 -rwxr-xr-x   1 yarn     hadoop    2842667 Mar 27 00:50 ./__spark_libs__/bcprov-jdk15on-1.51.jar
4326005  280 -rwxr-xr-x   1 yarn     hadoop     284220 Mar 27 00:50 ./__spark_libs__/commons-lang-2.6.jar
4326119 6144 -rwxr-xr-x   1 yarn     hadoop    6290315 Mar 27 00:50 ./__spark_libs__/spark-sql_2.11-2.0.1.jar
4326068   72 -rwxr-xr-x   1 yarn     hadoop      70688 Mar 27 00:50 ./__spark_libs__/hadoop-auth-2.7.1.jar
4325834  144 -rwxr-xr-x   1 yarn     hadoop     143602 Mar 27 00:50 ./__spark_libs__/commons-digester-1.8.jar
4326101    8 -rwxr-xr-x   1 yarn     hadoop       5711 Mar 27 00:50 ./__spark_libs__/minlog-1.3.0.jar
4326194   20 -rwxr-xr-x   1 yarn     hadoop      18336 Mar 27 00:50 ./__spark_libs__/jackson-jaxrs-1.9.13.jar
4325848   20 -rwxr-xr-x   1 yarn     hadoop      19827 Mar 27 00:50 ./__spark_libs__/opencsv-2.3.jar
11272775   48 -rwxr-xr-x   1 yarn     hadoop      45944 Apr  4 16:11 ./__spark_libs__/json-20090211.jar
4326084  220 -rwxr-xr-x   1 yarn     hadoop     223573 Mar 27 00:50 ./__spark_libs__/chill_2.11-0.8.0.jar
4325822   44 -rwxr-xr-x   1 yarn     hadoop      41263 Mar 27 00:50 ./__spark_libs__/jackson-module-paranamer-2.6.5.jar
4325782  176 -rwxr-xr-x   1 yarn     hadoop     177832 Mar 27 00:50 ./__spark_libs__/spark-mllib-local_2.11-2.0.1.jar
4326149   12 -rwxr-xr-x   1 yarn     hadoop      10023 Mar 27 00:50 ./__spark_libs__/java-xmlbuilder-1.0.jar
11272728 3440 -rwxr-xr-x   1 yarn     hadoop    3519262 Apr  4 16:11 ./__spark_libs__/calcite-core-1.2.0-incubating.jar
4325880   32 -rwxr-xr-x   1 yarn     hadoop      32145 Mar 27 00:50 ./__spark_libs__/hadoop-yarn-server-web-proxy-2.7.1.jar
4325984   88 -rwxr-xr-x   1 yarn     hadoop      86811 Mar 27 00:50 ./__spark_libs__/spire-macros_2.11-0.7.4.jar
4325804  188 -rwxr-xr-x   1 yarn     hadoop     190432 Mar 27 00:50 ./__spark_libs__/gson-2.2.4.jar
4325957 6716 -rwxr-xr-x   1 yarn     hadoop    6873892 Mar 27 00:50 ./__spark_libs__/spark-catalyst_2.11-2.0.1.jar
4326260  120 -rwxr-xr-x   1 yarn     hadoop     118973 Mar 27 00:50 ./__spark_libs__/hk2-utils-2.4.0-b34.jar
4325916  232 -rwxr-xr-x   1 yarn     hadoop     236660 Mar 27 00:50 ./__spark_libs__/ST4-4.0.4.jar
4326109  136 -rwxr-xr-x   1 yarn     hadoop     138464 Mar 27 00:50 ./__spark_libs__/hive-beeline-1.2.1.spark2.jar
4325865 1848 -rwxr-xr-x   1 yarn     hadoop    1890075 Mar 27 00:50 ./__spark_libs__/datanucleus-core-3.2.10.jar
4325755  576 -rwxr-xr-x   1 yarn     hadoop     589462 Mar 27 00:50 ./__spark_libs__/json4s-core_2.11-3.2.11.jar
4326086  676 -rwxr-xr-x   1 yarn     hadoop     691479 Mar 27 00:50 ./__spark_libs__/apacheds-kerberos-codec-2.0.0-M15.jar
4325900   52 -rwxr-xr-x   1 yarn     hadoop      50619 Mar 27 00:50 ./__spark_libs__/chill-java-0.8.0.jar
4325934  720 -rwxr-xr-x   1 yarn     hadoop     736658 Mar 27 00:50 ./__spark_libs__/httpclient-4.5.2.jar
4325896  112 -rwxr-xr-x   1 yarn     hadoop     110600 Mar 27 00:50 ./__spark_libs__/bonecp-0.8.0.RELEASE.jar
4326134 1768 -rwxr-xr-x   1 yarn     hadoop    1809447 Mar 27 00:50 ./__spark_libs__/datanucleus-rdbms-3.2.9.jar
4326233   68 -rwxr-xr-x   1 yarn     hadoop      69409 Mar 27 00:50 ./__spark_libs__/activation-1.1.1.jar
4326115  208 -rwxr-xr-x   1 yarn     hadoop     212453 Mar 27 00:50 ./__spark_libs__/commons-net-2.2.jar
4326121   80 -rwxr-xr-x   1 yarn     hadoop      79912 Mar 27 00:50 ./__spark_libs__/api-util-1.0.0-M20.jar
4325952  184 -rwxr-xr-x   1 yarn     hadoop     185140 Mar 27 00:50 ./__spark_libs__/commons-io-2.4.jar
4325931  528 -rwxr-xr-x   1 yarn     hadoop     539912 Mar 27 00:50 ./__spark_libs__/jetty-6.1.26.jar
4326163   20 -rwxr-xr-x   1 yarn     hadoop      18482 Mar 27 00:50 ./__spark_libs__/eigenbase-properties-1.1.5.jar
4325969  436 -rwxr-xr-x   1 yarn     hadoop     445288 Mar 27 00:50 ./__spark_libs__/antlr-2.7.7.jar
4325918 1024 -rwxr-xr-x   1 yarn     hadoop    1045744 Mar 27 00:50 ./__spark_libs__/leveldbjni-all-1.8.jar
4326076  356 -rwxr-xr-x   1 yarn     hadoop     363908 Mar 27 00:50 ./__spark_libs__/hadoop-yarn-server-common-2.7.1.jar
4326165   64 -rwxr-xr-x   1 yarn     hadoop      62050 Mar 27 00:50 ./__spark_libs__/commons-logging-1.1.3.jar
4326055  480 -rwxr-xr-x   1 yarn     hadoop     489884 Mar 27 00:50 ./__spark_libs__/log4j-1.2.17.jar
4326022  280 -rwxr-xr-x   1 yarn     hadoop     285447 Mar 27 00:50 ./__spark_libs__/parquet-encoding-1.7.0.jar
11272626 1512 -rwxr-xr-x   1 yarn     hadoop    1544875 Apr  4 16:11 ./__spark_libs__/hadoop-mapreduce-client-core-2.7.1.jar
4325876  764 -rwxr-xr-x   1 yarn     hadoop     780664 Mar 27 00:50 ./__spark_libs__/jackson-mapper-asl-1.9.13.jar
4326173   20 -rwxr-xr-x   1 yarn     hadoop      20235 Mar 27 00:50 ./__spark_libs__/osgi-resource-locator-1.0.1.jar
4326078  300 -rwxr-xr-x   1 yarn     hadoop     305001 Mar 27 00:50 ./__spark_libs__/commons-httpclient-3.1.jar
4325997  352 -rwxr-xr-x   1 yarn     hadoop     358390 Mar 27 00:50 ./__spark_libs__/kryo-shaded-3.0.3.jar
4326142   16 -rwxr-xr-x   1 yarn     hadoop      15010 Mar 27 00:50 ./__spark_libs__/xmlenc-0.52.jar
4326275   24 -rwxr-xr-x   1 yarn     hadoop      21243 Mar 27 00:50 ./__spark_libs__/parquet-generator-1.7.0.jar
4326107 11452 -rwxr-xr-x   1 yarn     hadoop   11723537 Mar 27 00:50 ./__spark_libs__/spark-core_2.11-2.0.1.jar
4325869  280 -rwxr-xr-x   1 yarn     hadoop     284184 Mar 27 00:50 ./__spark_libs__/commons-codec-1.10.jar
4326123 2304 -rwxr-xr-x   1 yarn     hadoop    2355465 Mar 27 00:50 ./__spark_libs__/spark-network-common_2.11-2.0.1.jar
4325943  436 -rwxr-xr-x   1 yarn     hadoop     442406 Mar 27 00:50 ./__spark_libs__/calcite-linq4j-1.2.0-incubating.jar
4326044  116 -rwxr-xr-x   1 yarn     hadoop     115534 Mar 27 00:50 ./__spark_libs__/javax.ws.rs-api-2.0.1.jar
4326097  200 -rwxr-xr-x   1 yarn     hadoop     201124 Mar 27 00:50 ./__spark_libs__/jdo-api-3.0.1.jar
4326295 2008 -rwxr-xr-x   1 yarn     hadoop    2054931 Mar 27 00:50 ./__spark_libs__/netty-all-4.0.29.Final.jar
4326304    8 -rwxr-xr-x   1 yarn     hadoop       5310 Mar 27 00:50 ./__spark_libs__/pmml-schema-1.2.15.jar
4326111 1204 -rwxr-xr-x   1 yarn     hadoop    1229125 Mar 27 00:50 ./__spark_libs__/xercesImpl-2.9.1.jar
4326256  468 -rwxr-xr-x   1 yarn     hadoop     477970 Mar 27 00:50 ./__spark_libs__/lift-json_2.11-2.6.3.jar
4326175   24 -rwxr-xr-x   1 yarn     hadoop      20852 Mar 27 00:50 ./__spark_libs__/metrics-graphite-3.1.2.jar
4325793   40 -rwxr-xr-x   1 yarn     hadoop      39280 Mar 27 00:50 ./__spark_libs__/metrics-jvm-3.1.2.jar
4326057  380 -rwxr-xr-x   1 yarn     hadoop     387188 Mar 27 00:50 ./__spark_libs__/parquet-format-2.3.0-incubating.jar
4326247   16 -rwxr-xr-x   1 yarn     hadoop      15071 Mar 27 00:50 ./__spark_libs__/jta-1.1.jar
4326029  684 -rwxr-xr-x   1 yarn     hadoop     698375 Mar 27 00:50 ./__spark_libs__/jersey-common-2.22.2.jar
11272651  176 -rwxr-xr-x   1 yarn     hadoop     177131 Apr  4 16:11 ./__spark_libs__/jetty-util-6.1.26.jar
4326235 1144 -rwxr-xr-x   1 yarn     hadoop    1171380 Mar 27 00:50 ./__spark_libs__/jackson-databind-2.6.5.jar
4326284  308 -rwxr-xr-x   1 yarn     hadoop     313686 Mar 27 00:50 ./__spark_libs__/libfb303-0.9.2.jar
4325836  176 -rwxr-xr-x   1 yarn     hadoop     178947 Mar 27 00:50 ./__spark_libs__/hk2-api-2.4.0-b34.jar
4325831  788 -rwxr-xr-x   1 yarn     hadoop     802818 Mar 27 00:50 ./__spark_libs__/scalap-2.11.8.jar
4326178  184 -rwxr-xr-x   1 yarn     hadoop     185245 Mar 27 00:50 ./__spark_libs__/curator-framework-2.6.0.jar
4325872   40 -rwxr-xr-x   1 yarn     hadoop      40509 Mar 27 00:50 ./__spark_libs__/slf4j-api-1.7.16.jar
4326007  148 -rwxr-xr-x   1 yarn     hadoop     148627 Mar 27 00:50 ./__spark_libs__/stringtemplate-3.2.1.jar
4326020  416 -rwxr-xr-x   1 yarn     hadoop     423753 Mar 27 00:50 ./__spark_libs__/scala-parser-combinators_2.11-1.0.4.jar
4325960   72 -rwxr-xr-x   1 yarn     hadoop      72733 Mar 27 00:50 ./__spark_libs__/jersey-media-jaxb-2.22.2.jar
4325817  932 -rwxr-xr-x   1 yarn     hadoop     951701 Mar 27 00:50 ./__spark_libs__/jersey-server-2.22.2.jar
4325986   20 -rwxr-xr-x   1 yarn     hadoop      17385 Mar 27 00:50 ./__spark_libs__/hadoop-annotations-2.7.1.jar
4325978  104 -rwxr-xr-x   1 yarn     hadoop     105134 Mar 27 00:50 ./__spark_libs__/jaxb-api-2.2.2.jar
4326200   44 -rwxr-xr-x   1 yarn     hadoop      45015 Mar 27 00:50 ./__spark_libs__/hadoop-mapreduce-client-shuffle-2.7.1.jar
4326080  576 -rwxr-xr-x   1 yarn     hadoop     588337 Mar 27 00:50 ./__spark_libs__/commons-collections-3.2.2.jar
4325878  192 -rwxr-xr-x   1 yarn     hadoop     192993 Mar 27 00:50 ./__spark_libs__/avro-ipc-1.7.7.jar
4325795 2732 -rwxr-xr-x   1 yarn     hadoop    2796935 Mar 27 00:50 ./__spark_libs__/parquet-hadoop-bundle-1.6.0.jar
4326062  404 -rwxr-xr-x   1 yarn     hadoop     412739 Mar 27 00:50 ./__spark_libs__/commons-lang3-3.3.2.jar
4326197   28 -rwxr-xr-x   1 yarn     hadoop      27084 Mar 27 00:50 ./__spark_libs__/jackson-xc-1.9.13.jar
4326060   12 -rwxr-xr-x   1 yarn     hadoop      12131 Mar 27 00:50 ./__spark_libs__/jpam-1.1.jar
4325802  532 -rwxr-xr-x   1 yarn     hadoop     540852 Mar 27 00:50 ./__spark_libs__/mysql-connector-java-5.0.8-bin.jar
4326270   28 -rwxr-xr-x   1 yarn     hadoop      26514 Mar 27 00:50 ./__spark_libs__/stax-api-1.0.1.jar
4326254 1024 -rwxr-xr-x   1 yarn     hadoop    1044511 Mar 27 00:50 ./__spark_libs__/spark-hive_2.11-2.0.1.jar
4325728  600 -rwxr-xr-x   1 yarn     hadoop     613299 Mar 27 00:50 ./__spark_libs__/janino-2.7.8.jar
11272751   16 -rwxr-xr-x   1 yarn     hadoop      15827 Apr  4 16:11 ./__spark_libs__/metrics-json-3.1.2.jar
4326298  508 -rwxr-xr-x   1 yarn     hadoop     516127 Mar 27 00:50 ./__spark_libs__/hadoop-mapreduce-client-app-2.7.1.jar
4326229  440 -rwxr-xr-x   1 yarn     hadoop     448794 Mar 27 00:50 ./__spark_libs__/apache-log4j-extras-1.2.17.jar
4326104   28 -rwxr-xr-x   1 yarn     hadoop      26366 Mar 27 00:50 ./__spark_libs__/javax.annotation-api-1.2.jar
4326191   96 -rwxr-xr-x   1 yarn     hadoop      94672 Mar 27 00:50 ./__spark_libs__/xz-1.0.jar
4326041   24 -rwxr-xr-x   1 yarn     hadoop      23346 Mar 27 00:50 ./__spark_libs__/stax-api-1.0-2.jar
4326221   40 -rwxr-xr-x   1 yarn     hadoop      38134 Mar 27 00:50 ./__spark_libs__/hadoop-mapreduce-client-jobclient-2.7.1.jar
11272793   96 -rwxr-xr-x   1 yarn     hadoop      96221 Apr  4 16:11 ./__spark_libs__/commons-pool-1.5.4.jar
4325759   40 -rwxr-xr-x   1 yarn     hadoop      40341 Mar 27 00:50 ./__spark_libs__/json4s-jackson_2.11-3.2.11.jar
4326185  896 -rwxr-xr-x   1 yarn     hadoop     917052 Mar 27 00:50 ./__spark_libs__/parquet-column-1.7.0.jar
4326278  172 -rwxr-xr-x   1 yarn     hadoop     174351 Mar 27 00:50 ./__spark_libs__/stream-2.7.0.jar
4325890   12 -rwxr-xr-x   1 yarn     hadoop       9939 Mar 27 00:50 ./__spark_libs__/slf4j-log4j12-1.7.16.jar
11272748   44 -rwxr-xr-x   1 yarn     hadoop      44925 Apr  4 16:11 ./__spark_libs__/apacheds-i18n-2.0.0-M15.jar
11272785    8 -rwxr-xr-x   1 yarn     hadoop       5950 Apr  4 16:11 ./__spark_libs__/javax.inject-2.4.0-b34.jar
11272788 1444 -rwxr-xr-x   1 yarn     hadoop    1475955 Apr  4 16:11 ./__spark_libs__/htrace-core-3.1.0-incubating.jar
4325813   64 -rwxr-xr-x   1 yarn     hadoop      63316 Mar 27 00:50 ./__spark_libs__/spark-repl_2.11-2.0.1.jar
11272784   96 -rwxr-xr-x   1 yarn     hadoop      95806 Apr  4 16:11 ./__spark_libs__/javax.servlet-api-3.1.0.jar
4325768   20 -rwxr-xr-x   1 yarn     hadoop      16430 Mar 27 00:50 ./__spark_libs__/jcl-over-slf4j-1.7.16.jar
4326137  616 -rwxr-xr-x   1 yarn     hadoop     627814 Mar 27 00:50 ./__spark_libs__/joda-time-2.9.3.jar
4325791 1988 -rwxr-xr-x   1 yarn     hadoop    2035066 Mar 27 00:50 ./__spark_libs__/commons-math3-3.4.1.jar
4326052  292 -rwxr-xr-x   1 yarn     hadoop     298829 Mar 27 00:50 ./__spark_libs__/commons-configuration-1.6.jar
4325925    8 -rwxr-xr-x   1 yarn     hadoop       4467 Mar 27 00:50 ./__spark_libs__/aopalliance-1.0.jar
4325966  164 -rwxr-xr-x   1 yarn     hadoop     164368 Mar 27 00:50 ./__spark_libs__/antlr-runtime-3.4.jar
4325999  332 -rwxr-xr-x   1 yarn     hadoop     339666 Mar 27 00:50 ./__spark_libs__/datanucleus-api-jdo-3.2.6.jar
4325949  116 -rwxr-xr-x   1 yarn     hadoop     114913 Mar 27 00:50 ./__spark_libs__/py4j-0.10.3.jar
4325862  164 -rwxr-xr-x   1 yarn     hadoop     164422 Mar 27 00:50 ./__spark_libs__/core-1.1.2.jar
4326025  748 -rwxr-xr-x   1 yarn     hadoop     764569 Mar 27 00:50 ./__spark_libs__/jtransforms-2.4.0.jar
4326064  100 -rwxr-xr-x   1 yarn     hadoop     100680 Mar 27 00:50 ./__spark_libs__/hive-jdbc-1.2.1.spark2.jar
4325757 2044 -rwxr-xr-x   1 yarn     hadoop    2090370 Mar 27 00:50 ./__spark_libs__/spark-streaming_2.11-2.0.1.jar
4325860 1256 -rwxr-xr-x   1 yarn     hadoop    1282424 Mar 27 00:50 ./__spark_libs__/ivy-2.4.0.jar
4326002   16 -rwxr-xr-x   1 yarn     hadoop      14766 Mar 27 00:50 ./__spark_libs__/aopalliance-repackaged-2.4.0-b34.jar
4326040  952 -rwxr-xr-x   1 yarn     hadoop     971310 Mar 27 00:50 ./__spark_libs__/jersey-guava-2.22.2.jar
4325775  640 -rwxr-xr-x   1 yarn     hadoop     654216 Mar 27 00:50 ./__spark_libs__/pmml-model-1.2.15.jar
11272763   48 -rwxr-xr-x   1 yarn     hadoop      46983 Apr  4 16:11 ./__spark_libs__/jackson-annotations-2.6.5.jar
4326302  736 -rwxr-xr-x   1 yarn     hadoop     753012 Mar 27 00:50 ./__spark_libs__/hadoop-mapreduce-client-common-2.7.1.jar
11272849 8068 -rwxr-xr-x   1 yarn     hadoop    8260573 Apr  4 16:11 ./__spark_libs__/hadoop-hdfs-2.7.1.jar
11272797   16 -rwxr-xr-x   1 yarn     hadoop      15305 Apr  4 16:11 ./__spark_libs__/spark-tags_2.11-2.0.1.jar
11272823   64 -rwxr-xr-x   1 yarn     hadoop      63777 Apr  4 16:11 ./__spark_libs__/validation-api-1.1.0.Final.jar
4326294  164 -rwxr-xr-x   1 yarn     hadoop     167421 Mar 27 00:50 ./__spark_libs__/jersey-client-2.22.2.jar
4325856  136 -rwxr-xr-x   1 yarn     hadoop     135552 Mar 27 00:50 ./__spark_libs__/breeze-macros_2.11-0.11.2.jar
4326249  388 -rwxr-xr-x   1 yarn     hadoop     395195 Mar 27 00:50 ./__spark_libs__/javolution-5.5.1.jar
4325920   20 -rwxr-xr-x   1 yarn     hadoop      17008 Mar 27 00:50 ./__spark_libs__/base64-2.3.8.jar
4326169  320 -rwxr-xr-x   1 yarn     hadoop     326724 Mar 27 00:50 ./__spark_libs__/httpcore-4.4.4.jar
4326293 7108 -rwxr-xr-x   1 yarn     hadoop    7276083 Mar 27 00:50 ./__spark_libs__/spire_2.11-0.7.4.jar
4325892  100 -rwxr-xr-x   1 yarn     hadoop     100636 Mar 27 00:50 ./__spark_libs__/jsp-api-2.1.jar
11272779  636 -rwxr-xr-x   1 yarn     hadoop     648678 Apr  4 16:11 ./__spark_libs__/scala-xml_2.11-1.0.2.jar
4326189 1024 -rwxr-xr-x   1 yarn     hadoop    1048110 Mar 27 00:50 ./__spark_libs__/parquet-jackson-1.7.0.jar
11272761   56 -rwxr-xr-x   1 yarn     hadoop      55511 Apr  4 16:11 ./__spark_libs__/spark-network-shuffle_2.11-2.0.1.jar
4326219  504 -rwxr-xr-x   1 yarn     hadoop     515604 Mar 27 00:50 ./__spark_libs__/jackson-module-scala_2.11-2.6.5.jar
4325928  180 -rwxr-xr-x   1 yarn     hadoop     181271 Mar 27 00:50 ./__spark_libs__/hk2-locator-2.4.0-b34.jar
4325846   20 -rwxr-xr-x   1 yarn     hadoop      18098 Mar 27 00:50 ./__spark_libs__/jersey-container-servlet-2.22.2.jar
4326126   64 -rwxr-xr-x   1 yarn     hadoop      65261 Mar 27 00:50 ./__spark_libs__/oro-2.0.8.jar
11272622  112 -rwxr-xr-x   1 yarn     hadoop     112558 Apr  4 16:11 ./__spark_libs__/metrics-core-3.1.2.jar
4325825 1996 -rwxr-xr-x   1 yarn     hadoop    2041628 Mar 27 00:50 ./__spark_libs__/jets3t-0.9.3.jar
4326264  400 -rwxr-xr-x   1 yarn     hadoop     409467 Mar 27 00:50 ./__spark_libs__/mx4j-3.0.2.jar
4326252  200 -rwxr-xr-x   1 yarn     hadoop     201928 Mar 27 00:50 ./__spark_libs__/RoaringBitmap-0.5.11.jar
4325777  188 -rwxr-xr-x   1 yarn     hadoop     188671 Mar 27 00:50 ./__spark_libs__/commons-beanutils-1.7.0.jar
4326167 1032 -rwxr-xr-x   1 yarn     hadoop    1056168 Mar 27 00:50 ./__spark_libs__/snappy-java-1.1.2.6.jar
4325954 4468 -rwxr-xr-x   1 yarn     hadoop    4573750 Mar 27 00:50 ./__spark_libs__/scala-reflect-2.11.8.jar
4326046 1616 -rwxr-xr-x   1 yarn     hadoop    1654097 Mar 27 00:50 ./__spark_libs__/hadoop-yarn-common-2.7.1.jar
4326182  512 -rwxr-xr-x   1 yarn     hadoop     521157 Mar 27 00:50 ./__spark_libs__/mail-1.4.7.jar
11272639  180 -rwxr-xr-x   1 yarn     hadoop     180736 Apr  4 16:11 ./__spark_libs__/avro-mapred-1.7.7-hadoop2.jar
4325990 1204 -rwxr-xr-x   1 yarn     hadoop    1230201 Mar 27 00:50 ./__spark_libs__/netty-3.8.0.Final.jar
4325907    4 -rwxr-xr-x   1 yarn     hadoop       2497 Mar 27 00:50 ./__spark_libs__/javax.inject-1.jar
11272713   44 -rwxr-xr-x   1 yarn     hadoop      41755 Apr  4 16:11 ./__spark_libs__/objenesis-2.1.jar
4325799   32 -rwxr-xr-x   1 yarn     hadoop      29555 Mar 27 00:50 ./__spark_libs__/paranamer-2.3.jar
4325806   20 -rwxr-xr-x   1 yarn     hadoop      16993 Mar 27 00:50 ./__spark_libs__/JavaEWAH-0.3.2.jar
4325867  656 -rwxr-xr-x   1 yarn     hadoop     669589 Mar 27 00:50 ./__spark_libs__/spark-yarn_2.11-2.0.1.jar
4325912   80 -rwxr-xr-x   1 yarn     hadoop      79845 Mar 27 00:50 ./__spark_libs__/compress-lzf-1.0.3.jar
4326215  296 -rwxr-xr-x   1 yarn     hadoop     302248 Mar 27 00:50 ./__spark_libs__/antlr4-runtime-4.5.3.jar
4326117 1168 -rwxr-xr-x   1 yarn     hadoop    1194003 Mar 27 00:50 ./__spark_libs__/arpack_combined_all-0.1.jar
4325963   32 -rwxr-xr-x   1 yarn     hadoop      29540 Mar 27 00:50 ./__spark_libs__/spark-sketch_2.11-2.0.1.jar
4325761 15128 -rwxr-xr-x   1 yarn     hadoop   15487351 Mar 27 00:50 ./__spark_libs__/scala-compiler-2.11.8.jar
4325898 1592 -rwxr-xr-x   1 yarn     hadoop    1627065 Mar 27 00:50 ./__spark_libs__/jersey-bundle-1.19.1.jar
4326231 2140 -rwxr-xr-x   1 yarn     hadoop    2189117 Mar 27 00:50 ./__spark_libs__/guava-14.0.1.jar
4326267 1772 -rwxr-xr-x   1 yarn     hadoop    1814309 Mar 27 00:50 ./__spark_libs__/spark-hive-thriftserver_2.11-2.0.1.jar
4325923 11232 -rwxr-xr-x   1 yarn     hadoop   11498852 Mar 27 00:50 ./__spark_libs__/hive-exec-1.2.1.spark2.jar
4325786   68 -rwxr-xr-x   1 yarn     hadoop      66270 Mar 27 00:50 ./__spark_libs__/jersey-container-servlet-core-2.22.2.jar
4325765  428 -rwxr-xr-x   1 yarn     hadoop     436303 Mar 27 00:50 ./__spark_libs__/avro-1.7.7.jar
4325945 5612 -rwxr-xr-x   1 yarn     hadoop    5744974 Mar 27 00:50 ./__spark_libs__/scala-library-2.11.8.jar
4326241  420 -rwxr-xr-x   1 yarn     hadoop     427780 Mar 27 00:50 ./__spark_libs__/jodd-core-3.5.2.jar
4326206 1972 -rwxr-xr-x   1 yarn     hadoop    2015514 Mar 27 00:50 ./__spark_libs__/hadoop-yarn-api-2.7.1.jar
11272682  776 -rwxr-xr-x   1 yarn     hadoop     792964 Apr  4 16:11 ./__spark_libs__/zookeeper-3.4.6.jar
4326031   20 -rwxr-xr-x   1 yarn     hadoop      16560 Mar 27 00:50 ./__spark_libs__/api-asn1-api-1.0.0-M20.jar
4326013 5380 -rwxr-xr-x   1 yarn     hadoop    5505200 Mar 27 00:50 ./__spark_libs__/hive-metastore-1.2.1.spark2.jar
4325973   68 -rwxr-xr-x   1 yarn     hadoop      68866 Mar 27 00:50 ./__spark_libs__/curator-client-2.6.0.jar
4326213  212 -rwxr-xr-x   1 yarn     hadoop     213911 Mar 27 00:50 ./__spark_libs__/jline-2.12.1.jar
4326074   24 -rwxr-xr-x   1 yarn     hadoop      21575 Mar 27 00:50 ./__spark_libs__/parquet-common-1.7.0.jar
11272735   64 -rwxr-xr-x   1 yarn     hadoop      65012 Apr  4 16:11 ./__spark_libs__/guice-servlet-3.0.jar
4325808  256 -rwxr-xr-x   1 yarn     hadoop     258876 Mar 27 00:50 ./__spark_libs__/jackson-core-2.6.5.jar
786447   76 -rwx------   1 yarn     hadoop      75763 May 16 11:10 ./launch_container.sh
786451    4 -rwx------   1 yarn     hadoop        730 May 16 11:10 ./default_container_executor.sh
786443    4 drwx--x---   2 yarn     hadoop       4096 May 16 11:10 ./tmp
broken symlinks(find -L . -maxdepth 5 -type l -ls):
End of LogType:directory.info

LogType:launch_container.sh
Log Upload Time:Tue May 16 11:12:19 +0700 2017
LogLength:75763
Log Contents:
#!/bin/bash

export SPARK_YARN_STAGING_DIR="hdfs://c408.hadoop.gda.lo:8020/user/fairy/.sparkStaging/application_1494395298335_303847"
export LOCAL_DIRS="/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847"
export HADOOP_CONF_DIR="/usr/hdp/current/hadoop-client/conf"
export NM_HTTP_PORT="8042"
export JAVA_HOME="/usr/jdk64/jdk1.8.0_92/"
export LOG_DIRS="/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000004"
export NM_AUX_SERVICE_mapreduce_shuffle="AAA0+gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=
"
export NM_PORT="45454"
export USER="fairy"
export HADOOP_YARN_HOME="/usr/hdp/current/hadoop-yarn-nodemanager"
export CLASSPATH="$PWD:$PWD/__spark_conf__:$PWD/__spark_libs__/*:$HADOOP_CONF_DIR:/usr/hdp/current/hadoop-client/*:/usr/hdp/current/hadoop-client/lib/*:/usr/hdp/current/hadoop-hdfs-client/*:/usr/hdp/current/hadoop-hdfs-client/lib/*:/usr/hdp/current/hadoop-yarn-client/*:/usr/hdp/current/hadoop-yarn-client/lib/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.4.2.0-258/hadoop/lib/hadoop-lzo-0.6.0.2.4.2.0-258.jar:/etc/hadoop/conf/secure"
export SPARK_LOG_URL_STDOUT="http://c411.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_01_000004/fairy/stdout?start=-4096"
export NM_HOST="c411.hadoop.gda.lo"
export SPARK_YARN_MODE="true"
export HADOOP_TOKEN_FILE_LOCATION="/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_01_000004/container_tokens"
export NM_AUX_SERVICE_spark_shuffle=""
export SPARK_USER="fairy"
export SPARK_LOG_URL_STDERR="http://c411.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_01_000004/fairy/stderr?start=-4096"
export LOGNAME="fairy"
export JVM_PID="$$"
export PWD="/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_01_000004"
export HOME="/home/"
export CONTAINER_ID="container_e81_1494395298335_303847_01_000004"
export MALLOC_ARENA_MAX="4"
ln -sf "/hadoop/yarn/local/usercache/fairy/filecache/34842/__spark_conf__.zip" "__spark_conf__"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65905/xz-1.0.jar" "__spark_libs__/xz-1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65886/joda-time-2.9.3.jar" "__spark_libs__/joda-time-2.9.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65812/ST4-4.0.4.jar" "__spark_libs__/ST4-4.0.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65817/jetty-6.1.26.jar" "__spark_libs__/jetty-6.1.26.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65939/jersey-client-2.22.2.jar" "__spark_libs__/jersey-client-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65834/bcprov-jdk15on-1.51.jar" "__spark_libs__/bcprov-jdk15on-1.51.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65784/commons-dbcp-1.4.jar" "__spark_libs__/commons-dbcp-1.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65832/commons-io-2.4.jar" "__spark_libs__/commons-io-2.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65880/spark-sql_2.11-2.0.1.jar" "__spark_libs__/spark-sql_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65849/jtransforms-2.4.0.jar" "__spark_libs__/jtransforms-2.4.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65860/parquet-format-2.3.0-incubating.jar" "__spark_libs__/parquet-format-2.3.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65871/spark-mllib_2.11-2.0.1.jar" "__spark_libs__/spark-mllib_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65778/jackson-core-2.6.5.jar" "__spark_libs__/jackson-core-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69768/calcite-core-1.2.0-incubating.jar" "__spark_libs__/calcite-core-1.2.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69777/scala-xml_2.11-1.0.2.jar" "__spark_libs__/scala-xml_2.11-1.0.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65913/hadoop-common-2.7.1.jar" "__spark_libs__/hadoop-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65833/jaxb-api-2.2.2.jar" "__spark_libs__/jaxb-api-2.2.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65842/commons-lang-2.6.jar" "__spark_libs__/commons-lang-2.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69782/spark-tags_2.11-2.0.1.jar" "__spark_libs__/spark-tags_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65806/bonecp-0.8.0.RELEASE.jar" "__spark_libs__/bonecp-0.8.0.RELEASE.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65822/scala-library-2.11.8.jar" "__spark_libs__/scala-library-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65890/java-xmlbuilder-1.0.jar" "__spark_libs__/java-xmlbuilder-1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65896/snappy-java-1.1.2.6.jar" "__spark_libs__/snappy-java-1.1.2.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65768/spark-mllib-local_2.11-2.0.1.jar" "__spark_libs__/spark-mllib-local_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65780/jersey-server-2.22.2.jar" "__spark_libs__/jersey-server-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65864/commons-lang3-3.3.2.jar" "__spark_libs__/commons-lang3-3.3.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65776/JavaEWAH-0.3.2.jar" "__spark_libs__/JavaEWAH-0.3.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65830/antlr-2.7.7.jar" "__spark_libs__/antlr-2.7.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65912/jline-2.12.1.jar" "__spark_libs__/jline-2.12.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65829/protobuf-java-2.5.0.jar" "__spark_libs__/protobuf-java-2.5.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65888/xmlenc-0.52.jar" "__spark_libs__/xmlenc-0.52.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69767/super-csv-2.2.0.jar" "__spark_libs__/super-csv-2.2.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65762/avro-1.7.7.jar" "__spark_libs__/avro-1.7.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65802/spark-launcher_2.11-2.0.1.jar" "__spark_libs__/spark-launcher_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65887/jsr305-1.3.9.jar" "__spark_libs__/jsr305-1.3.9.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65932/commons-cli-1.2.jar" "__spark_libs__/commons-cli-1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69781/commons-pool-1.5.4.jar" "__spark_libs__/commons-pool-1.5.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65920/activation-1.1.1.jar" "__spark_libs__/activation-1.1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65804/slf4j-log4j12-1.7.16.jar" "__spark_libs__/slf4j-log4j12-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65809/javax.inject-1.jar" "__spark_libs__/javax.inject-1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65869/apacheds-kerberos-codec-2.0.0-M15.jar" "__spark_libs__/apacheds-kerberos-codec-2.0.0-M15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69779/javax.inject-2.4.0-b34.jar" "__spark_libs__/javax.inject-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65885/datanucleus-rdbms-3.2.9.jar" "__spark_libs__/datanucleus-rdbms-3.2.9.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65884/derby-10.12.1.1.jar" "__spark_libs__/derby-10.12.1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69760/metrics-core-3.1.2.jar" "__spark_libs__/metrics-core-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65799/spark-yarn_2.11-2.0.1.jar" "__spark_libs__/spark-yarn_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65915/arpack_combined_all-0.1.jar" "__spark_libs__/arpack_combined_all-0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69762/libthrift-0.9.2.jar" "__spark_libs__/libthrift-0.9.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65767/json4s-core_2.11-3.2.11.jar" "__spark_libs__/json4s-core_2.11-3.2.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65790/opencsv-2.3.jar" "__spark_libs__/opencsv-2.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/usercache/fairy/filecache/34841/stats-etlr-1.0.jar" "__app__.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65936/libfb303-0.9.2.jar" "__spark_libs__/libfb303-0.9.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65847/scala-parser-combinators_2.11-1.0.4.jar" "__spark_libs__/scala-parser-combinators_2.11-1.0.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65771/commons-math3-3.4.1.jar" "__spark_libs__/commons-math3-3.4.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65825/scala-reflect-2.11.8.jar" "__spark_libs__/scala-reflect-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65785/univocity-parsers-2.1.1.jar" "__spark_libs__/univocity-parsers-2.1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65786/hk2-api-2.4.0-b34.jar" "__spark_libs__/hk2-api-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69784/validation-api-1.1.0.Final.jar" "__spark_libs__/validation-api-1.1.0.Final.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65846/spark-unsafe_2.11-2.0.1.jar" "__spark_libs__/spark-unsafe_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65908/hadoop-mapreduce-client-shuffle-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-shuffle-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65927/hk2-utils-2.4.0-b34.jar" "__spark_libs__/hk2-utils-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65916/hadoop-mapreduce-client-jobclient-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-jobclient-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65838/curator-client-2.6.0.jar" "__spark_libs__/curator-client-2.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65914/jackson-module-scala_2.11-2.6.5.jar" "__spark_libs__/jackson-module-scala_2.11-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69770/apacheds-i18n-2.0.0-M15.jar" "__spark_libs__/apacheds-i18n-2.0.0-M15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65917/apache-log4j-extras-1.2.17.jar" "__spark_libs__/apache-log4j-extras-1.2.17.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65800/jackson-mapper-asl-1.9.13.jar" "__spark_libs__/jackson-mapper-asl-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65902/mail-1.4.7.jar" "__spark_libs__/mail-1.4.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65933/parquet-generator-1.7.0.jar" "__spark_libs__/parquet-generator-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65937/spire_2.11-0.7.4.jar" "__spark_libs__/spire_2.11-0.7.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65892/guice-3.0.jar" "__spark_libs__/guice-3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65873/jdo-api-3.0.1.jar" "__spark_libs__/jdo-api-3.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65779/spark-repl_2.11-2.0.1.jar" "__spark_libs__/spark-repl_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65823/commons-compress-1.4.1.jar" "__spark_libs__/commons-compress-1.4.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65761/scala-compiler-2.11.8.jar" "__spark_libs__/scala-compiler-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65883/spark-network-common_2.11-2.0.1.jar" "__spark_libs__/spark-network-common_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65764/json4s-jackson_2.11-3.2.11.jar" "__spark_libs__/json4s-jackson_2.11-3.2.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65901/commons-logging-1.1.3.jar" "__spark_libs__/commons-logging-1.1.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69764/avro-mapred-1.7.7-hadoop2.jar" "__spark_libs__/avro-mapred-1.7.7-hadoop2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65808/jersey-bundle-1.19.1.jar" "__spark_libs__/jersey-bundle-1.19.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65877/spark-core_2.11-2.0.1.jar" "__spark_libs__/spark-core_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65795/core-1.1.2.jar" "__spark_libs__/core-1.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65774/paranamer-2.3.jar" "__spark_libs__/paranamer-2.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65796/datanucleus-core-3.2.10.jar" "__spark_libs__/datanucleus-core-3.2.10.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65811/compress-lzf-1.0.3.jar" "__spark_libs__/compress-lzf-1.0.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65882/hive-beeline-1.2.1.spark2.jar" "__spark_libs__/hive-beeline-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65942/pmml-schema-1.2.15.jar" "__spark_libs__/pmml-schema-1.2.15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65857/py4j-0.10.3.jar" "__spark_libs__/py4j-0.10.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65793/breeze-macros_2.11-0.11.2.jar" "__spark_libs__/breeze-macros_2.11-0.11.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69774/hive-cli-1.2.1.spark2.jar" "__spark_libs__/hive-cli-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65845/hive-metastore-1.2.1.spark2.jar" "__spark_libs__/hive-metastore-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65854/stax-api-1.0-2.jar" "__spark_libs__/stax-api-1.0-2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65926/lift-json_2.11-2.6.3.jar" "__spark_libs__/lift-json_2.11-2.6.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65760/spark-streaming_2.11-2.0.1.jar" "__spark_libs__/spark-streaming_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65904/parquet-jackson-1.7.0.jar" "__spark_libs__/parquet-jackson-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65894/javassist-3.18.1-GA.jar" "__spark_libs__/javassist-3.18.1-GA.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65906/jackson-jaxrs-1.9.13.jar" "__spark_libs__/jackson-jaxrs-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65866/hadoop-yarn-server-common-2.7.1.jar" "__spark_libs__/hadoop-yarn-server-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65895/eigenbase-properties-1.1.5.jar" "__spark_libs__/eigenbase-properties-1.1.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65909/spark-graphx_2.11-2.0.1.jar" "__spark_libs__/spark-graphx_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65875/minlog-1.3.0.jar" "__spark_libs__/minlog-1.3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65848/parquet-encoding-1.7.0.jar" "__spark_libs__/parquet-encoding-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65815/aopalliance-1.0.jar" "__spark_libs__/aopalliance-1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65930/spark-hive-thriftserver_2.11-2.0.1.jar" "__spark_libs__/spark-hive-thriftserver_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65766/commons-beanutils-1.7.0.jar" "__spark_libs__/commons-beanutils-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65881/api-util-1.0.0-M20.jar" "__spark_libs__/api-util-1.0.0-M20.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65837/netty-3.8.0.Final.jar" "__spark_libs__/netty-3.8.0.Final.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65899/metrics-graphite-3.1.2.jar" "__spark_libs__/metrics-graphite-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65770/xbean-asm5-shaded-4.4.jar" "__spark_libs__/xbean-asm5-shaded-4.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69778/javax.servlet-api-3.1.0.jar" "__spark_libs__/javax.servlet-api-3.1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65789/commons-digester-1.8.jar" "__spark_libs__/commons-digester-1.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65828/spark-sketch_2.11-2.0.1.jar" "__spark_libs__/spark-sketch_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65879/commons-net-2.2.jar" "__spark_libs__/commons-net-2.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65772/metrics-jvm-3.1.2.jar" "__spark_libs__/metrics-jvm-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65928/spark-hive_2.11-2.0.1.jar" "__spark_libs__/spark-hive_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65874/jul-to-slf4j-1.7.16.jar" "__spark_libs__/jul-to-slf4j-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65911/hadoop-yarn-api-2.7.1.jar" "__spark_libs__/hadoop-yarn-api-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65840/datanucleus-api-jdo-3.2.6.jar" "__spark_libs__/datanucleus-api-jdo-3.2.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65863/hadoop-auth-2.7.1.jar" "__spark_libs__/hadoop-auth-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65876/javax.annotation-api-1.2.jar" "__spark_libs__/javax.annotation-api-1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65893/oro-2.0.8.jar" "__spark_libs__/oro-2.0.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65868/chill_2.11-0.8.0.jar" "__spark_libs__/chill_2.11-0.8.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65844/jsp-api-2.1.jar" "__spark_libs__/jsp-api-2.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65816/base64-2.3.8.jar" "__spark_libs__/base64-2.3.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65781/curator-recipes-2.6.0.jar" "__spark_libs__/curator-recipes-2.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65918/guava-14.0.1.jar" "__spark_libs__/guava-14.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65787/scalap-2.11.8.jar" "__spark_libs__/scalap-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65941/hadoop-mapreduce-client-common-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65858/commons-configuration-1.6.jar" "__spark_libs__/commons-configuration-1.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65921/jodd-core-3.5.2.jar" "__spark_libs__/jodd-core-3.5.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65862/hive-jdbc-1.2.1.spark2.jar" "__spark_libs__/hive-jdbc-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65819/avro-ipc-1.7.7.jar" "__spark_libs__/avro-ipc-1.7.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65783/jets3t-0.9.3.jar" "__spark_libs__/jets3t-0.9.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65835/spire-macros_2.11-0.7.4.jar" "__spark_libs__/spire-macros_2.11-0.7.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69783/objenesis-2.1.jar" "__spark_libs__/objenesis-2.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65865/parquet-common-1.7.0.jar" "__spark_libs__/parquet-common-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65775/mysql-connector-java-5.0.8-bin.jar" "__spark_libs__/mysql-connector-java-5.0.8-bin.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69769/guice-servlet-3.0.jar" "__spark_libs__/guice-servlet-3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69773/jackson-annotations-2.6.5.jar" "__spark_libs__/jackson-annotations-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65836/mesos-0.21.1-shaded-protobuf.jar" "__spark_libs__/mesos-0.21.1-shaded-protobuf.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65839/hadoop-annotations-2.7.1.jar" "__spark_libs__/hadoop-annotations-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69763/commons-beanutils-core-1.8.0.jar" "__spark_libs__/commons-beanutils-core-1.8.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65818/hk2-locator-2.4.0-b34.jar" "__spark_libs__/hk2-locator-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65821/calcite-linq4j-1.2.0-incubating.jar" "__spark_libs__/calcite-linq4j-1.2.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65891/kryo-shaded-3.0.3.jar" "__spark_libs__/kryo-shaded-3.0.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65782/jackson-module-paranamer-2.6.5.jar" "__spark_libs__/jackson-module-paranamer-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65938/netty-all-4.0.29.Final.jar" "__spark_libs__/netty-all-4.0.29.Final.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65803/hadoop-yarn-server-web-proxy-2.7.1.jar" "__spark_libs__/hadoop-yarn-server-web-proxy-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65841/aopalliance-repackaged-2.4.0-b34.jar" "__spark_libs__/aopalliance-repackaged-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65898/osgi-resource-locator-1.0.1.jar" "__spark_libs__/osgi-resource-locator-1.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65850/jersey-common-2.22.2.jar" "__spark_libs__/jersey-common-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65931/stax-api-1.0.1.jar" "__spark_libs__/stax-api-1.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65855/javax.ws.rs-api-2.0.1.jar" "__spark_libs__/javax.ws.rs-api-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65820/httpclient-4.5.2.jar" "__spark_libs__/httpclient-4.5.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65773/parquet-hadoop-bundle-1.6.0.jar" "__spark_libs__/parquet-hadoop-bundle-1.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65788/jackson-core-asl-1.9.13.jar" "__spark_libs__/jackson-core-asl-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65900/curator-framework-2.6.0.jar" "__spark_libs__/curator-framework-2.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65777/gson-2.2.4.jar" "__spark_libs__/gson-2.2.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65935/antlr4-runtime-4.5.3.jar" "__spark_libs__/antlr4-runtime-4.5.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65807/chill-java-0.8.0.jar" "__spark_libs__/chill-java-0.8.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69761/hadoop-mapreduce-client-core-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-core-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65826/spark-catalyst_2.11-2.0.1.jar" "__spark_libs__/spark-catalyst_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65831/antlr-runtime-3.4.jar" "__spark_libs__/antlr-runtime-3.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65878/xercesImpl-2.9.1.jar" "__spark_libs__/xercesImpl-2.9.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65924/javolution-5.5.1.jar" "__spark_libs__/javolution-5.5.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65907/jackson-xc-1.9.13.jar" "__spark_libs__/jackson-xc-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65814/hive-exec-1.2.1.spark2.jar" "__spark_libs__/hive-exec-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65852/snappy-0.2.jar" "__spark_libs__/snappy-0.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65794/pyrolite-4.9.jar" "__spark_libs__/pyrolite-4.9.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65843/stringtemplate-3.2.1.jar" "__spark_libs__/stringtemplate-3.2.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65872/commons-httpclient-3.1.jar" "__spark_libs__/commons-httpclient-3.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65929/mx4j-3.0.2.jar" "__spark_libs__/mx4j-3.0.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65925/RoaringBitmap-0.5.11.jar" "__spark_libs__/RoaringBitmap-0.5.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65813/leveldbjni-all-1.8.jar" "__spark_libs__/leveldbjni-all-1.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65922/hadoop-yarn-client-2.7.1.jar" "__spark_libs__/hadoop-yarn-client-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65897/httpcore-4.4.4.jar" "__spark_libs__/httpcore-4.4.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69785/hadoop-hdfs-2.7.1.jar" "__spark_libs__/hadoop-hdfs-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65765/pmml-model-1.2.15.jar" "__spark_libs__/pmml-model-1.2.15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65798/slf4j-api-1.7.16.jar" "__spark_libs__/slf4j-api-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65769/jersey-container-servlet-core-2.22.2.jar" "__spark_libs__/jersey-container-servlet-core-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65919/jackson-databind-2.6.5.jar" "__spark_libs__/jackson-databind-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65940/hadoop-mapreduce-client-app-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-app-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69772/spark-network-shuffle_2.11-2.0.1.jar" "__spark_libs__/spark-network-shuffle_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65763/jcl-over-slf4j-1.7.16.jar" "__spark_libs__/jcl-over-slf4j-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69775/json-20090211.jar" "__spark_libs__/json-20090211.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65934/stream-2.7.0.jar" "__spark_libs__/stream-2.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65791/jersey-container-servlet-2.22.2.jar" "__spark_libs__/jersey-container-servlet-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69766/zookeeper-3.4.6.jar" "__spark_libs__/zookeeper-3.4.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69771/metrics-json-3.1.2.jar" "__spark_libs__/metrics-json-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65889/commons-compiler-2.7.6.jar" "__spark_libs__/commons-compiler-2.7.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65853/jersey-guava-2.22.2.jar" "__spark_libs__/jersey-guava-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65805/ivy-2.4.0.jar" "__spark_libs__/ivy-2.4.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65797/commons-codec-1.10.jar" "__spark_libs__/commons-codec-1.10.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65810/parquet-hadoop-1.7.0.jar" "__spark_libs__/parquet-hadoop-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65867/commons-collections-3.2.2.jar" "__spark_libs__/commons-collections-3.2.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65824/calcite-avatica-1.2.0-incubating.jar" "__spark_libs__/calcite-avatica-1.2.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65827/jersey-media-jaxb-2.22.2.jar" "__spark_libs__/jersey-media-jaxb-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69765/jetty-util-6.1.26.jar" "__spark_libs__/jetty-util-6.1.26.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69780/htrace-core-3.1.0-incubating.jar" "__spark_libs__/htrace-core-3.1.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65792/janino-2.7.8.jar" "__spark_libs__/janino-2.7.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65903/parquet-column-1.7.0.jar" "__spark_libs__/parquet-column-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65856/hadoop-yarn-common-2.7.1.jar" "__spark_libs__/hadoop-yarn-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65870/lz4-1.3.0.jar" "__spark_libs__/lz4-1.3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65851/api-asn1-api-1.0.0-M20.jar" "__spark_libs__/api-asn1-api-1.0.0-M20.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69776/hadoop-client-2.7.1.jar" "__spark_libs__/hadoop-client-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65801/breeze_2.11-0.11.2.jar" "__spark_libs__/breeze_2.11-0.11.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65910/json4s-ast_2.11-3.2.11.jar" "__spark_libs__/json4s-ast_2.11-3.2.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65923/jta-1.1.jar" "__spark_libs__/jta-1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65861/jpam-1.1.jar" "__spark_libs__/jpam-1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65859/log4j-1.2.17.jar" "__spark_libs__/log4j-1.2.17.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
# Creating copy of launch script
cp "launch_container.sh" "/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000004/launch_container.sh"
chmod 640 "/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000004/launch_container.sh"
# Determining directory contents
echo "ls -l:" 1>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000004/directory.info"
ls -l 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000004/directory.info"
echo "find -L . -maxdepth 5 -ls:" 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000004/directory.info"
find -L . -maxdepth 5 -ls 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000004/directory.info"
echo "broken symlinks(find -L . -maxdepth 5 -type l -ls):" 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000004/directory.info"
find -L . -maxdepth 5 -type l -ls 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000004/directory.info"
exec /bin/bash -c "$JAVA_HOME/bin/java -server -Xmx3072m '-Dhdp.version=2.4.2.0-258' -Djava.io.tmpdir=$PWD/tmp '-Dspark.history.ui.port=18080' '-Dspark.ui.port=0' '-Dspark.driver.port=19210' -Dspark.yarn.app.container.log.dir=/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000004 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.60.43.33:19210 --executor-id 3 --hostname c411.hadoop.gda.lo --cores 1 --app-id application_1494395298335_303847 --user-class-path file:$PWD/__app__.jar 1> /hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000004/stdout 2> /hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000004/stderr"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
End of LogType:launch_container.sh

LogType:stderr
Log Upload Time:Tue May 16 11:12:19 +0700 2017
LogLength:34054
Log Contents:
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/hadoop/yarn/local/filecache/65804/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/hdp/2.4.2.0-258/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
17/05/16 11:10:33 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 63241@c411.hadoop.gda.lo
17/05/16 11:10:33 INFO util.SignalUtils: Registered signal handler for TERM
17/05/16 11:10:33 INFO util.SignalUtils: Registered signal handler for HUP
17/05/16 11:10:33 INFO util.SignalUtils: Registered signal handler for INT
17/05/16 11:10:34 INFO spark.SecurityManager: Changing view acls to: yarn,fairy
17/05/16 11:10:34 INFO spark.SecurityManager: Changing modify acls to: yarn,fairy
17/05/16 11:10:34 INFO spark.SecurityManager: Changing view acls groups to: 
17/05/16 11:10:34 INFO spark.SecurityManager: Changing modify acls groups to: 
17/05/16 11:10:34 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(yarn, fairy); groups with view permissions: Set(); users  with modify permissions: Set(yarn, fairy); groups with modify permissions: Set()
17/05/16 11:10:35 INFO client.TransportClientFactory: Successfully created connection to /10.60.43.33:19210 after 279 ms (0 ms spent in bootstraps)
17/05/16 11:10:35 INFO spark.SecurityManager: Changing view acls to: yarn,fairy
17/05/16 11:10:35 INFO spark.SecurityManager: Changing modify acls to: yarn,fairy
17/05/16 11:10:35 INFO spark.SecurityManager: Changing view acls groups to: 
17/05/16 11:10:35 INFO spark.SecurityManager: Changing modify acls groups to: 
17/05/16 11:10:35 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(yarn, fairy); groups with view permissions: Set(); users  with modify permissions: Set(yarn, fairy); groups with modify permissions: Set()
17/05/16 11:10:35 INFO client.TransportClientFactory: Successfully created connection to /10.60.43.33:19210 after 1 ms (0 ms spent in bootstraps)
17/05/16 11:10:35 INFO storage.DiskBlockManager: Created local directory at /hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/blockmgr-0389accd-68c9-4a9e-b0ca-c3628225f958
17/05/16 11:10:35 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MB
17/05/16 11:10:35 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@10.60.43.33:19210
17/05/16 11:10:35 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver
17/05/16 11:10:35 INFO executor.Executor: Starting executor ID 3 on host c411.hadoop.gda.lo
17/05/16 11:10:36 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 27572.
17/05/16 11:10:36 INFO netty.NettyBlockTransferService: Server created on c411.hadoop.gda.lo:27572
17/05/16 11:10:36 INFO storage.BlockManager: external shuffle service port = 7337
17/05/16 11:10:36 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(3, c411.hadoop.gda.lo, 27572)
17/05/16 11:10:36 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(3, c411.hadoop.gda.lo, 27572)
17/05/16 11:10:36 INFO storage.BlockManager: Registering executor with local external shuffle service.
17/05/16 11:10:36 INFO client.TransportClientFactory: Successfully created connection to c411.hadoop.gda.lo/10.60.43.11:7337 after 1 ms (0 ms spent in bootstraps)
17/05/16 11:10:51 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1
17/05/16 11:10:51 INFO executor.Executor: Running task 1.0 in stage 1.0 (TID 1)
17/05/16 11:10:51 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 0
17/05/16 11:10:51 INFO client.TransportClientFactory: Successfully created connection to /10.60.43.33:5141 after 2 ms (0 ms spent in bootstraps)
17/05/16 11:10:51 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.2 KB, free 1458.6 MB)
17/05/16 11:10:51 INFO broadcast.TorrentBroadcast: Reading broadcast variable 0 took 172 ms
17/05/16 11:10:51 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 30.3 KB, free 1458.6 MB)
17/05/16 11:10:51 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
17/05/16 11:10:51 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.60.43.33:19210)
17/05/16 11:10:51 INFO spark.MapOutputTrackerWorker: Got the output locations
17/05/16 11:10:51 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:51 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
17/05/16 11:10:52 INFO codegen.CodeGenerator: Code generated in 315.454959 ms
17/05/16 11:10:52 INFO codegen.CodeGenerator: Code generated in 13.527605 ms
17/05/16 11:10:52 INFO codegen.CodeGenerator: Code generated in 12.142635 ms
17/05/16 11:10:52 INFO codegen.CodeGenerator: Code generated in 10.789392 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 1.0 in stage 1.0 (TID 1). 4219 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 4
17/05/16 11:10:52 INFO executor.Executor: Running task 4.0 in stage 1.0 (TID 4)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 4.0 in stage 1.0 (TID 4). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 6
17/05/16 11:10:52 INFO executor.Executor: Running task 6.0 in stage 1.0 (TID 6)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 6.0 in stage 1.0 (TID 6). 3414 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 9
17/05/16 11:10:52 INFO executor.Executor: Running task 9.0 in stage 1.0 (TID 9)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 9.0 in stage 1.0 (TID 9). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 11
17/05/16 11:10:52 INFO executor.Executor: Running task 11.0 in stage 1.0 (TID 11)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 11.0 in stage 1.0 (TID 11). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 13
17/05/16 11:10:52 INFO executor.Executor: Running task 13.0 in stage 1.0 (TID 13)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 13.0 in stage 1.0 (TID 13). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 17
17/05/16 11:10:52 INFO executor.Executor: Running task 17.0 in stage 1.0 (TID 17)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 17.0 in stage 1.0 (TID 17). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 18
17/05/16 11:10:52 INFO executor.Executor: Running task 18.0 in stage 1.0 (TID 18)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 18.0 in stage 1.0 (TID 18). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 21
17/05/16 11:10:52 INFO executor.Executor: Running task 21.0 in stage 1.0 (TID 21)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 21.0 in stage 1.0 (TID 21). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 23
17/05/16 11:10:52 INFO executor.Executor: Running task 23.0 in stage 1.0 (TID 23)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 23.0 in stage 1.0 (TID 23). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 26
17/05/16 11:10:52 INFO executor.Executor: Running task 26.0 in stage 1.0 (TID 26)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 26.0 in stage 1.0 (TID 26). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 30
17/05/16 11:10:52 INFO executor.Executor: Running task 30.0 in stage 1.0 (TID 30)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 30.0 in stage 1.0 (TID 30). 3414 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 33
17/05/16 11:10:52 INFO executor.Executor: Running task 33.0 in stage 1.0 (TID 33)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 33.0 in stage 1.0 (TID 33). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 35
17/05/16 11:10:52 INFO executor.Executor: Running task 35.0 in stage 1.0 (TID 35)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 35.0 in stage 1.0 (TID 35). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 37
17/05/16 11:10:52 INFO executor.Executor: Running task 37.0 in stage 1.0 (TID 37)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 37.0 in stage 1.0 (TID 37). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 40
17/05/16 11:10:52 INFO executor.Executor: Running task 40.0 in stage 1.0 (TID 40)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 40.0 in stage 1.0 (TID 40). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 43
17/05/16 11:10:52 INFO executor.Executor: Running task 43.0 in stage 1.0 (TID 43)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 43.0 in stage 1.0 (TID 43). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 47
17/05/16 11:10:52 INFO executor.Executor: Running task 47.0 in stage 1.0 (TID 47)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 47.0 in stage 1.0 (TID 47). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 51
17/05/16 11:10:52 INFO executor.Executor: Running task 51.0 in stage 1.0 (TID 51)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 51.0 in stage 1.0 (TID 51). 3414 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 54
17/05/16 11:10:52 INFO executor.Executor: Running task 54.0 in stage 1.0 (TID 54)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 54.0 in stage 1.0 (TID 54). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 57
17/05/16 11:10:52 INFO executor.Executor: Running task 57.0 in stage 1.0 (TID 57)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 57.0 in stage 1.0 (TID 57). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 60
17/05/16 11:10:52 INFO executor.Executor: Running task 60.0 in stage 1.0 (TID 60)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 60.0 in stage 1.0 (TID 60). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 62
17/05/16 11:10:52 INFO executor.Executor: Running task 62.0 in stage 1.0 (TID 62)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 62.0 in stage 1.0 (TID 62). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 65
17/05/16 11:10:52 INFO executor.Executor: Running task 65.0 in stage 1.0 (TID 65)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 65.0 in stage 1.0 (TID 65). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 69
17/05/16 11:10:52 INFO executor.Executor: Running task 69.0 in stage 1.0 (TID 69)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 69.0 in stage 1.0 (TID 69). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 72
17/05/16 11:10:52 INFO executor.Executor: Running task 72.0 in stage 1.0 (TID 72)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 72.0 in stage 1.0 (TID 72). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 76
17/05/16 11:10:52 INFO executor.Executor: Running task 76.0 in stage 1.0 (TID 76)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 76.0 in stage 1.0 (TID 76). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 79
17/05/16 11:10:52 INFO executor.Executor: Running task 79.0 in stage 1.0 (TID 79)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 79.0 in stage 1.0 (TID 79). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 82
17/05/16 11:10:53 INFO executor.Executor: Running task 82.0 in stage 1.0 (TID 82)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 82.0 in stage 1.0 (TID 82). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 86
17/05/16 11:10:53 INFO executor.Executor: Running task 86.0 in stage 1.0 (TID 86)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 86.0 in stage 1.0 (TID 86). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 88
17/05/16 11:10:53 INFO executor.Executor: Running task 88.0 in stage 1.0 (TID 88)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 88.0 in stage 1.0 (TID 88). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 90
17/05/16 11:10:53 INFO executor.Executor: Running task 90.0 in stage 1.0 (TID 90)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 90.0 in stage 1.0 (TID 90). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 94
17/05/16 11:10:53 INFO executor.Executor: Running task 94.0 in stage 1.0 (TID 94)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 94.0 in stage 1.0 (TID 94). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 97
17/05/16 11:10:53 INFO executor.Executor: Running task 97.0 in stage 1.0 (TID 97)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 97.0 in stage 1.0 (TID 97). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 100
17/05/16 11:10:53 INFO executor.Executor: Running task 100.0 in stage 1.0 (TID 100)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 100.0 in stage 1.0 (TID 100). 3414 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 103
17/05/16 11:10:53 INFO executor.Executor: Running task 103.0 in stage 1.0 (TID 103)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 103.0 in stage 1.0 (TID 103). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 105
17/05/16 11:10:53 INFO executor.Executor: Running task 105.0 in stage 1.0 (TID 105)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 105.0 in stage 1.0 (TID 105). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 108
17/05/16 11:10:53 INFO executor.Executor: Running task 108.0 in stage 1.0 (TID 108)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 108.0 in stage 1.0 (TID 108). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 113
17/05/16 11:10:53 INFO executor.Executor: Running task 113.0 in stage 1.0 (TID 113)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 113.0 in stage 1.0 (TID 113). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 117
17/05/16 11:10:53 INFO executor.Executor: Running task 117.0 in stage 1.0 (TID 117)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 117.0 in stage 1.0 (TID 117). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 120
17/05/16 11:10:53 INFO executor.Executor: Running task 120.0 in stage 1.0 (TID 120)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 120.0 in stage 1.0 (TID 120). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 124
17/05/16 11:10:53 INFO executor.Executor: Running task 124.0 in stage 1.0 (TID 124)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 124.0 in stage 1.0 (TID 124). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 129
17/05/16 11:10:53 INFO executor.Executor: Running task 129.0 in stage 1.0 (TID 129)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 129.0 in stage 1.0 (TID 129). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 136
17/05/16 11:10:53 INFO executor.Executor: Running task 136.0 in stage 1.0 (TID 136)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 136.0 in stage 1.0 (TID 136). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 141
17/05/16 11:10:53 INFO executor.Executor: Running task 141.0 in stage 1.0 (TID 141)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 141.0 in stage 1.0 (TID 141). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 144
17/05/16 11:10:53 INFO executor.Executor: Running task 144.0 in stage 1.0 (TID 144)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 144.0 in stage 1.0 (TID 144). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 148
17/05/16 11:10:53 INFO executor.Executor: Running task 148.0 in stage 1.0 (TID 148)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 148.0 in stage 1.0 (TID 148). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 153
17/05/16 11:10:53 INFO executor.Executor: Running task 153.0 in stage 1.0 (TID 153)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 153.0 in stage 1.0 (TID 153). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 157
17/05/16 11:10:53 INFO executor.Executor: Running task 157.0 in stage 1.0 (TID 157)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 157.0 in stage 1.0 (TID 157). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 162
17/05/16 11:10:53 INFO executor.Executor: Running task 162.0 in stage 1.0 (TID 162)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 162.0 in stage 1.0 (TID 162). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 167
17/05/16 11:10:53 INFO executor.Executor: Running task 167.0 in stage 1.0 (TID 167)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 167.0 in stage 1.0 (TID 167). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 170
17/05/16 11:10:53 INFO executor.Executor: Running task 170.0 in stage 1.0 (TID 170)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 170.0 in stage 1.0 (TID 170). 3414 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 175
17/05/16 11:10:53 INFO executor.Executor: Running task 175.0 in stage 1.0 (TID 175)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 175.0 in stage 1.0 (TID 175). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 179
17/05/16 11:10:53 INFO executor.Executor: Running task 179.0 in stage 1.0 (TID 179)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 179.0 in stage 1.0 (TID 179). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 183
17/05/16 11:10:53 INFO executor.Executor: Running task 183.0 in stage 1.0 (TID 183)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 183.0 in stage 1.0 (TID 183). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 186
17/05/16 11:10:53 INFO executor.Executor: Running task 186.0 in stage 1.0 (TID 186)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 186.0 in stage 1.0 (TID 186). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 188
17/05/16 11:10:53 INFO executor.Executor: Running task 188.0 in stage 1.0 (TID 188)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 188.0 in stage 1.0 (TID 188). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 191
17/05/16 11:10:53 INFO executor.Executor: Running task 191.0 in stage 1.0 (TID 191)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 191.0 in stage 1.0 (TID 191). 3414 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 196
17/05/16 11:10:53 INFO executor.Executor: Running task 196.0 in stage 1.0 (TID 196)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 196.0 in stage 1.0 (TID 196). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 199
17/05/16 11:10:53 INFO executor.Executor: Running task 199.0 in stage 1.0 (TID 199)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 199.0 in stage 1.0 (TID 199). 3327 bytes result sent to driver
17/05/16 11:10:54 INFO executor.CoarseGrainedExecutorBackend: Driver commanded a shutdown
17/05/16 11:10:54 INFO memory.MemoryStore: MemoryStore cleared
17/05/16 11:10:54 INFO storage.BlockManager: BlockManager stopped
17/05/16 11:10:54 INFO util.ShutdownHookManager: Shutdown hook called
End of LogType:stderr

LogType:stdout
Log Upload Time:Tue May 16 11:12:19 +0700 2017
LogLength:0
Log Contents:
End of LogType:stdout



Container: container_e81_1494395298335_303847_02_000002 on c412.hadoop.gda.lo_45454
=====================================================================================
LogType:directory.info
Log Upload Time:Tue May 16 11:12:18 +0700 2017
LogLength:28883
Log Contents:
ls -l:
total 112
lrwxrwxrwx 1 yarn hadoop    69 May 16 11:11 __app__.jar -> /hadoop/yarn/local/usercache/fairy/filecache/38115/stats-etlr-1.0.jar
-rw-r--r-- 1 yarn hadoop    87 May 16 11:11 container_tokens
-rwx------ 1 yarn hadoop   676 May 16 11:11 default_container_executor_session.sh
-rwx------ 1 yarn hadoop   730 May 16 11:11 default_container_executor.sh
-rwx------ 1 yarn hadoop 75972 May 16 11:11 launch_container.sh
lrwxrwxrwx 1 yarn hadoop    69 May 16 11:11 __spark_conf__ -> /hadoop/yarn/local/usercache/fairy/filecache/38116/__spark_conf__.zip
drwxr-xr-x 2 yarn hadoop 12288 May 16 11:11 __spark_libs__
drwx--x--- 2 yarn hadoop  4096 May 16 11:11 tmp
find -L . -maxdepth 5 -ls:
10486929    4 drwx--x---   4 yarn     hadoop       4096 May 16 11:11 .
10486934   76 -rwx------   1 yarn     hadoop      75972 May 16 11:11 ./launch_container.sh
10486933    4 -rw-r--r--   1 yarn     hadoop         12 May 16 11:11 ./.container_tokens.crc
10486937    4 -rw-r--r--   1 yarn     hadoop         16 May 16 11:11 ./.default_container_executor_session.sh.crc
11406333 1152 -r-x------   1 yarn     hadoop    1178454 May 16 11:11 ./__app__.jar
10486939    4 -rw-r--r--   1 yarn     hadoop         16 May 16 11:11 ./.default_container_executor.sh.crc
10486932    4 -rw-r--r--   1 yarn     hadoop         87 May 16 11:11 ./container_tokens
10486935    4 -rw-r--r--   1 yarn     hadoop        604 May 16 11:11 ./.launch_container.sh.crc
10486930    4 drwx--x---   2 yarn     hadoop       4096 May 16 11:11 ./tmp
11406338    4 drwx------   2 yarn     hadoop       4096 May 16 11:11 ./__spark_conf__
11406346    8 -r-x------   1 yarn     hadoop       4197 May 16 11:11 ./__spark_conf__/core-site.xml
11406361    4 -r-x------   1 yarn     hadoop        690 May 16 11:11 ./__spark_conf__/mapred-env.sh
11406358    4 -r-x------   1 yarn     hadoop       1020 May 16 11:11 ./__spark_conf__/commons-logging.properties
11406364    4 -r-x------   1 yarn     hadoop       1527 May 16 11:11 ./__spark_conf__/kms-env.sh
11406359    4 -r-x------   1 yarn     hadoop       2490 May 16 11:11 ./__spark_conf__/hadoop-metrics.properties
11406348    4 -r-x------   1 yarn     hadoop       2250 May 16 11:11 ./__spark_conf__/yarn-env.cmd
11406370    4 -r-x------   1 yarn     hadoop       2268 May 16 11:11 ./__spark_conf__/ssl-server.xml.example
11406360    8 -r-x------   1 yarn     hadoop       4221 May 16 11:11 ./__spark_conf__/task-log4j.properties
11406362    4 -r-x------   1 yarn     hadoop       1602 May 16 11:11 ./__spark_conf__/health_check
11406356    8 -r-x------   1 yarn     hadoop       6572 May 16 11:11 ./__spark_conf__/hdfs-site.xml
11406343   20 -r-x------   1 yarn     hadoop      16530 May 16 11:11 ./__spark_conf__/yarn-site.xml
11406353    4 -r-x------   1 yarn     hadoop       2358 May 16 11:11 ./__spark_conf__/topology_script.py
11406363    4 -r-x------   1 yarn     hadoop       2316 May 16 11:11 ./__spark_conf__/ssl-client.xml.example
11406345    0 -r-x------   1 yarn     hadoop          0 May 16 11:11 ./__spark_conf__/yarn.exclude
11406357    4 -r-x------   1 yarn     hadoop       1019 May 16 11:11 ./__spark_conf__/container-executor.cfg
11406351    4 -r-x------   1 yarn     hadoop       3518 May 16 11:11 ./__spark_conf__/kms-acls.xml
11406342    4 -r-x------   1 yarn     hadoop       2302 May 16 11:11 ./__spark_conf__/hadoop-metrics2.properties
11406347    4 -r-x------   1 yarn     hadoop       1631 May 16 11:11 ./__spark_conf__/kms-log4j.properties
11406340    8 -r-x------   1 yarn     hadoop       5467 May 16 11:11 ./__spark_conf__/hadoop-env.sh
11406374   36 -r-x------   1 yarn     hadoop      34061 May 16 11:11 ./__spark_conf__/__spark_conf__.properties
11406371    8 -r-x------   1 yarn     hadoop       4113 May 16 11:11 ./__spark_conf__/mapred-queues.xml.template
11406368    4 -r-x------   1 yarn     hadoop        951 May 16 11:11 ./__spark_conf__/mapred-env.cmd
11406339    8 -r-x------   1 yarn     hadoop       6836 May 16 11:11 ./__spark_conf__/mapred-site.xml
11406373    4 -r-x------   1 yarn     hadoop        945 May 16 11:11 ./__spark_conf__/taskcontroller.cfg
11406354    4 -r-x------   1 yarn     hadoop       1335 May 16 11:11 ./__spark_conf__/configuration.xsl
11406355    8 -r-x------   1 yarn     hadoop       5006 May 16 11:11 ./__spark_conf__/yarn-env.sh
11406366    4 -r-x------   1 yarn     hadoop        229 May 16 11:11 ./__spark_conf__/slaves
11406349    4 -r-x------   1 yarn     hadoop        884 May 16 11:11 ./__spark_conf__/ssl-client.xml
11406367    4 -r-x------   1 yarn     hadoop        724 May 16 11:11 ./__spark_conf__/topology_mappings.data
11406365    4 -r-x------   1 yarn     hadoop       1308 May 16 11:11 ./__spark_conf__/hadoop-policy.xml
11406369    4 -r-x------   1 yarn     hadoop       1000 May 16 11:11 ./__spark_conf__/ssl-server.xml
11406344    4 -r-x------   1 yarn     hadoop       3979 May 16 11:11 ./__spark_conf__/hadoop-env.cmd
11406341   12 -r-x------   1 yarn     hadoop       8699 May 16 11:11 ./__spark_conf__/log4j.properties
11406350    8 -r-x------   1 yarn     hadoop       5404 May 16 11:11 ./__spark_conf__/capacity-scheduler.xml
11406372    8 -r-x------   1 yarn     hadoop       5511 May 16 11:11 ./__spark_conf__/kms-site.xml
11406352    4 -r-x------   1 yarn     hadoop        758 May 16 11:11 ./__spark_conf__/mapred-site.xml.template
10486940   12 drwxr-xr-x   2 yarn     hadoop      12288 May 16 11:11 ./__spark_libs__
24907650  468 -rwxr-xr-x   1 yarn     hadoop     477970 Apr  4 16:10 ./__spark_libs__/lift-json_2.11-2.6.3.jar
24907594  736 -rwxr-xr-x   1 yarn     hadoop     753012 Apr  4 16:10 ./__spark_libs__/hadoop-mapreduce-client-common-2.7.1.jar
24907213  180 -rwxr-xr-x   1 yarn     hadoop     181271 Apr  4 16:10 ./__spark_libs__/hk2-locator-2.4.0-b34.jar
24907469    8 -rwxr-xr-x   1 yarn     hadoop       4596 Apr  4 16:10 ./__spark_libs__/jul-to-slf4j-1.7.16.jar
24904669  764 -rwxr-xr-x   1 yarn     hadoop     780664 Apr  4 16:10 ./__spark_libs__/jackson-mapper-asl-1.9.13.jar
24904318   20 -rwxr-xr-x   1 yarn     hadoop      18098 Apr  4 16:10 ./__spark_libs__/jersey-container-servlet-2.22.2.jar
24641847  188 -rwxr-xr-x   1 yarn     hadoop     188671 Apr  4 16:10 ./__spark_libs__/commons-beanutils-1.7.0.jar
24907688  164 -rwxr-xr-x   1 yarn     hadoop     167421 Apr  4 16:10 ./__spark_libs__/jersey-client-2.22.2.jar
24641836  428 -rwxr-xr-x   1 yarn     hadoop     436303 Apr  4 16:10 ./__spark_libs__/avro-1.7.7.jar
24907211  528 -rwxr-xr-x   1 yarn     hadoop     539912 Apr  4 16:10 ./__spark_libs__/jetty-6.1.26.jar
24641859  144 -rwxr-xr-x   1 yarn     hadoop     144660 Apr  4 16:10 ./__spark_libs__/xbean-asm5-shaded-4.4.jar
24907599   84 -rwxr-xr-x   1 yarn     hadoop      82421 Apr  4 16:10 ./__spark_libs__/json4s-ast_2.11-3.2.11.jar
24641842   40 -rwxr-xr-x   1 yarn     hadoop      40341 Apr  4 16:10 ./__spark_libs__/json4s-jackson_2.11-3.2.11.jar
24906430 1592 -rwxr-xr-x   1 yarn     hadoop    1627065 Apr  4 16:10 ./__spark_libs__/jersey-bundle-1.19.1.jar
24641821 15128 -rwxr-xr-x   1 yarn     hadoop   15487351 Apr  4 16:10 ./__spark_libs__/scala-compiler-2.11.8.jar
24905240   32 -rwxr-xr-x   1 yarn     hadoop      32145 Apr  4 16:10 ./__spark_libs__/hadoop-yarn-server-web-proxy-2.7.1.jar
24907532   16 -rwxr-xr-x   1 yarn     hadoop      15071 Apr  4 16:10 ./__spark_libs__/jta-1.1.jar
24904194  144 -rwxr-xr-x   1 yarn     hadoop     143602 Apr  4 16:10 ./__spark_libs__/commons-digester-1.8.jar
24641903  532 -rwxr-xr-x   1 yarn     hadoop     540852 Apr  4 16:10 ./__spark_libs__/mysql-connector-java-5.0.8-bin.jar
24642022  176 -rwxr-xr-x   1 yarn     hadoop     178947 Apr  4 16:10 ./__spark_libs__/hk2-api-2.4.0-b34.jar
24907387  220 -rwxr-xr-x   1 yarn     hadoop     223573 Apr  4 16:10 ./__spark_libs__/chill_2.11-0.8.0.jar
24907334   48 -rwxr-xr-x   1 yarn     hadoop      48720 Apr  4 16:10 ./__spark_libs__/snappy-0.2.jar
24907226  720 -rwxr-xr-x   1 yarn     hadoop     736658 Apr  4 16:10 ./__spark_libs__/httpclient-4.5.2.jar
24907515 3152 -rwxr-xr-x   1 yarn     hadoop    3224708 Apr  4 16:10 ./__spark_libs__/derby-10.12.1.1.jar
24641785   56 -rwxr-xr-x   1 yarn     hadoop      55511 Apr  4 16:10 ./__spark_libs__/spark-network-shuffle_2.11-2.0.1.jar
24907360   12 -rwxr-xr-x   1 yarn     hadoop      12131 Apr  4 16:10 ./__spark_libs__/jpam-1.1.jar
24641863 1988 -rwxr-xr-x   1 yarn     hadoop    2035066 Apr  4 16:10 ./__spark_libs__/commons-math3-3.4.1.jar
24641769   44 -rwxr-xr-x   1 yarn     hadoop      44925 Apr  4 16:10 ./__spark_libs__/apacheds-i18n-2.0.0-M15.jar
24907620  440 -rwxr-xr-x   1 yarn     hadoop     448794 Apr  4 16:10 ./__spark_libs__/apache-log4j-extras-1.2.17.jar
24907453  232 -rwxr-xr-x   1 yarn     hadoop     236880 Apr  4 16:10 ./__spark_libs__/lz4-1.3.0.jar
24641818   16 -rwxr-xr-x   1 yarn     hadoop      15305 Apr  4 16:10 ./__spark_libs__/spark-tags_2.11-2.0.1.jar
24907238 6716 -rwxr-xr-x   1 yarn     hadoop    6873892 Apr  4 16:10 ./__spark_libs__/spark-catalyst_2.11-2.0.1.jar
24907244   32 -rwxr-xr-x   1 yarn     hadoop      29540 Apr  4 16:10 ./__spark_libs__/spark-sketch_2.11-2.0.1.jar
24907583   96 -rwxr-xr-x   1 yarn     hadoop      94672 Apr  4 16:10 ./__spark_libs__/xz-1.0.jar
24907457 5712 -rwxr-xr-x   1 yarn     hadoop    5847591 Apr  4 16:10 ./__spark_libs__/spark-mllib_2.11-2.0.1.jar
24907524   36 -rwxr-xr-x   1 yarn     hadoop      33015 Apr  4 16:10 ./__spark_libs__/jsr305-1.3.9.jar
24641906   20 -rwxr-xr-x   1 yarn     hadoop      16993 Apr  4 16:10 ./__spark_libs__/JavaEWAH-0.3.2.jar
24641794   48 -rwxr-xr-x   1 yarn     hadoop      45944 Apr  4 16:10 ./__spark_libs__/json-20090211.jar
24641771   16 -rwxr-xr-x   1 yarn     hadoop      15827 Apr  4 16:10 ./__spark_libs__/metrics-json-3.1.2.jar
24907536  352 -rwxr-xr-x   1 yarn     hadoop     358390 Apr  4 16:10 ./__spark_libs__/kryo-shaded-3.0.3.jar
24641857   68 -rwxr-xr-x   1 yarn     hadoop      66270 Apr  4 16:10 ./__spark_libs__/jersey-container-servlet-core-2.22.2.jar
24906427   52 -rwxr-xr-x   1 yarn     hadoop      50619 Apr  4 16:10 ./__spark_libs__/chill-java-0.8.0.jar
24641812   44 -rwxr-xr-x   1 yarn     hadoop      41755 Apr  4 16:10 ./__spark_libs__/objenesis-2.1.jar
24907587   28 -rwxr-xr-x   1 yarn     hadoop      27084 Apr  4 16:10 ./__spark_libs__/jackson-xc-1.9.13.jar
24907540   20 -rwxr-xr-x   1 yarn     hadoop      18482 Apr  4 16:10 ./__spark_libs__/eigenbase-properties-1.1.5.jar
24907371   72 -rwxr-xr-x   1 yarn     hadoop      70688 Apr  4 16:10 ./__spark_libs__/hadoop-auth-2.7.1.jar
24904211   20 -rwxr-xr-x   1 yarn     hadoop      19827 Apr  4 16:10 ./__spark_libs__/opencsv-2.3.jar
24906426  112 -rwxr-xr-x   1 yarn     hadoop     110600 Apr  4 16:10 ./__spark_libs__/bonecp-0.8.0.RELEASE.jar
24907595  668 -rwxr-xr-x   1 yarn     hadoop     680106 Apr  4 16:10 ./__spark_libs__/spark-graphx_2.11-2.0.1.jar
24907321  280 -rwxr-xr-x   1 yarn     hadoop     285447 Apr  4 16:10 ./__spark_libs__/parquet-encoding-1.7.0.jar
24907338  952 -rwxr-xr-x   1 yarn     hadoop     971310 Apr  4 16:10 ./__spark_libs__/jersey-guava-2.22.2.jar
24907333   20 -rwxr-xr-x   1 yarn     hadoop      16560 Apr  4 16:10 ./__spark_libs__/api-asn1-api-1.0.0-M20.jar
24907208   20 -rwxr-xr-x   1 yarn     hadoop      17008 Apr  4 16:10 ./__spark_libs__/base64-2.3.8.jar
24907551 1032 -rwxr-xr-x   1 yarn     hadoop    1056168 Apr  4 16:10 ./__spark_libs__/snappy-java-1.1.2.6.jar
24907233 4468 -rwxr-xr-x   1 yarn     hadoop    4573750 Apr  4 16:10 ./__spark_libs__/scala-reflect-2.11.8.jar
24642016  160 -rwxr-xr-x   1 yarn     hadoop     160519 Apr  4 16:10 ./__spark_libs__/commons-dbcp-1.4.jar
24904384   40 -rwxr-xr-x   1 yarn     hadoop      40509 Apr  4 16:10 ./__spark_libs__/slf4j-api-1.7.16.jar
24906420 1256 -rwxr-xr-x   1 yarn     hadoop    1282424 Apr  4 16:10 ./__spark_libs__/ivy-2.4.0.jar
24907248  436 -rwxr-xr-x   1 yarn     hadoop     445288 Apr  4 16:10 ./__spark_libs__/antlr-2.7.7.jar
24907476    8 -rwxr-xr-x   1 yarn     hadoop       5711 Apr  4 16:10 ./__spark_libs__/minlog-1.3.0.jar
24641839   20 -rwxr-xr-x   1 yarn     hadoop      16430 Apr  4 16:10 ./__spark_libs__/jcl-over-slf4j-1.7.16.jar
24907623 2140 -rwxr-xr-x   1 yarn     hadoop    2189117 Apr  4 16:10 ./__spark_libs__/guava-14.0.1.jar
24904183  228 -rwxr-xr-x   1 yarn     hadoop     232248 Apr  4 16:10 ./__spark_libs__/jackson-core-asl-1.9.13.jar
24907296  280 -rwxr-xr-x   1 yarn     hadoop     284220 Apr  4 16:10 ./__spark_libs__/commons-lang-2.6.jar
24907611  504 -rwxr-xr-x   1 yarn     hadoop     515604 Apr  4 16:10 ./__spark_libs__/jackson-module-scala_2.11-2.6.5.jar
24907269 1248 -rwxr-xr-x   1 yarn     hadoop    1277883 Apr  4 16:10 ./__spark_libs__/mesos-0.21.1-shaded-protobuf.jar
24907322  416 -rwxr-xr-x   1 yarn     hadoop     423753 Apr  4 16:10 ./__spark_libs__/scala-parser-combinators_2.11-1.0.4.jar
24907287  332 -rwxr-xr-x   1 yarn     hadoop     339666 Apr  4 16:10 ./__spark_libs__/datanucleus-api-jdo-3.2.6.jar
24907512 2304 -rwxr-xr-x   1 yarn     hadoop    2355465 Apr  4 16:10 ./__spark_libs__/spark-network-common_2.11-2.0.1.jar
24907662 1772 -rwxr-xr-x   1 yarn     hadoop    1814309 Apr  4 16:10 ./__spark_libs__/spark-hive-thriftserver_2.11-2.0.1.jar
24907613 1168 -rwxr-xr-x   1 yarn     hadoop    1194003 Apr  4 16:10 ./__spark_libs__/arpack_combined_all-0.1.jar
24907202 11232 -rwxr-xr-x   1 yarn     hadoop   11498852 Apr  4 16:10 ./__spark_libs__/hive-exec-1.2.1.spark2.jar
24907503 6144 -rwxr-xr-x   1 yarn     hadoop    6290315 Apr  4 16:10 ./__spark_libs__/spark-sql_2.11-2.0.1.jar
24907341   24 -rwxr-xr-x   1 yarn     hadoop      23346 Apr  4 16:10 ./__spark_libs__/stax-api-1.0-2.jar
24907538  696 -rwxr-xr-x   1 yarn     hadoop     710492 Apr  4 16:10 ./__spark_libs__/guice-3.0.jar
24641790   48 -rwxr-xr-x   1 yarn     hadoop      46983 Apr  4 16:10 ./__spark_libs__/jackson-annotations-2.6.5.jar
24904339  136 -rwxr-xr-x   1 yarn     hadoop     135552 Apr  4 16:10 ./__spark_libs__/breeze-macros_2.11-0.11.2.jar
24641996  256 -rwxr-xr-x   1 yarn     hadoop     258876 Apr  4 16:10 ./__spark_libs__/jackson-core-2.6.5.jar
24907204    8 -rwxr-xr-x   1 yarn     hadoop       4467 Apr  4 16:10 ./__spark_libs__/aopalliance-1.0.jar
24907563   24 -rwxr-xr-x   1 yarn     hadoop      20852 Apr  4 16:10 ./__spark_libs__/metrics-graphite-3.1.2.jar
24907643  388 -rwxr-xr-x   1 yarn     hadoop     395195 Apr  4 16:10 ./__spark_libs__/javolution-5.5.1.jar
24907615   68 -rwxr-xr-x   1 yarn     hadoop      69409 Apr  4 16:10 ./__spark_libs__/activation-1.1.1.jar
24641802  636 -rwxr-xr-x   1 yarn     hadoop     648678 Apr  4 16:10 ./__spark_libs__/scala-xml_2.11-1.0.2.jar
24641829 2044 -rwxr-xr-x   1 yarn     hadoop    2090370 Apr  4 16:10 ./__spark_libs__/spark-streaming_2.11-2.0.1.jar
24907577  896 -rwxr-xr-x   1 yarn     hadoop     917052 Apr  4 16:10 ./__spark_libs__/parquet-column-1.7.0.jar
24641764 3440 -rwxr-xr-x   1 yarn     hadoop    3519262 Apr  4 16:10 ./__spark_libs__/calcite-core-1.2.0-incubating.jar
24907375  404 -rwxr-xr-x   1 yarn     hadoop     412739 Apr  4 16:10 ./__spark_libs__/commons-lang3-3.3.2.jar
24642003  932 -rwxr-xr-x   1 yarn     hadoop     951701 Apr  4 16:10 ./__spark_libs__/jersey-server-2.22.2.jar
24907665   28 -rwxr-xr-x   1 yarn     hadoop      26514 Apr  4 16:10 ./__spark_libs__/stax-api-1.0.1.jar
24907383  576 -rwxr-xr-x   1 yarn     hadoop     588337 Apr  4 16:10 ./__spark_libs__/commons-collections-3.2.2.jar
24907196  232 -rwxr-xr-x   1 yarn     hadoop     236660 Apr  4 16:10 ./__spark_libs__/ST4-4.0.4.jar
24904321  600 -rwxr-xr-x   1 yarn     hadoop     613299 Apr  4 16:10 ./__spark_libs__/janino-2.7.8.jar
24642013 1996 -rwxr-xr-x   1 yarn     hadoop    2041628 Apr  4 16:10 ./__spark_libs__/jets3t-0.9.3.jar
24907317   44 -rwxr-xr-x   1 yarn     hadoop      41070 Apr  4 16:10 ./__spark_libs__/spark-unsafe_2.11-2.0.1.jar
24907658  400 -rwxr-xr-x   1 yarn     hadoop     409467 Apr  4 16:10 ./__spark_libs__/mx4j-3.0.2.jar
24641823   64 -rwxr-xr-x   1 yarn     hadoop      63777 Apr  4 16:10 ./__spark_libs__/validation-api-1.1.0.Final.jar
24641897 2732 -rwxr-xr-x   1 yarn     hadoop    2796935 Apr  4 16:10 ./__spark_libs__/parquet-hadoop-bundle-1.6.0.jar
24907698    8 -rwxr-xr-x   1 yarn     hadoop       5310 Apr  4 16:10 ./__spark_libs__/pmml-schema-1.2.15.jar
24907261 2780 -rwxr-xr-x   1 yarn     hadoop    2842667 Apr  4 16:10 ./__spark_libs__/bcprov-jdk15on-1.51.jar
24907542   64 -rwxr-xr-x   1 yarn     hadoop      65261 Apr  4 16:10 ./__spark_libs__/oro-2.0.8.jar
24642009   44 -rwxr-xr-x   1 yarn     hadoop      41263 Apr  4 16:10 ./__spark_libs__/jackson-module-paranamer-2.6.5.jar
24907568  184 -rwxr-xr-x   1 yarn     hadoop     185245 Apr  4 16:10 ./__spark_libs__/curator-framework-2.6.0.jar
24907647  200 -rwxr-xr-x   1 yarn     hadoop     201928 Apr  4 16:10 ./__spark_libs__/RoaringBitmap-0.5.11.jar
24641742  224 -rwxr-xr-x   1 yarn     hadoop     227712 Apr  4 16:10 ./__spark_libs__/libthrift-0.9.2.jar
24907633  420 -rwxr-xr-x   1 yarn     hadoop     427780 Apr  4 16:10 ./__spark_libs__/jodd-core-3.5.2.jar
24907573  512 -rwxr-xr-x   1 yarn     hadoop     521157 Apr  4 16:10 ./__spark_libs__/mail-1.4.7.jar
24907361  380 -rwxr-xr-x   1 yarn     hadoop     387188 Apr  4 16:10 ./__spark_libs__/parquet-format-2.3.0-incubating.jar
24907605  212 -rwxr-xr-x   1 yarn     hadoop     213911 Apr  4 16:10 ./__spark_libs__/jline-2.12.1.jar
24907674  172 -rwxr-xr-x   1 yarn     hadoop     174351 Apr  4 16:10 ./__spark_libs__/stream-2.7.0.jar
24907653  120 -rwxr-xr-x   1 yarn     hadoop     118973 Apr  4 16:10 ./__spark_libs__/hk2-utils-2.4.0-b34.jar
24907506   80 -rwxr-xr-x   1 yarn     hadoop      79912 Apr  4 16:10 ./__spark_libs__/api-util-1.0.0-M20.jar
24904363 1848 -rwxr-xr-x   1 yarn     hadoop    1890075 Apr  4 16:10 ./__spark_libs__/datanucleus-core-3.2.10.jar
24907565   20 -rwxr-xr-x   1 yarn     hadoop      18336 Apr  4 16:10 ./__spark_libs__/jackson-jaxrs-1.9.13.jar
24907304  100 -rwxr-xr-x   1 yarn     hadoop     100636 Apr  4 16:10 ./__spark_libs__/jsp-api-2.1.jar
24907368  100 -rwxr-xr-x   1 yarn     hadoop     100680 Apr  4 16:10 ./__spark_libs__/hive-jdbc-1.2.1.spark2.jar
24907627 1144 -rwxr-xr-x   1 yarn     hadoop    1171380 Apr  4 16:10 ./__spark_libs__/jackson-databind-2.6.5.jar
24642006  244 -rwxr-xr-x   1 yarn     hadoop     248171 Apr  4 16:10 ./__spark_libs__/curator-recipes-2.6.0.jar
24907225 5612 -rwxr-xr-x   1 yarn     hadoop    5744974 Apr  4 16:10 ./__spark_libs__/scala-library-2.11.8.jar
24641756  776 -rwxr-xr-x   1 yarn     hadoop     792964 Apr  4 16:10 ./__spark_libs__/zookeeper-3.4.6.jar
24907527   16 -rwxr-xr-x   1 yarn     hadoop      15010 Apr  4 16:10 ./__spark_libs__/xmlenc-0.52.jar
24907354  292 -rwxr-xr-x   1 yarn     hadoop     298829 Apr  4 16:10 ./__spark_libs__/commons-configuration-1.6.jar
24641801   96 -rwxr-xr-x   1 yarn     hadoop      95806 Apr  4 16:10 ./__spark_libs__/javax.servlet-api-3.1.0.jar
24641866   40 -rwxr-xr-x   1 yarn     hadoop      39280 Apr  4 16:10 ./__spark_libs__/metrics-jvm-3.1.2.jar
24641763   64 -rwxr-xr-x   1 yarn     hadoop      65012 Apr  4 16:10 ./__spark_libs__/guice-servlet-3.0.jar
24907200 1024 -rwxr-xr-x   1 yarn     hadoop    1045744 Apr  4 16:10 ./__spark_libs__/leveldbjni-all-1.8.jar
24641745  204 -rwxr-xr-x   1 yarn     hadoop     206035 Apr  4 16:10 ./__spark_libs__/commons-beanutils-core-1.8.0.jar
24641789   40 -rwxr-xr-x   1 yarn     hadoop      40817 Apr  4 16:10 ./__spark_libs__/hive-cli-1.2.1.spark2.jar
24907192   80 -rwxr-xr-x   1 yarn     hadoop      79845 Apr  4 16:10 ./__spark_libs__/compress-lzf-1.0.3.jar
24904802 13136 -rwxr-xr-x   1 yarn     hadoop   13448966 Apr  4 16:10 ./__spark_libs__/breeze_2.11-0.11.2.jar
24641797    4 -rwxr-xr-x   1 yarn     hadoop       2545 Apr  4 16:10 ./__spark_libs__/hadoop-client-2.7.1.jar
24907497  208 -rwxr-xr-x   1 yarn     hadoop     212453 Apr  4 16:10 ./__spark_libs__/commons-net-2.2.jar
24907465  200 -rwxr-xr-x   1 yarn     hadoop     201124 Apr  4 16:10 ./__spark_libs__/jdo-api-3.0.1.jar
24641851  576 -rwxr-xr-x   1 yarn     hadoop     589462 Apr  4 16:10 ./__spark_libs__/json4s-core_2.11-3.2.11.jar
24641900   32 -rwxr-xr-x   1 yarn     hadoop      29555 Apr  4 16:10 ./__spark_libs__/paranamer-2.3.jar
24905235   68 -rwxr-xr-x   1 yarn     hadoop      65653 Apr  4 16:10 ./__spark_libs__/spark-launcher_2.11-2.0.1.jar
24907315  684 -rwxr-xr-x   1 yarn     hadoop     698375 Apr  4 16:10 ./__spark_libs__/jersey-common-2.22.2.jar
24907687 2008 -rwxr-xr-x   1 yarn     hadoop    2054931 Apr  4 16:10 ./__spark_libs__/netty-all-4.0.29.Final.jar
24905245  148 -rwxr-xr-x   1 yarn     hadoop     148627 Apr  4 16:10 ./__spark_libs__/stringtemplate-3.2.1.jar
24907311 5380 -rwxr-xr-x   1 yarn     hadoop    5505200 Apr  4 16:10 ./__spark_libs__/hive-metastore-1.2.1.spark2.jar
24907509  136 -rwxr-xr-x   1 yarn     hadoop     138464 Apr  4 16:10 ./__spark_libs__/hive-beeline-1.2.1.spark2.jar
24904393  656 -rwxr-xr-x   1 yarn     hadoop     669589 Apr  4 16:10 ./__spark_libs__/spark-yarn_2.11-2.0.1.jar
24907692  508 -rwxr-xr-x   1 yarn     hadoop     516127 Apr  4 16:10 ./__spark_libs__/hadoop-mapreduce-client-app-2.7.1.jar
24907301  320 -rwxr-xr-x   1 yarn     hadoop     326724 Apr  4 16:10 ./__spark_libs__/httpcore-4.4.4.jar
24641827 8068 -rwxr-xr-x   1 yarn     hadoop    8260573 Apr  4 16:10 ./__spark_libs__/hadoop-hdfs-2.7.1.jar
24641816   96 -rwxr-xr-x   1 yarn     hadoop      96221 Apr  4 16:10 ./__spark_libs__/commons-pool-1.5.4.jar
24907667   44 -rwxr-xr-x   1 yarn     hadoop      41123 Apr  4 16:10 ./__spark_libs__/commons-cli-1.2.jar
24907545  700 -rwxr-xr-x   1 yarn     hadoop     714194 Apr  4 16:10 ./__spark_libs__/javassist-3.18.1-GA.jar
24907223  236 -rwxr-xr-x   1 yarn     hadoop     241367 Apr  4 16:10 ./__spark_libs__/commons-compress-1.4.1.jar
24641757   92 -rwxr-xr-x   1 yarn     hadoop      93210 Apr  4 16:10 ./__spark_libs__/super-csv-2.2.0.jar
24907530   32 -rwxr-xr-x   1 yarn     hadoop      30595 Apr  4 16:10 ./__spark_libs__/commons-compiler-2.7.6.jar
24907592   44 -rwxr-xr-x   1 yarn     hadoop      45015 Apr  4 16:10 ./__spark_libs__/hadoop-mapreduce-client-shuffle-2.7.1.jar
24904375  280 -rwxr-xr-x   1 yarn     hadoop     284184 Apr  4 16:10 ./__spark_libs__/commons-codec-1.10.jar
24907253  164 -rwxr-xr-x   1 yarn     hadoop     164368 Apr  4 16:10 ./__spark_libs__/antlr-runtime-3.4.jar
24904175  788 -rwxr-xr-x   1 yarn     hadoop     802818 Apr  4 16:10 ./__spark_libs__/scalap-2.11.8.jar
24641749  180 -rwxr-xr-x   1 yarn     hadoop     180736 Apr  4 16:10 ./__spark_libs__/avro-mapred-1.7.7-hadoop2.jar
24907285   16 -rwxr-xr-x   1 yarn     hadoop      14766 Apr  4 16:10 ./__spark_libs__/aopalliance-repackaged-2.4.0-b34.jar
24907278   20 -rwxr-xr-x   1 yarn     hadoop      17385 Apr  4 16:10 ./__spark_libs__/hadoop-annotations-2.7.1.jar
24907259  104 -rwxr-xr-x   1 yarn     hadoop     105134 Apr  4 16:10 ./__spark_libs__/jaxb-api-2.2.2.jar
24907394  676 -rwxr-xr-x   1 yarn     hadoop     691479 Apr  4 16:10 ./__spark_libs__/apacheds-kerberos-codec-2.0.0-M15.jar
24907683 7108 -rwxr-xr-x   1 yarn     hadoop    7276083 Apr  4 16:10 ./__spark_libs__/spire_2.11-0.7.4.jar
24642020  284 -rwxr-xr-x   1 yarn     hadoop     290506 Apr  4 16:10 ./__spark_libs__/univocity-parsers-2.1.1.jar
24907326  748 -rwxr-xr-x   1 yarn     hadoop     764569 Apr  4 16:10 ./__spark_libs__/jtransforms-2.4.0.jar
24907488   28 -rwxr-xr-x   1 yarn     hadoop      26366 Apr  4 16:10 ./__spark_libs__/javax.annotation-api-1.2.jar
24907266   88 -rwxr-xr-x   1 yarn     hadoop      86811 Apr  4 16:10 ./__spark_libs__/spire-macros_2.11-0.7.4.jar
24907617   40 -rwxr-xr-x   1 yarn     hadoop      38134 Apr  4 16:10 ./__spark_libs__/hadoop-mapreduce-client-jobclient-2.7.1.jar
24907680  308 -rwxr-xr-x   1 yarn     hadoop     313686 Apr  4 16:10 ./__spark_libs__/libfb303-0.9.2.jar
24906421   12 -rwxr-xr-x   1 yarn     hadoop       9939 Apr  4 16:10 ./__spark_libs__/slf4j-log4j12-1.7.16.jar
24907556   20 -rwxr-xr-x   1 yarn     hadoop      20235 Apr  4 16:10 ./__spark_libs__/osgi-resource-locator-1.0.1.jar
24907273 1204 -rwxr-xr-x   1 yarn     hadoop    1230201 Apr  4 16:10 ./__spark_libs__/netty-3.8.0.Final.jar
24904348   92 -rwxr-xr-x   1 yarn     hadoop      93407 Apr  4 16:10 ./__spark_libs__/pyrolite-4.9.jar
24907534   12 -rwxr-xr-x   1 yarn     hadoop      10023 Apr  4 16:10 ./__spark_libs__/java-xmlbuilder-1.0.jar
24641999   64 -rwxr-xr-x   1 yarn     hadoop      63316 Apr  4 16:10 ./__spark_libs__/spark-repl_2.11-2.0.1.jar
24907256  184 -rwxr-xr-x   1 yarn     hadoop     185140 Apr  4 16:10 ./__spark_libs__/commons-io-2.4.jar
24907217  192 -rwxr-xr-x   1 yarn     hadoop     192993 Apr  4 16:10 ./__spark_libs__/avro-ipc-1.7.7.jar
24907348 1616 -rwxr-xr-x   1 yarn     hadoop    1654097 Apr  4 16:10 ./__spark_libs__/hadoop-yarn-common-2.7.1.jar
24641981  188 -rwxr-xr-x   1 yarn     hadoop     190432 Apr  4 16:10 ./__spark_libs__/gson-2.2.4.jar
24907676  296 -rwxr-xr-x   1 yarn     hadoop     302248 Apr  4 16:10 ./__spark_libs__/antlr4-runtime-4.5.3.jar
24907378   24 -rwxr-xr-x   1 yarn     hadoop      21575 Apr  4 16:10 ./__spark_libs__/parquet-common-1.7.0.jar
24907357  480 -rwxr-xr-x   1 yarn     hadoop     489884 Apr  4 16:10 ./__spark_libs__/log4j-1.2.17.jar
24907232  256 -rwxr-xr-x   1 yarn     hadoop     258370 Apr  4 16:10 ./__spark_libs__/calcite-avatica-1.2.0-incubating.jar
24907608 3352 -rwxr-xr-x   1 yarn     hadoop    3431544 Apr  4 16:10 ./__spark_libs__/hadoop-common-2.7.1.jar
24907228  436 -rwxr-xr-x   1 yarn     hadoop     442406 Apr  4 16:10 ./__spark_libs__/calcite-linq4j-1.2.0-incubating.jar
24907572   64 -rwxr-xr-x   1 yarn     hadoop      62050 Apr  4 16:10 ./__spark_libs__/commons-logging-1.1.3.jar
24906780    4 -rwxr-xr-x   1 yarn     hadoop       2497 Apr  4 16:10 ./__spark_libs__/javax.inject-1.jar
24641810 1444 -rwxr-xr-x   1 yarn     hadoop    1475955 Apr  4 16:10 ./__spark_libs__/htrace-core-3.1.0-incubating.jar
24641672  112 -rwxr-xr-x   1 yarn     hadoop     112558 Apr  4 16:10 ./__spark_libs__/metrics-core-3.1.2.jar
24907185  208 -rwxr-xr-x   1 yarn     hadoop     209622 Apr  4 16:10 ./__spark_libs__/parquet-hadoop-1.7.0.jar
24907494 1204 -rwxr-xr-x   1 yarn     hadoop    1229125 Apr  4 16:10 ./__spark_libs__/xercesImpl-2.9.1.jar
24907657 1024 -rwxr-xr-x   1 yarn     hadoop    1044511 Apr  4 16:10 ./__spark_libs__/spark-hive_2.11-2.0.1.jar
24907580 1024 -rwxr-xr-x   1 yarn     hadoop    1048110 Apr  4 16:10 ./__spark_libs__/parquet-jackson-1.7.0.jar
24907521  616 -rwxr-xr-x   1 yarn     hadoop     627814 Apr  4 16:10 ./__spark_libs__/joda-time-2.9.3.jar
24641738 1512 -rwxr-xr-x   1 yarn     hadoop    1544875 Apr  4 16:10 ./__spark_libs__/hadoop-mapreduce-client-core-2.7.1.jar
24907462  300 -rwxr-xr-x   1 yarn     hadoop     305001 Apr  4 16:10 ./__spark_libs__/commons-httpclient-3.1.jar
24641845  640 -rwxr-xr-x   1 yarn     hadoop     654216 Apr  4 16:10 ./__spark_libs__/pmml-model-1.2.15.jar
24907518 1768 -rwxr-xr-x   1 yarn     hadoop    1809447 Apr  4 16:10 ./__spark_libs__/datanucleus-rdbms-3.2.9.jar
24641807    8 -rwxr-xr-x   1 yarn     hadoop       5950 Apr  4 16:10 ./__spark_libs__/javax.inject-2.4.0-b34.jar
24641752  176 -rwxr-xr-x   1 yarn     hadoop     177131 Apr  4 16:10 ./__spark_libs__/jetty-util-6.1.26.jar
24907351  116 -rwxr-xr-x   1 yarn     hadoop     114913 Apr  4 16:10 ./__spark_libs__/py4j-0.10.3.jar
24907636  140 -rwxr-xr-x   1 yarn     hadoop     142631 Apr  4 16:10 ./__spark_libs__/hadoop-yarn-client-2.7.1.jar
24907484 11452 -rwxr-xr-x   1 yarn     hadoop   11723537 Apr  4 16:10 ./__spark_libs__/spark-core_2.11-2.0.1.jar
24641854  176 -rwxr-xr-x   1 yarn     hadoop     177832 Apr  4 16:10 ./__spark_libs__/spark-mllib-local_2.11-2.0.1.jar
24907249  524 -rwxr-xr-x   1 yarn     hadoop     533455 Apr  4 16:10 ./__spark_libs__/protobuf-java-2.5.0.jar
24907602 1972 -rwxr-xr-x   1 yarn     hadoop    2015514 Apr  4 16:10 ./__spark_libs__/hadoop-yarn-api-2.7.1.jar
24907384  356 -rwxr-xr-x   1 yarn     hadoop     363908 Apr  4 16:10 ./__spark_libs__/hadoop-yarn-server-common-2.7.1.jar
24907671   24 -rwxr-xr-x   1 yarn     hadoop      21243 Apr  4 16:10 ./__spark_libs__/parquet-generator-1.7.0.jar
24904357  164 -rwxr-xr-x   1 yarn     hadoop     164422 Apr  4 16:10 ./__spark_libs__/core-1.1.2.jar
24907343  116 -rwxr-xr-x   1 yarn     hadoop     115534 Apr  4 16:10 ./__spark_libs__/javax.ws.rs-api-2.0.1.jar
24907241   72 -rwxr-xr-x   1 yarn     hadoop      72733 Apr  4 16:10 ./__spark_libs__/jersey-media-jaxb-2.22.2.jar
24907275   68 -rwxr-xr-x   1 yarn     hadoop      68866 Apr  4 16:10 ./__spark_libs__/curator-client-2.6.0.jar
10486938    4 -rwx------   1 yarn     hadoop        730 May 16 11:11 ./default_container_executor.sh
10486936    4 -rwx------   1 yarn     hadoop        676 May 16 11:11 ./default_container_executor_session.sh
broken symlinks(find -L . -maxdepth 5 -type l -ls):
End of LogType:directory.info

LogType:launch_container.sh
Log Upload Time:Tue May 16 11:12:18 +0700 2017
LogLength:75972
Log Contents:
#!/bin/bash

export SPARK_YARN_STAGING_DIR="hdfs://c408.hadoop.gda.lo:8020/user/fairy/.sparkStaging/application_1494395298335_303847"
export LOCAL_DIRS="/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847"
export HADOOP_CONF_DIR="/usr/hdp/current/hadoop-client/conf"
export NM_HTTP_PORT="8042"
export JAVA_HOME="/usr/jdk64/jdk1.8.0_92/"
export LOG_DIRS="/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000002"
export NM_AUX_SERVICE_mapreduce_shuffle="AAA0+gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=
"
export NM_PORT="45454"
export USER="fairy"
export HADOOP_YARN_HOME="/usr/hdp/current/hadoop-yarn-nodemanager"
export CLASSPATH="$PWD:$PWD/__spark_conf__:$PWD/__spark_libs__/*:$HADOOP_CONF_DIR:/usr/hdp/current/hadoop-client/*:/usr/hdp/current/hadoop-client/lib/*:/usr/hdp/current/hadoop-hdfs-client/*:/usr/hdp/current/hadoop-hdfs-client/lib/*:/usr/hdp/current/hadoop-yarn-client/*:/usr/hdp/current/hadoop-yarn-client/lib/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.4.2.0-258/hadoop/lib/hadoop-lzo-0.6.0.2.4.2.0-258.jar:/etc/hadoop/conf/secure"
export SPARK_LOG_URL_STDOUT="http://c412.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_02_000002/fairy/stdout?start=-4096"
export NM_HOST="c412.hadoop.gda.lo"
export SPARK_YARN_MODE="true"
export HADOOP_TOKEN_FILE_LOCATION="/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_02_000002/container_tokens"
export NM_AUX_SERVICE_spark_shuffle=""
export SPARK_USER="fairy"
export SPARK_LOG_URL_STDERR="http://c412.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_02_000002/fairy/stderr?start=-4096"
export LOGNAME="fairy"
export JVM_PID="$$"
export PWD="/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_02_000002"
export HOME="/home/"
export CONTAINER_ID="container_e81_1494395298335_303847_02_000002"
export MALLOC_ARENA_MAX="4"
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355866/spark-hive_2.11-2.0.1.jar" "__spark_libs__/spark-hive_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355747/parquet-hadoop-1.7.0.jar" "__spark_libs__/parquet-hadoop-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355761/calcite-avatica-1.2.0-incubating.jar" "__spark_libs__/calcite-avatica-1.2.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355802/commons-lang3-3.3.2.jar" "__spark_libs__/commons-lang3-3.3.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355856/guava-14.0.1.jar" "__spark_libs__/guava-14.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355693/objenesis-2.1.jar" "__spark_libs__/objenesis-2.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355806/chill_2.11-0.8.0.jar" "__spark_libs__/chill_2.11-0.8.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355810/commons-httpclient-3.1.jar" "__spark_libs__/commons-httpclient-3.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355756/avro-ipc-1.7.7.jar" "__spark_libs__/avro-ipc-1.7.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355718/jackson-module-paranamer-2.6.5.jar" "__spark_libs__/jackson-module-paranamer-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355809/spark-mllib_2.11-2.0.1.jar" "__spark_libs__/spark-mllib_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355714/jackson-core-2.6.5.jar" "__spark_libs__/jackson-core-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355803/parquet-common-1.7.0.jar" "__spark_libs__/parquet-common-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355778/aopalliance-repackaged-2.4.0-b34.jar" "__spark_libs__/aopalliance-repackaged-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355708/metrics-jvm-3.1.2.jar" "__spark_libs__/metrics-jvm-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355775/curator-client-2.6.0.jar" "__spark_libs__/curator-client-2.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355854/hadoop-mapreduce-client-jobclient-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-jobclient-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355871/parquet-generator-1.7.0.jar" "__spark_libs__/parquet-generator-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355691/commons-pool-1.5.4.jar" "__spark_libs__/commons-pool-1.5.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355765/spark-sketch_2.11-2.0.1.jar" "__spark_libs__/spark-sketch_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355725/jackson-core-asl-1.9.13.jar" "__spark_libs__/jackson-core-asl-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355703/json4s-core_2.11-3.2.11.jar" "__spark_libs__/json4s-core_2.11-3.2.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355800/hive-jdbc-1.2.1.spark2.jar" "__spark_libs__/hive-jdbc-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355804/hadoop-yarn-server-common-2.7.1.jar" "__spark_libs__/hadoop-yarn-server-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355728/jersey-container-servlet-2.22.2.jar" "__spark_libs__/jersey-container-servlet-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355820/hive-beeline-1.2.1.spark2.jar" "__spark_libs__/hive-beeline-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355737/jackson-mapper-asl-1.9.13.jar" "__spark_libs__/jackson-mapper-asl-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355705/jersey-container-servlet-core-2.22.2.jar" "__spark_libs__/jersey-container-servlet-core-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355834/snappy-java-1.1.2.6.jar" "__spark_libs__/snappy-java-1.1.2.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355757/httpclient-4.5.2.jar" "__spark_libs__/httpclient-4.5.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355692/spark-tags_2.11-2.0.1.jar" "__spark_libs__/spark-tags_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355695/hadoop-hdfs-2.7.1.jar" "__spark_libs__/hadoop-hdfs-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355690/htrace-core-3.1.0-incubating.jar" "__spark_libs__/htrace-core-3.1.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355842/parquet-jackson-1.7.0.jar" "__spark_libs__/parquet-jackson-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355843/xz-1.0.jar" "__spark_libs__/xz-1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355758/calcite-linq4j-1.2.0-incubating.jar" "__spark_libs__/calcite-linq4j-1.2.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355764/jersey-media-jaxb-2.22.2.jar" "__spark_libs__/jersey-media-jaxb-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355676/zookeeper-3.4.6.jar" "__spark_libs__/zookeeper-3.4.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355671/hadoop-mapreduce-client-core-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-core-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355771/bcprov-jdk15on-1.51.jar" "__spark_libs__/bcprov-jdk15on-1.51.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355751/hive-exec-1.2.1.spark2.jar" "__spark_libs__/hive-exec-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355829/kryo-shaded-3.0.3.jar" "__spark_libs__/kryo-shaded-3.0.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355738/breeze_2.11-0.11.2.jar" "__spark_libs__/breeze_2.11-0.11.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355837/metrics-graphite-3.1.2.jar" "__spark_libs__/metrics-graphite-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355772/spire-macros_2.11-0.7.4.jar" "__spark_libs__/spire-macros_2.11-0.7.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355729/janino-2.7.8.jar" "__spark_libs__/janino-2.7.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355727/opencsv-2.3.jar" "__spark_libs__/opencsv-2.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/usercache/fairy/filecache/38116/__spark_conf__.zip" "__spark_conf__"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355777/datanucleus-api-jdo-3.2.6.jar" "__spark_libs__/datanucleus-api-jdo-3.2.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355694/validation-api-1.1.0.Final.jar" "__spark_libs__/validation-api-1.1.0.Final.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355700/json4s-jackson_2.11-3.2.11.jar" "__spark_libs__/json4s-jackson_2.11-3.2.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355795/py4j-0.10.3.jar" "__spark_libs__/py4j-0.10.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355734/commons-codec-1.10.jar" "__spark_libs__/commons-codec-1.10.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355825/jsr305-1.3.9.jar" "__spark_libs__/jsr305-1.3.9.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355836/osgi-resource-locator-1.0.1.jar" "__spark_libs__/osgi-resource-locator-1.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355786/parquet-encoding-1.7.0.jar" "__spark_libs__/parquet-encoding-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355794/hadoop-yarn-common-2.7.1.jar" "__spark_libs__/hadoop-yarn-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355686/hadoop-client-2.7.1.jar" "__spark_libs__/hadoop-client-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355711/mysql-connector-java-5.0.8-bin.jar" "__spark_libs__/mysql-connector-java-5.0.8-bin.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355736/spark-yarn_2.11-2.0.1.jar" "__spark_libs__/spark-yarn_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355811/jdo-api-3.0.1.jar" "__spark_libs__/jdo-api-3.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355798/parquet-format-2.3.0-incubating.jar" "__spark_libs__/parquet-format-2.3.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355685/json-20090211.jar" "__spark_libs__/json-20090211.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355766/protobuf-java-2.5.0.jar" "__spark_libs__/protobuf-java-2.5.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355864/lift-json_2.11-2.6.3.jar" "__spark_libs__/lift-json_2.11-2.6.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355680/apacheds-i18n-2.0.0-M15.jar" "__spark_libs__/apacheds-i18n-2.0.0-M15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355785/scala-parser-combinators_2.11-1.0.4.jar" "__spark_libs__/scala-parser-combinators_2.11-1.0.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355699/jcl-over-slf4j-1.7.16.jar" "__spark_libs__/jcl-over-slf4j-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355781/jsp-api-2.1.jar" "__spark_libs__/jsp-api-2.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355855/apache-log4j-extras-1.2.17.jar" "__spark_libs__/apache-log4j-extras-1.2.17.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355828/java-xmlbuilder-1.0.jar" "__spark_libs__/java-xmlbuilder-1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355769/commons-io-2.4.jar" "__spark_libs__/commons-io-2.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355835/httpcore-4.4.4.jar" "__spark_libs__/httpcore-4.4.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355688/javax.servlet-api-3.1.0.jar" "__spark_libs__/javax.servlet-api-3.1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355813/minlog-1.3.0.jar" "__spark_libs__/minlog-1.3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355833/eigenbase-properties-1.1.5.jar" "__spark_libs__/eigenbase-properties-1.1.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355673/commons-beanutils-core-1.8.0.jar" "__spark_libs__/commons-beanutils-core-1.8.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355702/commons-beanutils-1.7.0.jar" "__spark_libs__/commons-beanutils-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355876/netty-all-4.0.29.Final.jar" "__spark_libs__/netty-all-4.0.29.Final.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355701/pmml-model-1.2.15.jar" "__spark_libs__/pmml-model-1.2.15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355865/hk2-utils-2.4.0-b34.jar" "__spark_libs__/hk2-utils-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355817/commons-net-2.2.jar" "__spark_libs__/commons-net-2.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355753/base64-2.3.8.jar" "__spark_libs__/base64-2.3.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355790/snappy-0.2.jar" "__spark_libs__/snappy-0.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355830/guice-3.0.jar" "__spark_libs__/guice-3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355867/mx4j-3.0.2.jar" "__spark_libs__/mx4j-3.0.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355755/hk2-locator-2.4.0-b34.jar" "__spark_libs__/hk2-locator-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355821/spark-network-common_2.11-2.0.1.jar" "__spark_libs__/spark-network-common_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355746/javax.inject-1.jar" "__spark_libs__/javax.inject-1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355760/commons-compress-1.4.1.jar" "__spark_libs__/commons-compress-1.4.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355731/pyrolite-4.9.jar" "__spark_libs__/pyrolite-4.9.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355819/api-util-1.0.0-M20.jar" "__spark_libs__/api-util-1.0.0-M20.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355733/datanucleus-core-3.2.10.jar" "__spark_libs__/datanucleus-core-3.2.10.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355724/scalap-2.11.8.jar" "__spark_libs__/scalap-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355726/commons-digester-1.8.jar" "__spark_libs__/commons-digester-1.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355784/spark-unsafe_2.11-2.0.1.jar" "__spark_libs__/spark-unsafe_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355808/lz4-1.3.0.jar" "__spark_libs__/lz4-1.3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355787/jtransforms-2.4.0.jar" "__spark_libs__/jtransforms-2.4.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355818/spark-sql_2.11-2.0.1.jar" "__spark_libs__/spark-sql_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355844/jackson-jaxrs-1.9.13.jar" "__spark_libs__/jackson-jaxrs-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355763/spark-catalyst_2.11-2.0.1.jar" "__spark_libs__/spark-catalyst_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355715/spark-repl_2.11-2.0.1.jar" "__spark_libs__/spark-repl_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355869/stax-api-1.0.1.jar" "__spark_libs__/stax-api-1.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355796/commons-configuration-1.6.jar" "__spark_libs__/commons-configuration-1.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355862/javolution-5.5.1.jar" "__spark_libs__/javolution-5.5.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/usercache/fairy/filecache/38115/stats-etlr-1.0.jar" "__app__.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355874/libfb303-0.9.2.jar" "__spark_libs__/libfb303-0.9.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355832/javassist-3.18.1-GA.jar" "__spark_libs__/javassist-3.18.1-GA.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355696/spark-streaming_2.11-2.0.1.jar" "__spark_libs__/spark-streaming_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355852/jackson-module-scala_2.11-2.6.5.jar" "__spark_libs__/jackson-module-scala_2.11-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355750/leveldbjni-all-1.8.jar" "__spark_libs__/leveldbjni-all-1.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355780/stringtemplate-3.2.1.jar" "__spark_libs__/stringtemplate-3.2.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355857/jackson-databind-2.6.5.jar" "__spark_libs__/jackson-databind-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355745/jersey-bundle-1.19.1.jar" "__spark_libs__/jersey-bundle-1.19.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355848/json4s-ast_2.11-3.2.11.jar" "__spark_libs__/json4s-ast_2.11-3.2.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355868/spark-hive-thriftserver_2.11-2.0.1.jar" "__spark_libs__/spark-hive-thriftserver_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355840/mail-1.4.7.jar" "__spark_libs__/mail-1.4.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355710/paranamer-2.3.jar" "__spark_libs__/paranamer-2.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355799/jpam-1.1.jar" "__spark_libs__/jpam-1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355805/commons-collections-3.2.2.jar" "__spark_libs__/commons-collections-3.2.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355850/jline-2.12.1.jar" "__spark_libs__/jline-2.12.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355713/gson-2.2.4.jar" "__spark_libs__/gson-2.2.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355792/stax-api-1.0-2.jar" "__spark_libs__/stax-api-1.0-2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355776/hadoop-annotations-2.7.1.jar" "__spark_libs__/hadoop-annotations-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355770/jaxb-api-2.2.2.jar" "__spark_libs__/jaxb-api-2.2.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355742/ivy-2.4.0.jar" "__spark_libs__/ivy-2.4.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355791/jersey-guava-2.22.2.jar" "__spark_libs__/jersey-guava-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355870/commons-cli-1.2.jar" "__spark_libs__/commons-cli-1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355730/breeze-macros_2.11-0.11.2.jar" "__spark_libs__/breeze-macros_2.11-0.11.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355822/derby-10.12.1.1.jar" "__spark_libs__/derby-10.12.1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355739/spark-launcher_2.11-2.0.1.jar" "__spark_libs__/spark-launcher_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355812/jul-to-slf4j-1.7.16.jar" "__spark_libs__/jul-to-slf4j-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355875/spire_2.11-0.7.4.jar" "__spark_libs__/spire_2.11-0.7.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355762/scala-reflect-2.11.8.jar" "__spark_libs__/scala-reflect-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355846/hadoop-mapreduce-client-shuffle-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-shuffle-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355839/commons-logging-1.1.3.jar" "__spark_libs__/commons-logging-1.1.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355732/core-1.1.2.jar" "__spark_libs__/core-1.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355754/jetty-6.1.26.jar" "__spark_libs__/jetty-6.1.26.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355860/hadoop-yarn-client-2.7.1.jar" "__spark_libs__/hadoop-yarn-client-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355681/metrics-json-3.1.2.jar" "__spark_libs__/metrics-json-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355767/antlr-2.7.7.jar" "__spark_libs__/antlr-2.7.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355859/jodd-core-3.5.2.jar" "__spark_libs__/jodd-core-3.5.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355815/spark-core_2.11-2.0.1.jar" "__spark_libs__/spark-core_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355858/activation-1.1.1.jar" "__spark_libs__/activation-1.1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355720/commons-dbcp-1.4.jar" "__spark_libs__/commons-dbcp-1.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355845/jackson-xc-1.9.13.jar" "__spark_libs__/jackson-xc-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355741/slf4j-log4j12-1.7.16.jar" "__spark_libs__/slf4j-log4j12-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355677/super-csv-2.2.0.jar" "__spark_libs__/super-csv-2.2.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355826/xmlenc-0.52.jar" "__spark_libs__/xmlenc-0.52.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355748/compress-lzf-1.0.3.jar" "__spark_libs__/compress-lzf-1.0.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355823/datanucleus-rdbms-3.2.9.jar" "__spark_libs__/datanucleus-rdbms-3.2.9.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355851/hadoop-common-2.7.1.jar" "__spark_libs__/hadoop-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355847/spark-graphx_2.11-2.0.1.jar" "__spark_libs__/spark-graphx_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355759/scala-library-2.11.8.jar" "__spark_libs__/scala-library-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355768/antlr-runtime-3.4.jar" "__spark_libs__/antlr-runtime-3.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355697/scala-compiler-2.11.8.jar" "__spark_libs__/scala-compiler-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355814/javax.annotation-api-1.2.jar" "__spark_libs__/javax.annotation-api-1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355740/hadoop-yarn-server-web-proxy-2.7.1.jar" "__spark_libs__/hadoop-yarn-server-web-proxy-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355816/xercesImpl-2.9.1.jar" "__spark_libs__/xercesImpl-2.9.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355721/univocity-parsers-2.1.1.jar" "__spark_libs__/univocity-parsers-2.1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355878/hadoop-mapreduce-client-app-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-app-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355838/curator-framework-2.6.0.jar" "__spark_libs__/curator-framework-2.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355698/avro-1.7.7.jar" "__spark_libs__/avro-1.7.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355788/jersey-common-2.22.2.jar" "__spark_libs__/jersey-common-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355670/metrics-core-3.1.2.jar" "__spark_libs__/metrics-core-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355706/xbean-asm5-shaded-4.4.jar" "__spark_libs__/xbean-asm5-shaded-4.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355872/stream-2.7.0.jar" "__spark_libs__/stream-2.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355831/oro-2.0.8.jar" "__spark_libs__/oro-2.0.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355674/avro-mapred-1.7.7-hadoop2.jar" "__spark_libs__/avro-mapred-1.7.7-hadoop2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355752/aopalliance-1.0.jar" "__spark_libs__/aopalliance-1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355682/spark-network-shuffle_2.11-2.0.1.jar" "__spark_libs__/spark-network-shuffle_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355879/hadoop-mapreduce-client-common-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355679/guice-servlet-3.0.jar" "__spark_libs__/guice-servlet-3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355743/bonecp-0.8.0.RELEASE.jar" "__spark_libs__/bonecp-0.8.0.RELEASE.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355687/scala-xml_2.11-1.0.2.jar" "__spark_libs__/scala-xml_2.11-1.0.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355675/jetty-util-6.1.26.jar" "__spark_libs__/jetty-util-6.1.26.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355801/hadoop-auth-2.7.1.jar" "__spark_libs__/hadoop-auth-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355683/jackson-annotations-2.6.5.jar" "__spark_libs__/jackson-annotations-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355704/spark-mllib-local_2.11-2.0.1.jar" "__spark_libs__/spark-mllib-local_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355689/javax.inject-2.4.0-b34.jar" "__spark_libs__/javax.inject-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355774/netty-3.8.0.Final.jar" "__spark_libs__/netty-3.8.0.Final.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355807/apacheds-kerberos-codec-2.0.0-M15.jar" "__spark_libs__/apacheds-kerberos-codec-2.0.0-M15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355880/pmml-schema-1.2.15.jar" "__spark_libs__/pmml-schema-1.2.15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355873/antlr4-runtime-4.5.3.jar" "__spark_libs__/antlr4-runtime-4.5.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355793/javax.ws.rs-api-2.0.1.jar" "__spark_libs__/javax.ws.rs-api-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355672/libthrift-0.9.2.jar" "__spark_libs__/libthrift-0.9.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355678/calcite-core-1.2.0-incubating.jar" "__spark_libs__/calcite-core-1.2.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355717/curator-recipes-2.6.0.jar" "__spark_libs__/curator-recipes-2.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355877/jersey-client-2.22.2.jar" "__spark_libs__/jersey-client-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355827/commons-compiler-2.7.6.jar" "__spark_libs__/commons-compiler-2.7.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355722/hk2-api-2.4.0-b34.jar" "__spark_libs__/hk2-api-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355735/slf4j-api-1.7.16.jar" "__spark_libs__/slf4j-api-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355782/hive-metastore-1.2.1.spark2.jar" "__spark_libs__/hive-metastore-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355863/RoaringBitmap-0.5.11.jar" "__spark_libs__/RoaringBitmap-0.5.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355841/parquet-column-1.7.0.jar" "__spark_libs__/parquet-column-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355797/log4j-1.2.17.jar" "__spark_libs__/log4j-1.2.17.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355849/hadoop-yarn-api-2.7.1.jar" "__spark_libs__/hadoop-yarn-api-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355773/mesos-0.21.1-shaded-protobuf.jar" "__spark_libs__/mesos-0.21.1-shaded-protobuf.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355779/commons-lang-2.6.jar" "__spark_libs__/commons-lang-2.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355712/JavaEWAH-0.3.2.jar" "__spark_libs__/JavaEWAH-0.3.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355824/joda-time-2.9.3.jar" "__spark_libs__/joda-time-2.9.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355707/commons-math3-3.4.1.jar" "__spark_libs__/commons-math3-3.4.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355861/jta-1.1.jar" "__spark_libs__/jta-1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355744/chill-java-0.8.0.jar" "__spark_libs__/chill-java-0.8.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355853/arpack_combined_all-0.1.jar" "__spark_libs__/arpack_combined_all-0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355684/hive-cli-1.2.1.spark2.jar" "__spark_libs__/hive-cli-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355709/parquet-hadoop-bundle-1.6.0.jar" "__spark_libs__/parquet-hadoop-bundle-1.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355716/jersey-server-2.22.2.jar" "__spark_libs__/jersey-server-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355749/ST4-4.0.4.jar" "__spark_libs__/ST4-4.0.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355789/api-asn1-api-1.0.0-M20.jar" "__spark_libs__/api-asn1-api-1.0.0-M20.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/355719/jets3t-0.9.3.jar" "__spark_libs__/jets3t-0.9.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
# Creating copy of launch script
cp "launch_container.sh" "/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000002/launch_container.sh"
chmod 640 "/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000002/launch_container.sh"
# Determining directory contents
echo "ls -l:" 1>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000002/directory.info"
ls -l 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000002/directory.info"
echo "find -L . -maxdepth 5 -ls:" 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000002/directory.info"
find -L . -maxdepth 5 -ls 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000002/directory.info"
echo "broken symlinks(find -L . -maxdepth 5 -type l -ls):" 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000002/directory.info"
find -L . -maxdepth 5 -type l -ls 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000002/directory.info"
exec /bin/bash -c "$JAVA_HOME/bin/java -server -Xmx3072m '-Dhdp.version=2.4.2.0-258' -Djava.io.tmpdir=$PWD/tmp '-Dspark.driver.port=39032' '-Dspark.history.ui.port=18080' '-Dspark.ui.port=0' -Dspark.yarn.app.container.log.dir=/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000002 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.60.43.16:39032 --executor-id 1 --hostname c412.hadoop.gda.lo --cores 1 --app-id application_1494395298335_303847 --user-class-path file:$PWD/__app__.jar 1> /hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000002/stdout 2> /hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000002/stderr"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
End of LogType:launch_container.sh

LogType:stderr
Log Upload Time:Tue May 16 11:12:18 +0700 2017
LogLength:25220
Log Contents:
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/hadoop/yarn/local/filecache/355741/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/hdp/2.4.2.0-258/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
17/05/16 11:11:04 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 30661@c412.hadoop.gda.lo
17/05/16 11:11:04 INFO util.SignalUtils: Registered signal handler for TERM
17/05/16 11:11:04 INFO util.SignalUtils: Registered signal handler for HUP
17/05/16 11:11:04 INFO util.SignalUtils: Registered signal handler for INT
17/05/16 11:11:05 INFO spark.SecurityManager: Changing view acls to: yarn,fairy
17/05/16 11:11:05 INFO spark.SecurityManager: Changing modify acls to: yarn,fairy
17/05/16 11:11:05 INFO spark.SecurityManager: Changing view acls groups to: 
17/05/16 11:11:05 INFO spark.SecurityManager: Changing modify acls groups to: 
17/05/16 11:11:05 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(yarn, fairy); groups with view permissions: Set(); users  with modify permissions: Set(yarn, fairy); groups with modify permissions: Set()
17/05/16 11:11:07 INFO client.TransportClientFactory: Successfully created connection to /10.60.43.16:39032 after 1221 ms (0 ms spent in bootstraps)
17/05/16 11:11:07 INFO spark.SecurityManager: Changing view acls to: yarn,fairy
17/05/16 11:11:07 INFO spark.SecurityManager: Changing modify acls to: yarn,fairy
17/05/16 11:11:07 INFO spark.SecurityManager: Changing view acls groups to: 
17/05/16 11:11:07 INFO spark.SecurityManager: Changing modify acls groups to: 
17/05/16 11:11:07 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(yarn, fairy); groups with view permissions: Set(); users  with modify permissions: Set(yarn, fairy); groups with modify permissions: Set()
17/05/16 11:11:07 INFO client.TransportClientFactory: Successfully created connection to /10.60.43.16:39032 after 2 ms (0 ms spent in bootstraps)
17/05/16 11:11:07 INFO storage.DiskBlockManager: Created local directory at /hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/blockmgr-2b186756-5482-4bd1-a9db-77fb677c95c7
17/05/16 11:11:07 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MB
17/05/16 11:11:07 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@10.60.43.16:39032
17/05/16 11:11:07 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver
17/05/16 11:11:07 INFO executor.Executor: Starting executor ID 1 on host c412.hadoop.gda.lo
17/05/16 11:11:08 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 29206.
17/05/16 11:11:08 INFO netty.NettyBlockTransferService: Server created on c412.hadoop.gda.lo:29206
17/05/16 11:11:08 INFO storage.BlockManager: external shuffle service port = 7337
17/05/16 11:11:08 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(1, c412.hadoop.gda.lo, 29206)
17/05/16 11:11:08 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(1, c412.hadoop.gda.lo, 29206)
17/05/16 11:11:08 INFO storage.BlockManager: Registering executor with local external shuffle service.
17/05/16 11:11:08 INFO client.TransportClientFactory: Successfully created connection to c412.hadoop.gda.lo/10.60.43.12:7337 after 2 ms (0 ms spent in bootstraps)
17/05/16 11:11:18 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 0
17/05/16 11:11:18 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 0)
17/05/16 11:11:18 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 0
17/05/16 11:11:18 INFO client.TransportClientFactory: Successfully created connection to /10.60.43.16:33694 after 3 ms (0 ms spent in bootstraps)
17/05/16 11:11:18 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.2 KB, free 1458.6 MB)
17/05/16 11:11:18 INFO broadcast.TorrentBroadcast: Reading broadcast variable 0 took 145 ms
17/05/16 11:11:18 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 30.3 KB, free 1458.6 MB)
17/05/16 11:11:18 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
17/05/16 11:11:18 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.60.43.16:39032)
17/05/16 11:11:18 INFO spark.MapOutputTrackerWorker: Got the output locations
17/05/16 11:11:18 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:18 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms
17/05/16 11:11:19 INFO codegen.CodeGenerator: Code generated in 353.5744 ms
17/05/16 11:11:19 INFO codegen.CodeGenerator: Code generated in 17.400482 ms
17/05/16 11:11:19 INFO codegen.CodeGenerator: Code generated in 12.220559 ms
17/05/16 11:11:19 INFO codegen.CodeGenerator: Code generated in 12.499702 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 0). 4219 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 11
17/05/16 11:11:19 INFO executor.Executor: Running task 11.0 in stage 1.0 (TID 11)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 11.0 in stage 1.0 (TID 11). 3414 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 16
17/05/16 11:11:19 INFO executor.Executor: Running task 16.0 in stage 1.0 (TID 16)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 16.0 in stage 1.0 (TID 16). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 19
17/05/16 11:11:19 INFO executor.Executor: Running task 19.0 in stage 1.0 (TID 19)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 19.0 in stage 1.0 (TID 19). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 23
17/05/16 11:11:19 INFO executor.Executor: Running task 23.0 in stage 1.0 (TID 23)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 23.0 in stage 1.0 (TID 23). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 26
17/05/16 11:11:19 INFO executor.Executor: Running task 26.0 in stage 1.0 (TID 26)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 26.0 in stage 1.0 (TID 26). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 30
17/05/16 11:11:19 INFO executor.Executor: Running task 30.0 in stage 1.0 (TID 30)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 30.0 in stage 1.0 (TID 30). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 33
17/05/16 11:11:19 INFO executor.Executor: Running task 33.0 in stage 1.0 (TID 33)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 33.0 in stage 1.0 (TID 33). 3414 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 38
17/05/16 11:11:19 INFO executor.Executor: Running task 38.0 in stage 1.0 (TID 38)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 38.0 in stage 1.0 (TID 38). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 41
17/05/16 11:11:19 INFO executor.Executor: Running task 41.0 in stage 1.0 (TID 41)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 41.0 in stage 1.0 (TID 41). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 72
17/05/16 11:11:19 INFO executor.Executor: Running task 72.0 in stage 1.0 (TID 72)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 72.0 in stage 1.0 (TID 72). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 77
17/05/16 11:11:19 INFO executor.Executor: Running task 77.0 in stage 1.0 (TID 77)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 77.0 in stage 1.0 (TID 77). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 80
17/05/16 11:11:19 INFO executor.Executor: Running task 80.0 in stage 1.0 (TID 80)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 80.0 in stage 1.0 (TID 80). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 87
17/05/16 11:11:19 INFO executor.Executor: Running task 87.0 in stage 1.0 (TID 87)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 87.0 in stage 1.0 (TID 87). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 91
17/05/16 11:11:19 INFO executor.Executor: Running task 91.0 in stage 1.0 (TID 91)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 91.0 in stage 1.0 (TID 91). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 95
17/05/16 11:11:19 INFO executor.Executor: Running task 95.0 in stage 1.0 (TID 95)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 95.0 in stage 1.0 (TID 95). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 99
17/05/16 11:11:19 INFO executor.Executor: Running task 99.0 in stage 1.0 (TID 99)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 99.0 in stage 1.0 (TID 99). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 104
17/05/16 11:11:19 INFO executor.Executor: Running task 104.0 in stage 1.0 (TID 104)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 104.0 in stage 1.0 (TID 104). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 107
17/05/16 11:11:19 INFO executor.Executor: Running task 107.0 in stage 1.0 (TID 107)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 107.0 in stage 1.0 (TID 107). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 111
17/05/16 11:11:19 INFO executor.Executor: Running task 111.0 in stage 1.0 (TID 111)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 111.0 in stage 1.0 (TID 111). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 114
17/05/16 11:11:19 INFO executor.Executor: Running task 114.0 in stage 1.0 (TID 114)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 114.0 in stage 1.0 (TID 114). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 118
17/05/16 11:11:19 INFO executor.Executor: Running task 118.0 in stage 1.0 (TID 118)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 118.0 in stage 1.0 (TID 118). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 122
17/05/16 11:11:19 INFO executor.Executor: Running task 122.0 in stage 1.0 (TID 122)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 122.0 in stage 1.0 (TID 122). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 127
17/05/16 11:11:19 INFO executor.Executor: Running task 127.0 in stage 1.0 (TID 127)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 127.0 in stage 1.0 (TID 127). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 132
17/05/16 11:11:19 INFO executor.Executor: Running task 132.0 in stage 1.0 (TID 132)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 132.0 in stage 1.0 (TID 132). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 137
17/05/16 11:11:19 INFO executor.Executor: Running task 137.0 in stage 1.0 (TID 137)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 137.0 in stage 1.0 (TID 137). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 141
17/05/16 11:11:20 INFO executor.Executor: Running task 141.0 in stage 1.0 (TID 141)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 141.0 in stage 1.0 (TID 141). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 144
17/05/16 11:11:20 INFO executor.Executor: Running task 144.0 in stage 1.0 (TID 144)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 144.0 in stage 1.0 (TID 144). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 146
17/05/16 11:11:20 INFO executor.Executor: Running task 146.0 in stage 1.0 (TID 146)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 146.0 in stage 1.0 (TID 146). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 150
17/05/16 11:11:20 INFO executor.Executor: Running task 150.0 in stage 1.0 (TID 150)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 150.0 in stage 1.0 (TID 150). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 152
17/05/16 11:11:20 INFO executor.Executor: Running task 152.0 in stage 1.0 (TID 152)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 152.0 in stage 1.0 (TID 152). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 156
17/05/16 11:11:20 INFO executor.Executor: Running task 156.0 in stage 1.0 (TID 156)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 156.0 in stage 1.0 (TID 156). 3414 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 159
17/05/16 11:11:20 INFO executor.Executor: Running task 159.0 in stage 1.0 (TID 159)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 159.0 in stage 1.0 (TID 159). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 162
17/05/16 11:11:20 INFO executor.Executor: Running task 162.0 in stage 1.0 (TID 162)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 162.0 in stage 1.0 (TID 162). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 166
17/05/16 11:11:20 INFO executor.Executor: Running task 166.0 in stage 1.0 (TID 166)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 166.0 in stage 1.0 (TID 166). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 169
17/05/16 11:11:20 INFO executor.Executor: Running task 169.0 in stage 1.0 (TID 169)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 169.0 in stage 1.0 (TID 169). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 173
17/05/16 11:11:20 INFO executor.Executor: Running task 173.0 in stage 1.0 (TID 173)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 173.0 in stage 1.0 (TID 173). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 177
17/05/16 11:11:20 INFO executor.Executor: Running task 177.0 in stage 1.0 (TID 177)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 177.0 in stage 1.0 (TID 177). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 181
17/05/16 11:11:20 INFO executor.Executor: Running task 181.0 in stage 1.0 (TID 181)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 181.0 in stage 1.0 (TID 181). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 184
17/05/16 11:11:20 INFO executor.Executor: Running task 184.0 in stage 1.0 (TID 184)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 184.0 in stage 1.0 (TID 184). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 189
17/05/16 11:11:20 INFO executor.Executor: Running task 189.0 in stage 1.0 (TID 189)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 189.0 in stage 1.0 (TID 189). 3327 bytes result sent to driver
17/05/16 11:12:17 INFO executor.CoarseGrainedExecutorBackend: Driver commanded a shutdown
17/05/16 11:12:17 INFO executor.CoarseGrainedExecutorBackend: Driver from 10.60.43.16:39032 disconnected during shutdown
17/05/16 11:12:17 INFO executor.CoarseGrainedExecutorBackend: Driver from 10.60.43.16:39032 disconnected during shutdown
17/05/16 11:12:17 INFO memory.MemoryStore: MemoryStore cleared
17/05/16 11:12:17 INFO storage.BlockManager: BlockManager stopped
17/05/16 11:12:18 INFO util.ShutdownHookManager: Shutdown hook called
End of LogType:stderr

LogType:stdout
Log Upload Time:Tue May 16 11:12:18 +0700 2017
LogLength:0
Log Contents:
End of LogType:stdout



Container: container_e81_1494395298335_303847_01_000005 on c414.hadoop.gda.lo_45454
=====================================================================================
LogType:directory.info
Log Upload Time:Tue May 16 11:12:18 +0700 2017
LogLength:28625
Log Contents:
ls -l:
total 112
lrwxrwxrwx 1 yarn hadoop    69 May 16 11:10 __app__.jar -> /hadoop/yarn/local/usercache/fairy/filecache/38959/stats-etlr-1.0.jar
-rw-r--r-- 1 yarn hadoop    87 May 16 11:10 container_tokens
-rwx------ 1 yarn hadoop   676 May 16 11:10 default_container_executor_session.sh
-rwx------ 1 yarn hadoop   730 May 16 11:10 default_container_executor.sh
-rwx------ 1 yarn hadoop 75763 May 16 11:10 launch_container.sh
lrwxrwxrwx 1 yarn hadoop    69 May 16 11:10 __spark_conf__ -> /hadoop/yarn/local/usercache/fairy/filecache/38960/__spark_conf__.zip
drwxr-xr-x 2 yarn hadoop 12288 May 16 11:10 __spark_libs__
drwx--x--- 2 yarn hadoop  4096 May 16 11:10 tmp
find -L . -maxdepth 5 -ls:
5375228    4 drwx--x---   4 yarn     hadoop       4096 May 16 11:10 .
5375238    4 drwx------   2 yarn     hadoop       4096 May 16 11:10 ./__spark_conf__
5375264    4 -r-x------   1 yarn     hadoop       1527 May 16 11:10 ./__spark_conf__/kms-env.sh
5375243   20 -r-x------   1 yarn     hadoop      16530 May 16 11:10 ./__spark_conf__/yarn-site.xml
5375265    4 -r-x------   1 yarn     hadoop       1308 May 16 11:10 ./__spark_conf__/hadoop-policy.xml
5375271    8 -r-x------   1 yarn     hadoop       4113 May 16 11:10 ./__spark_conf__/mapred-queues.xml.template
5375272    8 -r-x------   1 yarn     hadoop       5511 May 16 11:10 ./__spark_conf__/kms-site.xml
5375239    8 -r-x------   1 yarn     hadoop       6836 May 16 11:10 ./__spark_conf__/mapred-site.xml
5375254    4 -r-x------   1 yarn     hadoop       1335 May 16 11:10 ./__spark_conf__/configuration.xsl
5375260    8 -r-x------   1 yarn     hadoop       4221 May 16 11:10 ./__spark_conf__/task-log4j.properties
5375252    4 -r-x------   1 yarn     hadoop        758 May 16 11:10 ./__spark_conf__/mapred-site.xml.template
5375253    4 -r-x------   1 yarn     hadoop       2358 May 16 11:10 ./__spark_conf__/topology_script.py
5375257    4 -r-x------   1 yarn     hadoop       1019 May 16 11:10 ./__spark_conf__/container-executor.cfg
5375250    8 -r-x------   1 yarn     hadoop       5404 May 16 11:10 ./__spark_conf__/capacity-scheduler.xml
5375242    4 -r-x------   1 yarn     hadoop       2302 May 16 11:10 ./__spark_conf__/hadoop-metrics2.properties
5375266    4 -r-x------   1 yarn     hadoop        229 May 16 11:10 ./__spark_conf__/slaves
5375249    4 -r-x------   1 yarn     hadoop        884 May 16 11:10 ./__spark_conf__/ssl-client.xml
5375262    4 -r-x------   1 yarn     hadoop       1602 May 16 11:10 ./__spark_conf__/health_check
5375270    4 -r-x------   1 yarn     hadoop       2268 May 16 11:10 ./__spark_conf__/ssl-server.xml.example
5375261    4 -r-x------   1 yarn     hadoop        690 May 16 11:10 ./__spark_conf__/mapred-env.sh
5375240    8 -r-x------   1 yarn     hadoop       5467 May 16 11:10 ./__spark_conf__/hadoop-env.sh
5375258    4 -r-x------   1 yarn     hadoop       1020 May 16 11:10 ./__spark_conf__/commons-logging.properties
5375273    4 -r-x------   1 yarn     hadoop        945 May 16 11:10 ./__spark_conf__/taskcontroller.cfg
5375244    4 -r-x------   1 yarn     hadoop       3979 May 16 11:10 ./__spark_conf__/hadoop-env.cmd
5375248    4 -r-x------   1 yarn     hadoop       2250 May 16 11:10 ./__spark_conf__/yarn-env.cmd
5375259    4 -r-x------   1 yarn     hadoop       2490 May 16 11:10 ./__spark_conf__/hadoop-metrics.properties
5375267    4 -r-x------   1 yarn     hadoop        724 May 16 11:10 ./__spark_conf__/topology_mappings.data
5375263    4 -r-x------   1 yarn     hadoop       2316 May 16 11:10 ./__spark_conf__/ssl-client.xml.example
5375274   36 -r-x------   1 yarn     hadoop      34061 May 16 11:10 ./__spark_conf__/__spark_conf__.properties
5375251    4 -r-x------   1 yarn     hadoop       3518 May 16 11:10 ./__spark_conf__/kms-acls.xml
5375268    4 -r-x------   1 yarn     hadoop        951 May 16 11:10 ./__spark_conf__/mapred-env.cmd
5375255    8 -r-x------   1 yarn     hadoop       5006 May 16 11:10 ./__spark_conf__/yarn-env.sh
5375246    8 -r-x------   1 yarn     hadoop       4197 May 16 11:10 ./__spark_conf__/core-site.xml
5375247    4 -r-x------   1 yarn     hadoop       1631 May 16 11:10 ./__spark_conf__/kms-log4j.properties
5375245    0 -r-x------   1 yarn     hadoop          0 May 16 11:10 ./__spark_conf__/yarn.exclude
5375241   12 -r-x------   1 yarn     hadoop       8699 May 16 11:10 ./__spark_conf__/log4j.properties
5375256    8 -r-x------   1 yarn     hadoop       6572 May 16 11:10 ./__spark_conf__/hdfs-site.xml
5375269    4 -r-x------   1 yarn     hadoop       1000 May 16 11:10 ./__spark_conf__/ssl-server.xml
5375279    4 -rw-r--r--   1 yarn     hadoop         16 May 16 11:10 ./.default_container_executor_session.sh.crc
5375278    4 -rwx------   1 yarn     hadoop        676 May 16 11:10 ./default_container_executor_session.sh
5375276   76 -rwx------   1 yarn     hadoop      75763 May 16 11:10 ./launch_container.sh
5375277    4 -rw-r--r--   1 yarn     hadoop        600 May 16 11:10 ./.launch_container.sh.crc
5375275    4 -rw-r--r--   1 yarn     hadoop         12 May 16 11:10 ./.container_tokens.crc
5375235    4 -rw-r--r--   1 yarn     hadoop         87 May 16 11:10 ./container_tokens
5375281    4 -rw-r--r--   1 yarn     hadoop         16 May 16 11:10 ./.default_container_executor.sh.crc
5375234    4 drwx--x---   2 yarn     hadoop       4096 May 16 11:10 ./tmp
5375280    4 -rwx------   1 yarn     hadoop        730 May 16 11:10 ./default_container_executor.sh
5375231 1152 -r-x------   1 yarn     hadoop    1178454 May 16 11:10 ./__app__.jar
5375282   12 drwxr-xr-x   2 yarn     hadoop      12288 May 16 11:10 ./__spark_libs__
4197096 1848 -rwxr-xr-x   1 yarn     hadoop    1890075 Apr  5 13:40 ./__spark_libs__/datanucleus-core-3.2.10.jar
4197398  696 -rwxr-xr-x   1 yarn     hadoop     710492 Apr  5 13:40 ./__spark_libs__/guice-3.0.jar
4197082   92 -rwxr-xr-x   1 yarn     hadoop      93407 Apr  5 13:40 ./__spark_libs__/pyrolite-4.9.jar
4197357 1204 -rwxr-xr-x   1 yarn     hadoop    1229125 Apr  5 13:40 ./__spark_libs__/xercesImpl-2.9.1.jar
4197063  932 -rwxr-xr-x   1 yarn     hadoop     951701 Apr  5 13:40 ./__spark_libs__/jersey-server-2.22.2.jar
4197265  280 -rwxr-xr-x   1 yarn     hadoop     285447 Apr  5 13:40 ./__spark_libs__/parquet-encoding-1.7.0.jar
4197447   44 -rwxr-xr-x   1 yarn     hadoop      45015 Apr  5 13:40 ./__spark_libs__/hadoop-mapreduce-client-shuffle-2.7.1.jar
4197303  380 -rwxr-xr-x   1 yarn     hadoop     387188 Apr  5 13:40 ./__spark_libs__/parquet-format-2.3.0-incubating.jar
4196934  204 -rwxr-xr-x   1 yarn     hadoop     206035 Apr  5 13:40 ./__spark_libs__/commons-beanutils-core-1.8.0.jar
4197198 4468 -rwxr-xr-x   1 yarn     hadoop    4573750 Apr  5 13:40 ./__spark_libs__/scala-reflect-2.11.8.jar
4197030   68 -rwxr-xr-x   1 yarn     hadoop      66270 Apr  5 13:40 ./__spark_libs__/jersey-container-servlet-core-2.22.2.jar
4197553  296 -rwxr-xr-x   1 yarn     hadoop     302248 Apr  5 13:40 ./__spark_libs__/antlr4-runtime-4.5.3.jar
4197264   44 -rwxr-xr-x   1 yarn     hadoop      41070 Apr  5 13:40 ./__spark_libs__/spark-unsafe_2.11-2.0.1.jar
4197426   64 -rwxr-xr-x   1 yarn     hadoop      62050 Apr  5 13:40 ./__spark_libs__/commons-logging-1.1.3.jar
4197024  576 -rwxr-xr-x   1 yarn     hadoop     589462 Apr  5 13:40 ./__spark_libs__/json4s-core_2.11-3.2.11.jar
4196962   40 -rwxr-xr-x   1 yarn     hadoop      40817 Apr  5 13:40 ./__spark_libs__/hive-cli-1.2.1.spark2.jar
4197378 1768 -rwxr-xr-x   1 yarn     hadoop    1809447 Apr  5 13:40 ./__spark_libs__/datanucleus-rdbms-3.2.9.jar
4197482  468 -rwxr-xr-x   1 yarn     hadoop     477970 Apr  5 13:40 ./__spark_libs__/lift-json_2.11-2.6.3.jar
4197429  512 -rwxr-xr-x   1 yarn     hadoop     521157 Apr  5 13:40 ./__spark_libs__/mail-1.4.7.jar
4196988   96 -rwxr-xr-x   1 yarn     hadoop      96221 Apr  5 13:40 ./__spark_libs__/commons-pool-1.5.4.jar
4197204   72 -rwxr-xr-x   1 yarn     hadoop      72733 Apr  5 13:40 ./__spark_libs__/jersey-media-jaxb-2.22.2.jar
4197452   84 -rwxr-xr-x   1 yarn     hadoop      82421 Apr  5 13:40 ./__spark_libs__/json4s-ast_2.11-3.2.11.jar
4197123  764 -rwxr-xr-x   1 yarn     hadoop     780664 Apr  5 13:40 ./__spark_libs__/jackson-mapper-asl-1.9.13.jar
4197393  352 -rwxr-xr-x   1 yarn     hadoop     358390 Apr  5 13:40 ./__spark_libs__/kryo-shaded-3.0.3.jar
4197371 2304 -rwxr-xr-x   1 yarn     hadoop    2355465 Apr  5 13:40 ./__spark_libs__/spark-network-common_2.11-2.0.1.jar
4197338  200 -rwxr-xr-x   1 yarn     hadoop     201124 Apr  5 13:40 ./__spark_libs__/jdo-api-3.0.1.jar
4197232   20 -rwxr-xr-x   1 yarn     hadoop      17385 Apr  5 13:40 ./__spark_libs__/hadoop-annotations-2.7.1.jar
4197455 1972 -rwxr-xr-x   1 yarn     hadoop    2015514 Apr  5 13:40 ./__spark_libs__/hadoop-yarn-api-2.7.1.jar
4197146 1592 -rwxr-xr-x   1 yarn     hadoop    1627065 Apr  5 13:40 ./__spark_libs__/jersey-bundle-1.19.1.jar
4197186  436 -rwxr-xr-x   1 yarn     hadoop     442406 Apr  5 13:40 ./__spark_libs__/calcite-linq4j-1.2.0-incubating.jar
4197153  208 -rwxr-xr-x   1 yarn     hadoop     209622 Apr  5 13:40 ./__spark_libs__/parquet-hadoop-1.7.0.jar
4197244  332 -rwxr-xr-x   1 yarn     hadoop     339666 Apr  5 13:40 ./__spark_libs__/datanucleus-api-jdo-3.2.6.jar
4197072 1996 -rwxr-xr-x   1 yarn     hadoop    2041628 Apr  5 13:40 ./__spark_libs__/jets3t-0.9.3.jar
4328650    8 -rwxr-xr-x   1 yarn     hadoop       5310 Mar 26 01:25 ./__spark_libs__/pmml-schema-1.2.15.jar
4197311   72 -rwxr-xr-x   1 yarn     hadoop      70688 Apr  5 13:40 ./__spark_libs__/hadoop-auth-2.7.1.jar
4197327  220 -rwxr-xr-x   1 yarn     hadoop     223573 Apr  5 13:40 ./__spark_libs__/chill_2.11-0.8.0.jar
4197279   48 -rwxr-xr-x   1 yarn     hadoop      48720 Apr  5 13:40 ./__spark_libs__/snappy-0.2.jar
4196996   64 -rwxr-xr-x   1 yarn     hadoop      63777 Apr  5 13:40 ./__spark_libs__/validation-api-1.1.0.Final.jar
4197339  300 -rwxr-xr-x   1 yarn     hadoop     305001 Apr  5 13:40 ./__spark_libs__/commons-httpclient-3.1.jar
4197549  172 -rwxr-xr-x   1 yarn     hadoop     174351 Apr  5 13:40 ./__spark_libs__/stream-2.7.0.jar
4197383  616 -rwxr-xr-x   1 yarn     hadoop     627814 Apr  5 13:40 ./__spark_libs__/joda-time-2.9.3.jar
4196937 8068 -rwxr-xr-x   1 yarn     hadoop    8260573 Apr  5 13:40 ./__spark_libs__/hadoop-hdfs-2.7.1.jar
4197234 1204 -rwxr-xr-x   1 yarn     hadoop    1230201 Apr  5 13:40 ./__spark_libs__/netty-3.8.0.Final.jar
4328638 2008 -rwxr-xr-x   1 yarn     hadoop    2054931 Mar 26 01:25 ./__spark_libs__/netty-all-4.0.29.Final.jar
4197300  480 -rwxr-xr-x   1 yarn     hadoop     489884 Apr  5 13:40 ./__spark_libs__/log4j-1.2.17.jar
4197080  176 -rwxr-xr-x   1 yarn     hadoop     178947 Apr  5 13:40 ./__spark_libs__/hk2-api-2.4.0-b34.jar
4197310  100 -rwxr-xr-x   1 yarn     hadoop     100680 Apr  5 13:40 ./__spark_libs__/hive-jdbc-1.2.1.spark2.jar
4197174  528 -rwxr-xr-x   1 yarn     hadoop     539912 Apr  5 13:40 ./__spark_libs__/jetty-6.1.26.jar
4197347    8 -rwxr-xr-x   1 yarn     hadoop       5711 Apr  5 13:40 ./__spark_libs__/minlog-1.3.0.jar
4196926 1512 -rwxr-xr-x   1 yarn     hadoop    1544875 Apr  5 13:40 ./__spark_libs__/hadoop-mapreduce-client-core-2.7.1.jar
4197324  576 -rwxr-xr-x   1 yarn     hadoop     588337 Apr  5 13:40 ./__spark_libs__/commons-collections-3.2.2.jar
4197222  104 -rwxr-xr-x   1 yarn     hadoop     105134 Apr  5 13:40 ./__spark_libs__/jaxb-api-2.2.2.jar
4197094   20 -rwxr-xr-x   1 yarn     hadoop      18098 Apr  5 13:40 ./__spark_libs__/jersey-container-servlet-2.22.2.jar
4197437   96 -rwxr-xr-x   1 yarn     hadoop      94672 Apr  5 13:40 ./__spark_libs__/xz-1.0.jar
4197532 1024 -rwxr-xr-x   1 yarn     hadoop    1044511 Apr  5 13:40 ./__spark_libs__/spark-hive_2.11-2.0.1.jar
4197027  176 -rwxr-xr-x   1 yarn     hadoop     177832 Apr  5 13:40 ./__spark_libs__/spark-mllib-local_2.11-2.0.1.jar
4197210  524 -rwxr-xr-x   1 yarn     hadoop     533455 Apr  5 13:40 ./__spark_libs__/protobuf-java-2.5.0.jar
4194622  284 -rwxr-xr-x   1 yarn     hadoop     290506 Apr  5 13:40 ./__spark_libs__/univocity-parsers-2.1.1.jar
4196994   44 -rwxr-xr-x   1 yarn     hadoop      41755 Apr  5 13:40 ./__spark_libs__/objenesis-2.1.jar
4197496   28 -rwxr-xr-x   1 yarn     hadoop      26514 Apr  5 13:40 ./__spark_libs__/stax-api-1.0.1.jar
4197218  184 -rwxr-xr-x   1 yarn     hadoop     185140 Apr  5 13:40 ./__spark_libs__/commons-io-2.4.jar
4197351   28 -rwxr-xr-x   1 yarn     hadoop      26366 Apr  5 13:40 ./__spark_libs__/javax.annotation-api-1.2.jar
4197228   88 -rwxr-xr-x   1 yarn     hadoop      86811 Apr  5 13:40 ./__spark_libs__/spire-macros_2.11-0.7.4.jar
4197128   68 -rwxr-xr-x   1 yarn     hadoop      65653 Apr  5 13:40 ./__spark_libs__/spark-launcher_2.11-2.0.1.jar
4197165 11232 -rwxr-xr-x   1 yarn     hadoop   11498852 Apr  5 13:40 ./__spark_libs__/hive-exec-1.2.1.spark2.jar
4197050   20 -rwxr-xr-x   1 yarn     hadoop      16993 Apr  5 13:40 ./__spark_libs__/JavaEWAH-0.3.2.jar
4197075  160 -rwxr-xr-x   1 yarn     hadoop     160519 Apr  5 13:40 ./__spark_libs__/commons-dbcp-1.4.jar
4197361 6144 -rwxr-xr-x   1 yarn     hadoop    6290315 Apr  5 13:40 ./__spark_libs__/spark-sql_2.11-2.0.1.jar
4197150    4 -rwxr-xr-x   1 yarn     hadoop       2497 Apr  5 13:40 ./__spark_libs__/javax.inject-1.jar
4197535  400 -rwxr-xr-x   1 yarn     hadoop     409467 Apr  5 13:40 ./__spark_libs__/mx4j-3.0.2.jar
4197215  164 -rwxr-xr-x   1 yarn     hadoop     164368 Apr  5 13:40 ./__spark_libs__/antlr-runtime-3.4.jar
4197107  164 -rwxr-xr-x   1 yarn     hadoop     164422 Apr  5 13:40 ./__spark_libs__/core-1.1.2.jar
4196976   16 -rwxr-xr-x   1 yarn     hadoop      15305 Apr  5 13:40 ./__spark_libs__/spark-tags_2.11-2.0.1.jar
4197333  232 -rwxr-xr-x   1 yarn     hadoop     236880 Apr  5 13:40 ./__spark_libs__/lz4-1.3.0.jar
4197020  188 -rwxr-xr-x   1 yarn     hadoop     188671 Apr  5 13:40 ./__spark_libs__/commons-beanutils-1.7.0.jar
4197052  188 -rwxr-xr-x   1 yarn     hadoop     190432 Apr  5 13:40 ./__spark_libs__/gson-2.2.4.jar
4197179  192 -rwxr-xr-x   1 yarn     hadoop     192993 Apr  5 13:40 ./__spark_libs__/avro-ipc-1.7.7.jar
4197125 13136 -rwxr-xr-x   1 yarn     hadoop   13448966 Apr  5 13:40 ./__spark_libs__/breeze_2.11-0.11.2.jar
4197060   64 -rwxr-xr-x   1 yarn     hadoop      63316 Apr  5 13:40 ./__spark_libs__/spark-repl_2.11-2.0.1.jar
4197002 2044 -rwxr-xr-x   1 yarn     hadoop    2090370 Apr  5 13:40 ./__spark_libs__/spark-streaming_2.11-2.0.1.jar
4197469   40 -rwxr-xr-x   1 yarn     hadoop      38134 Apr  5 13:40 ./__spark_libs__/hadoop-mapreduce-client-jobclient-2.7.1.jar
4197102   12 -rwxr-xr-x   1 yarn     hadoop      12131 Apr  5 13:40 ./__spark_libs__/jpam-1.1.jar
4197240 5380 -rwxr-xr-x   1 yarn     hadoop    5505200 Apr  5 13:40 ./__spark_libs__/hive-metastore-1.2.1.spark2.jar
4197121  656 -rwxr-xr-x   1 yarn     hadoop     669589 Apr  5 13:40 ./__spark_libs__/spark-yarn_2.11-2.0.1.jar
4197530   24 -rwxr-xr-x   1 yarn     hadoop      21243 Apr  5 13:40 ./__spark_libs__/parquet-generator-1.7.0.jar
4197039   40 -rwxr-xr-x   1 yarn     hadoop      39280 Apr  5 13:40 ./__spark_libs__/metrics-jvm-3.1.2.jar
4196928  180 -rwxr-xr-x   1 yarn     hadoop     180736 Apr  5 13:40 ./__spark_libs__/avro-mapred-1.7.7-hadoop2.jar
4197201 6716 -rwxr-xr-x   1 yarn     hadoop    6873892 Apr  5 13:40 ./__spark_libs__/spark-catalyst_2.11-2.0.1.jar
4328641  164 -rwxr-xr-x   1 yarn     hadoop     167421 Mar 26 01:25 ./__spark_libs__/jersey-client-2.22.2.jar
4197250  280 -rwxr-xr-x   1 yarn     hadoop     284220 Apr  5 13:40 ./__spark_libs__/commons-lang-2.6.jar
4197408   20 -rwxr-xr-x   1 yarn     hadoop      18482 Apr  5 13:40 ./__spark_libs__/eigenbase-properties-1.1.5.jar
4197036 1988 -rwxr-xr-x   1 yarn     hadoop    2035066 Apr  5 13:40 ./__spark_libs__/commons-math3-3.4.1.jar
4197255  100 -rwxr-xr-x   1 yarn     hadoop     100636 Apr  5 13:40 ./__spark_libs__/jsp-api-2.1.jar
4197443   28 -rwxr-xr-x   1 yarn     hadoop      27084 Apr  5 13:40 ./__spark_libs__/jackson-xc-1.9.13.jar
4197045   32 -rwxr-xr-x   1 yarn     hadoop      29555 Apr  5 13:40 ./__spark_libs__/paranamer-2.3.jar
4197237   68 -rwxr-xr-x   1 yarn     hadoop      68866 Apr  5 13:40 ./__spark_libs__/curator-client-2.6.0.jar
4197389   32 -rwxr-xr-x   1 yarn     hadoop      30595 Apr  5 13:40 ./__spark_libs__/commons-compiler-2.7.6.jar
4197170   20 -rwxr-xr-x   1 yarn     hadoop      17008 Apr  5 13:40 ./__spark_libs__/base64-2.3.8.jar
4197354 11452 -rwxr-xr-x   1 yarn     hadoop   11723537 Apr  5 13:40 ./__spark_libs__/spark-core_2.11-2.0.1.jar
4197230 1248 -rwxr-xr-x   1 yarn     hadoop    1277883 Apr  5 13:40 ./__spark_libs__/mesos-0.21.1-shaded-protobuf.jar
4197329  676 -rwxr-xr-x   1 yarn     hadoop     691479 Apr  5 13:40 ./__spark_libs__/apacheds-kerberos-codec-2.0.0-M15.jar
4197498   16 -rwxr-xr-x   1 yarn     hadoop      15071 Apr  5 13:40 ./__spark_libs__/jta-1.1.jar
4197177  180 -rwxr-xr-x   1 yarn     hadoop     181271 Apr  5 13:40 ./__spark_libs__/hk2-locator-2.4.0-b34.jar
4197109   40 -rwxr-xr-x   1 yarn     hadoop      40509 Apr  5 13:40 ./__spark_libs__/slf4j-api-1.7.16.jar
4196981    8 -rwxr-xr-x   1 yarn     hadoop       5950 Apr  5 13:40 ./__spark_libs__/javax.inject-2.4.0-b34.jar
4197321  356 -rwxr-xr-x   1 yarn     hadoop     363908 Apr  5 13:40 ./__spark_libs__/hadoop-yarn-server-common-2.7.1.jar
4197369  136 -rwxr-xr-x   1 yarn     hadoop     138464 Apr  5 13:40 ./__spark_libs__/hive-beeline-1.2.1.spark2.jar
4197212  436 -rwxr-xr-x   1 yarn     hadoop     445288 Apr  5 13:40 ./__spark_libs__/antlr-2.7.7.jar
4197431  896 -rwxr-xr-x   1 yarn     hadoop     917052 Apr  5 13:40 ./__spark_libs__/parquet-column-1.7.0.jar
4196941   44 -rwxr-xr-x   1 yarn     hadoop      44925 Apr  5 13:40 ./__spark_libs__/apacheds-i18n-2.0.0-M15.jar
4197306  404 -rwxr-xr-x   1 yarn     hadoop     412739 Apr  5 13:40 ./__spark_libs__/commons-lang3-3.3.2.jar
4197394   12 -rwxr-xr-x   1 yarn     hadoop      10023 Apr  5 13:40 ./__spark_libs__/java-xmlbuilder-1.0.jar
4196964   48 -rwxr-xr-x   1 yarn     hadoop      46983 Apr  5 13:40 ./__spark_libs__/jackson-annotations-2.6.5.jar
4197168    8 -rwxr-xr-x   1 yarn     hadoop       4467 Apr  5 13:40 ./__spark_libs__/aopalliance-1.0.jar
4197544   44 -rwxr-xr-x   1 yarn     hadoop      41123 Apr  5 13:40 ./__spark_libs__/commons-cli-1.2.jar
4197405  700 -rwxr-xr-x   1 yarn     hadoop     714194 Apr  5 13:40 ./__spark_libs__/javassist-3.18.1-GA.jar
4196957   16 -rwxr-xr-x   1 yarn     hadoop      15827 Apr  5 13:40 ./__spark_libs__/metrics-json-3.1.2.jar
4197207   32 -rwxr-xr-x   1 yarn     hadoop      29540 Apr  5 13:40 ./__spark_libs__/spark-sketch_2.11-2.0.1.jar
4197423  184 -rwxr-xr-x   1 yarn     hadoop     185245 Apr  5 13:40 ./__spark_libs__/curator-framework-2.6.0.jar
4197345    8 -rwxr-xr-x   1 yarn     hadoop       4596 Apr  5 13:40 ./__spark_libs__/jul-to-slf4j-1.7.16.jar
4197462 3352 -rwxr-xr-x   1 yarn     hadoop    3431544 Apr  5 13:40 ./__spark_libs__/hadoop-common-2.7.1.jar
4197468 1168 -rwxr-xr-x   1 yarn     hadoop    1194003 Apr  5 13:40 ./__spark_libs__/arpack_combined_all-0.1.jar
4328647  736 -rwxr-xr-x   1 yarn     hadoop     753012 Mar 26 01:25 ./__spark_libs__/hadoop-mapreduce-client-common-2.7.1.jar
4197258  208 -rwxr-xr-x   1 yarn     hadoop     212453 Apr  5 13:40 ./__spark_libs__/commons-net-2.2.jar
4197528  120 -rwxr-xr-x   1 yarn     hadoop     118973 Apr  5 13:40 ./__spark_libs__/hk2-utils-2.4.0-b34.jar
4197415  320 -rwxr-xr-x   1 yarn     hadoop     326724 Apr  5 13:40 ./__spark_libs__/httpcore-4.4.4.jar
4197499  388 -rwxr-xr-x   1 yarn     hadoop     395195 Apr  5 13:40 ./__spark_libs__/javolution-5.5.1.jar
4328645  508 -rwxr-xr-x   1 yarn     hadoop     516127 Mar 26 01:25 ./__spark_libs__/hadoop-mapreduce-client-app-2.7.1.jar
4197484  420 -rwxr-xr-x   1 yarn     hadoop     427780 Apr  5 13:40 ./__spark_libs__/jodd-core-3.5.2.jar
4197489  140 -rwxr-xr-x   1 yarn     hadoop     142631 Apr  5 13:40 ./__spark_libs__/hadoop-yarn-client-2.7.1.jar
4197195  256 -rwxr-xr-x   1 yarn     hadoop     258370 Apr  5 13:40 ./__spark_libs__/calcite-avatica-1.2.0-incubating.jar
4197294  116 -rwxr-xr-x   1 yarn     hadoop     114913 Apr  5 13:40 ./__spark_libs__/py4j-0.10.3.jar
4197015   40 -rwxr-xr-x   1 yarn     hadoop      40341 Apr  5 13:40 ./__spark_libs__/json4s-jackson_2.11-3.2.11.jar
4197286   24 -rwxr-xr-x   1 yarn     hadoop      23346 Apr  5 13:40 ./__spark_libs__/stax-api-1.0-2.jar
4197156   80 -rwxr-xr-x   1 yarn     hadoop      79845 Apr  5 13:40 ./__spark_libs__/compress-lzf-1.0.3.jar
4197269  748 -rwxr-xr-x   1 yarn     hadoop     764569 Apr  5 13:40 ./__spark_libs__/jtransforms-2.4.0.jar
4196923  112 -rwxr-xr-x   1 yarn     hadoop     112558 Apr  5 13:40 ./__spark_libs__/metrics-core-3.1.2.jar
4197005 15128 -rwxr-xr-x   1 yarn     hadoop   15487351 Apr  5 13:40 ./__spark_libs__/scala-compiler-2.11.8.jar
4197420   24 -rwxr-xr-x   1 yarn     hadoop      20852 Apr  5 13:40 ./__spark_libs__/metrics-graphite-3.1.2.jar
4197291 1616 -rwxr-xr-x   1 yarn     hadoop    1654097 Apr  5 13:40 ./__spark_libs__/hadoop-yarn-common-2.7.1.jar
4197224 2780 -rwxr-xr-x   1 yarn     hadoop    2842667 Apr  5 13:40 ./__spark_libs__/bcprov-jdk15on-1.51.jar
4197441   20 -rwxr-xr-x   1 yarn     hadoop      18336 Apr  5 13:40 ./__spark_libs__/jackson-jaxrs-1.9.13.jar
4197088  136 -rwxr-xr-x   1 yarn     hadoop     135552 Apr  5 13:40 ./__spark_libs__/breeze-macros_2.11-0.11.2.jar
4197556  308 -rwxr-xr-x   1 yarn     hadoop     313686 Apr  5 13:40 ./__spark_libs__/libfb303-0.9.2.jar
4197477 2140 -rwxr-xr-x   1 yarn     hadoop    2189117 Apr  5 13:40 ./__spark_libs__/guava-14.0.1.jar
4196930  224 -rwxr-xr-x   1 yarn     hadoop     227712 Apr  5 13:40 ./__spark_libs__/libthrift-0.9.2.jar
4196974  636 -rwxr-xr-x   1 yarn     hadoop     648678 Apr  5 13:40 ./__spark_libs__/scala-xml_2.11-1.0.2.jar
4197058  256 -rwxr-xr-x   1 yarn     hadoop     258876 Apr  5 13:40 ./__spark_libs__/jackson-core-2.6.5.jar
4197445  668 -rwxr-xr-x   1 yarn     hadoop     680106 Apr  5 13:40 ./__spark_libs__/spark-graphx_2.11-2.0.1.jar
4197067  244 -rwxr-xr-x   1 yarn     hadoop     248171 Apr  5 13:40 ./__spark_libs__/curator-recipes-2.6.0.jar
4197115  280 -rwxr-xr-x   1 yarn     hadoop     284184 Apr  5 13:40 ./__spark_libs__/commons-codec-1.10.jar
4197098  600 -rwxr-xr-x   1 yarn     hadoop     613299 Apr  5 13:40 ./__spark_libs__/janino-2.7.8.jar
4196939  176 -rwxr-xr-x   1 yarn     hadoop     177131 Apr  5 13:40 ./__spark_libs__/jetty-util-6.1.26.jar
4197375 3152 -rwxr-xr-x   1 yarn     hadoop    3224708 Apr  5 13:40 ./__spark_libs__/derby-10.12.1.1.jar
4196978   96 -rwxr-xr-x   1 yarn     hadoop      95806 Apr  5 13:40 ./__spark_libs__/javax.servlet-api-3.1.0.jar
4197485   68 -rwxr-xr-x   1 yarn     hadoop      69409 Apr  5 13:40 ./__spark_libs__/activation-1.1.1.jar
4197538 1772 -rwxr-xr-x   1 yarn     hadoop    1814309 Apr  5 13:40 ./__spark_libs__/spark-hive-thriftserver_2.11-2.0.1.jar
4197246   16 -rwxr-xr-x   1 yarn     hadoop      14766 Apr  5 13:40 ./__spark_libs__/aopalliance-repackaged-2.4.0-b34.jar
4197013   20 -rwxr-xr-x   1 yarn     hadoop      16430 Apr  5 13:40 ./__spark_libs__/jcl-over-slf4j-1.7.16.jar
4196943  776 -rwxr-xr-x   1 yarn     hadoop     792964 Apr  5 13:40 ./__spark_libs__/zookeeper-3.4.6.jar
4197143   52 -rwxr-xr-x   1 yarn     hadoop      50619 Apr  5 13:40 ./__spark_libs__/chill-java-0.8.0.jar
4197085  144 -rwxr-xr-x   1 yarn     hadoop     143602 Apr  5 13:40 ./__spark_libs__/commons-digester-1.8.jar
4196986   20 -rwxr-xr-x   1 yarn     hadoop      19827 Apr  5 13:40 ./__spark_libs__/opencsv-2.3.jar
4196960   56 -rwxr-xr-x   1 yarn     hadoop      55511 Apr  5 13:40 ./__spark_libs__/spark-network-shuffle_2.11-2.0.1.jar
4197296  292 -rwxr-xr-x   1 yarn     hadoop     298829 Apr  5 13:40 ./__spark_libs__/commons-configuration-1.6.jar
4197318   24 -rwxr-xr-x   1 yarn     hadoop      21575 Apr  5 13:40 ./__spark_libs__/parquet-common-1.7.0.jar
4197192  236 -rwxr-xr-x   1 yarn     hadoop     241367 Apr  5 13:40 ./__spark_libs__/commons-compress-1.4.1.jar
4196971    4 -rwxr-xr-x   1 yarn     hadoop       2545 Apr  5 13:40 ./__spark_libs__/hadoop-client-2.7.1.jar
4196969   48 -rwxr-xr-x   1 yarn     hadoop      45944 Apr  5 13:40 ./__spark_libs__/json-20090211.jar
4197401   64 -rwxr-xr-x   1 yarn     hadoop      65261 Apr  5 13:40 ./__spark_libs__/oro-2.0.8.jar
4197041 2732 -rwxr-xr-x   1 yarn     hadoop    2796935 Apr  5 13:40 ./__spark_libs__/parquet-hadoop-bundle-1.6.0.jar
4196984 1444 -rwxr-xr-x   1 yarn     hadoop    1475955 Apr  5 13:40 ./__spark_libs__/htrace-core-3.1.0-incubating.jar
4197263  416 -rwxr-xr-x   1 yarn     hadoop     423753 Apr  5 13:40 ./__spark_libs__/scala-parser-combinators_2.11-1.0.4.jar
4197141  112 -rwxr-xr-x   1 yarn     hadoop     110600 Apr  5 13:40 ./__spark_libs__/bonecp-0.8.0.RELEASE.jar
4197460  212 -rwxr-xr-x   1 yarn     hadoop     213911 Apr  5 13:40 ./__spark_libs__/jline-2.12.1.jar
4197249  148 -rwxr-xr-x   1 yarn     hadoop     148627 Apr  5 13:40 ./__spark_libs__/stringtemplate-3.2.1.jar
4197403 1032 -rwxr-xr-x   1 yarn     hadoop    1056168 Apr  5 13:40 ./__spark_libs__/snappy-java-1.1.2.6.jar
4196945   92 -rwxr-xr-x   1 yarn     hadoop      93210 Apr  5 13:40 ./__spark_libs__/super-csv-2.2.0.jar
4197416   20 -rwxr-xr-x   1 yarn     hadoop      20235 Apr  5 13:40 ./__spark_libs__/osgi-resource-locator-1.0.1.jar
4197282  952 -rwxr-xr-x   1 yarn     hadoop     971310 Apr  5 13:40 ./__spark_libs__/jersey-guava-2.22.2.jar
4197559 7108 -rwxr-xr-x   1 yarn     hadoop    7276083 Apr  5 13:40 ./__spark_libs__/spire_2.11-0.7.4.jar
4197273  684 -rwxr-xr-x   1 yarn     hadoop     698375 Apr  5 13:40 ./__spark_libs__/jersey-common-2.22.2.jar
4196948 3440 -rwxr-xr-x   1 yarn     hadoop    3519262 Apr  5 13:40 ./__spark_libs__/calcite-core-1.2.0-incubating.jar
4197337 5712 -rwxr-xr-x   1 yarn     hadoop    5847591 Apr  5 13:40 ./__spark_libs__/spark-mllib_2.11-2.0.1.jar
4197138 1256 -rwxr-xr-x   1 yarn     hadoop    1282424 Apr  5 13:40 ./__spark_libs__/ivy-2.4.0.jar
4197366   80 -rwxr-xr-x   1 yarn     hadoop      79912 Apr  5 13:40 ./__spark_libs__/api-util-1.0.0-M20.jar
4197382   36 -rwxr-xr-x   1 yarn     hadoop      33015 Apr  5 13:40 ./__spark_libs__/jsr305-1.3.9.jar
4197051  532 -rwxr-xr-x   1 yarn     hadoop     540852 Apr  5 13:40 ./__spark_libs__/mysql-connector-java-5.0.8-bin.jar
4197018  640 -rwxr-xr-x   1 yarn     hadoop     654216 Apr  5 13:40 ./__spark_libs__/pmml-model-1.2.15.jar
4197474  440 -rwxr-xr-x   1 yarn     hadoop     448794 Apr  5 13:40 ./__spark_libs__/apache-log4j-extras-1.2.17.jar
4197158  232 -rwxr-xr-x   1 yarn     hadoop     236660 Apr  5 13:40 ./__spark_libs__/ST4-4.0.4.jar
4197450  504 -rwxr-xr-x   1 yarn     hadoop     515604 Apr  5 13:40 ./__spark_libs__/jackson-module-scala_2.11-2.6.5.jar
4197276   20 -rwxr-xr-x   1 yarn     hadoop      16560 Apr  5 13:40 ./__spark_libs__/api-asn1-api-1.0.0-M20.jar
4196951   64 -rwxr-xr-x   1 yarn     hadoop      65012 Apr  5 13:40 ./__spark_libs__/guice-servlet-3.0.jar
4197056  228 -rwxr-xr-x   1 yarn     hadoop     232248 Apr  5 13:40 ./__spark_libs__/jackson-core-asl-1.9.13.jar
4197189 5612 -rwxr-xr-x   1 yarn     hadoop    5744974 Apr  5 13:40 ./__spark_libs__/scala-library-2.11.8.jar
4197032  144 -rwxr-xr-x   1 yarn     hadoop     144660 Apr  5 13:40 ./__spark_libs__/xbean-asm5-shaded-4.4.jar
4197009  428 -rwxr-xr-x   1 yarn     hadoop     436303 Apr  5 13:40 ./__spark_libs__/avro-1.7.7.jar
4197183  720 -rwxr-xr-x   1 yarn     hadoop     736658 Apr  5 13:40 ./__spark_libs__/httpclient-4.5.2.jar
4197502  200 -rwxr-xr-x   1 yarn     hadoop     201928 Apr  5 13:40 ./__spark_libs__/RoaringBitmap-0.5.11.jar
4197135   12 -rwxr-xr-x   1 yarn     hadoop       9939 Apr  5 13:40 ./__spark_libs__/slf4j-log4j12-1.7.16.jar
4197069   44 -rwxr-xr-x   1 yarn     hadoop      41263 Apr  5 13:40 ./__spark_libs__/jackson-module-paranamer-2.6.5.jar
4197435 1024 -rwxr-xr-x   1 yarn     hadoop    1048110 Apr  5 13:40 ./__spark_libs__/parquet-jackson-1.7.0.jar
4197078  788 -rwxr-xr-x   1 yarn     hadoop     802818 Apr  5 13:40 ./__spark_libs__/scalap-2.11.8.jar
4197288  116 -rwxr-xr-x   1 yarn     hadoop     115534 Apr  5 13:40 ./__spark_libs__/javax.ws.rs-api-2.0.1.jar
4197480 1144 -rwxr-xr-x   1 yarn     hadoop    1171380 Apr  5 13:40 ./__spark_libs__/jackson-databind-2.6.5.jar
4197161 1024 -rwxr-xr-x   1 yarn     hadoop    1045744 Apr  5 13:40 ./__spark_libs__/leveldbjni-all-1.8.jar
4197388   16 -rwxr-xr-x   1 yarn     hadoop      15010 Apr  5 13:40 ./__spark_libs__/xmlenc-0.52.jar
4197131   32 -rwxr-xr-x   1 yarn     hadoop      32145 Apr  5 13:40 ./__spark_libs__/hadoop-yarn-server-web-proxy-2.7.1.jar
broken symlinks(find -L . -maxdepth 5 -type l -ls):
End of LogType:directory.info

LogType:launch_container.sh
Log Upload Time:Tue May 16 11:12:18 +0700 2017
LogLength:75763
Log Contents:
#!/bin/bash

export SPARK_YARN_STAGING_DIR="hdfs://c408.hadoop.gda.lo:8020/user/fairy/.sparkStaging/application_1494395298335_303847"
export LOCAL_DIRS="/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847"
export HADOOP_CONF_DIR="/usr/hdp/current/hadoop-client/conf"
export NM_HTTP_PORT="8042"
export JAVA_HOME="/usr/jdk64/jdk1.8.0_92/"
export LOG_DIRS="/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000005"
export NM_AUX_SERVICE_mapreduce_shuffle="AAA0+gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=
"
export NM_PORT="45454"
export USER="fairy"
export HADOOP_YARN_HOME="/usr/hdp/current/hadoop-yarn-nodemanager"
export CLASSPATH="$PWD:$PWD/__spark_conf__:$PWD/__spark_libs__/*:$HADOOP_CONF_DIR:/usr/hdp/current/hadoop-client/*:/usr/hdp/current/hadoop-client/lib/*:/usr/hdp/current/hadoop-hdfs-client/*:/usr/hdp/current/hadoop-hdfs-client/lib/*:/usr/hdp/current/hadoop-yarn-client/*:/usr/hdp/current/hadoop-yarn-client/lib/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.4.2.0-258/hadoop/lib/hadoop-lzo-0.6.0.2.4.2.0-258.jar:/etc/hadoop/conf/secure"
export SPARK_LOG_URL_STDOUT="http://c414.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_01_000005/fairy/stdout?start=-4096"
export NM_HOST="c414.hadoop.gda.lo"
export SPARK_YARN_MODE="true"
export HADOOP_TOKEN_FILE_LOCATION="/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_01_000005/container_tokens"
export NM_AUX_SERVICE_spark_shuffle=""
export SPARK_USER="fairy"
export SPARK_LOG_URL_STDERR="http://c414.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_01_000005/fairy/stderr?start=-4096"
export LOGNAME="fairy"
export JVM_PID="$$"
export PWD="/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_01_000005"
export HOME="/home/"
export CONTAINER_ID="container_e81_1494395298335_303847_01_000005"
export MALLOC_ARENA_MAX="4"
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91366/jersey-media-jaxb-2.22.2.jar" "__spark_libs__/jersey-media-jaxb-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91406/commons-collections-3.2.2.jar" "__spark_libs__/commons-collections-3.2.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91450/hadoop-yarn-api-2.7.1.jar" "__spark_libs__/hadoop-yarn-api-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91454/arpack_combined_all-0.1.jar" "__spark_libs__/arpack_combined_all-0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91379/datanucleus-api-jdo-3.2.6.jar" "__spark_libs__/datanucleus-api-jdo-3.2.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91369/antlr-2.7.7.jar" "__spark_libs__/antlr-2.7.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91296/objenesis-2.1.jar" "__spark_libs__/objenesis-2.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91293/htrace-core-3.1.0-incubating.jar" "__spark_libs__/htrace-core-3.1.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91428/commons-compiler-2.7.6.jar" "__spark_libs__/commons-compiler-2.7.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91420/api-util-1.0.0-M20.jar" "__spark_libs__/api-util-1.0.0-M20.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91361/scala-library-2.11.8.jar" "__spark_libs__/scala-library-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91393/stax-api-1.0-2.jar" "__spark_libs__/stax-api-1.0-2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91334/core-1.1.2.jar" "__spark_libs__/core-1.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91463/javolution-5.5.1.jar" "__spark_libs__/javolution-5.5.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91345/bonecp-0.8.0.RELEASE.jar" "__spark_libs__/bonecp-0.8.0.RELEASE.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91451/jline-2.12.1.jar" "__spark_libs__/jline-2.12.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91424/datanucleus-rdbms-3.2.9.jar" "__spark_libs__/datanucleus-rdbms-3.2.9.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/85716/hadoop-mapreduce-client-app-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-app-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91405/hadoop-yarn-server-common-2.7.1.jar" "__spark_libs__/hadoop-yarn-server-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91467/spark-hive_2.11-2.0.1.jar" "__spark_libs__/spark-hive_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91283/apacheds-i18n-2.0.0-M15.jar" "__spark_libs__/apacheds-i18n-2.0.0-M15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91312/parquet-hadoop-bundle-1.6.0.jar" "__spark_libs__/parquet-hadoop-bundle-1.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91441/mail-1.4.7.jar" "__spark_libs__/mail-1.4.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91461/hadoop-yarn-client-2.7.1.jar" "__spark_libs__/hadoop-yarn-client-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91333/pyrolite-4.9.jar" "__spark_libs__/pyrolite-4.9.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91280/super-csv-2.2.0.jar" "__spark_libs__/super-csv-2.2.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91318/spark-repl_2.11-2.0.1.jar" "__spark_libs__/spark-repl_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91356/jetty-6.1.26.jar" "__spark_libs__/jetty-6.1.26.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91381/commons-lang-2.6.jar" "__spark_libs__/commons-lang-2.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91433/javassist-3.18.1-GA.jar" "__spark_libs__/javassist-3.18.1-GA.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91452/hadoop-common-2.7.1.jar" "__spark_libs__/hadoop-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91279/zookeeper-3.4.6.jar" "__spark_libs__/zookeeper-3.4.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91357/hk2-locator-2.4.0-b34.jar" "__spark_libs__/hk2-locator-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91412/jdo-api-3.0.1.jar" "__spark_libs__/jdo-api-3.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91409/lz4-1.3.0.jar" "__spark_libs__/lz4-1.3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91431/guice-3.0.jar" "__spark_libs__/guice-3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91292/javax.inject-2.4.0-b34.jar" "__spark_libs__/javax.inject-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91394/javax.ws.rs-api-2.0.1.jar" "__spark_libs__/javax.ws.rs-api-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91346/chill-java-0.8.0.jar" "__spark_libs__/chill-java-0.8.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91413/jul-to-slf4j-1.7.16.jar" "__spark_libs__/jul-to-slf4j-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91327/jackson-core-asl-1.9.13.jar" "__spark_libs__/jackson-core-asl-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91460/jodd-core-3.5.2.jar" "__spark_libs__/jodd-core-3.5.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/85714/netty-all-4.0.29.Final.jar" "__spark_libs__/netty-all-4.0.29.Final.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91286/jackson-annotations-2.6.5.jar" "__spark_libs__/jackson-annotations-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91309/xbean-asm5-shaded-4.4.jar" "__spark_libs__/xbean-asm5-shaded-4.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91294/commons-pool-1.5.4.jar" "__spark_libs__/commons-pool-1.5.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91284/metrics-json-3.1.2.jar" "__spark_libs__/metrics-json-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91397/commons-configuration-1.6.jar" "__spark_libs__/commons-configuration-1.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91407/chill_2.11-0.8.0.jar" "__spark_libs__/chill_2.11-0.8.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91288/json-20090211.jar" "__spark_libs__/json-20090211.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91376/netty-3.8.0.Final.jar" "__spark_libs__/netty-3.8.0.Final.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91316/gson-2.2.4.jar" "__spark_libs__/gson-2.2.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91462/jta-1.1.jar" "__spark_libs__/jta-1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91347/jersey-bundle-1.19.1.jar" "__spark_libs__/jersey-bundle-1.19.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91399/parquet-format-2.3.0-incubating.jar" "__spark_libs__/parquet-format-2.3.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91367/spark-sketch_2.11-2.0.1.jar" "__spark_libs__/spark-sketch_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91377/curator-client-2.6.0.jar" "__spark_libs__/curator-client-2.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91343/slf4j-log4j12-1.7.16.jar" "__spark_libs__/slf4j-log4j12-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91423/derby-10.12.1.1.jar" "__spark_libs__/derby-10.12.1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91332/breeze-macros_2.11-0.11.2.jar" "__spark_libs__/breeze-macros_2.11-0.11.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91390/api-asn1-api-1.0.0-M20.jar" "__spark_libs__/api-asn1-api-1.0.0-M20.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91310/commons-math3-3.4.1.jar" "__spark_libs__/commons-math3-3.4.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91340/breeze_2.11-0.11.2.jar" "__spark_libs__/breeze_2.11-0.11.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91471/commons-cli-1.2.jar" "__spark_libs__/commons-cli-1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91439/curator-framework-2.6.0.jar" "__spark_libs__/curator-framework-2.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91458/jackson-databind-2.6.5.jar" "__spark_libs__/jackson-databind-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91476/spire_2.11-0.7.4.jar" "__spark_libs__/spire_2.11-0.7.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91300/scala-compiler-2.11.8.jar" "__spark_libs__/scala-compiler-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91414/minlog-1.3.0.jar" "__spark_libs__/minlog-1.3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91342/hadoop-yarn-server-web-proxy-2.7.1.jar" "__spark_libs__/hadoop-yarn-server-web-proxy-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/85717/hadoop-mapreduce-client-common-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91443/parquet-jackson-1.7.0.jar" "__spark_libs__/parquet-jackson-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91449/json4s-ast_2.11-3.2.11.jar" "__spark_libs__/json4s-ast_2.11-3.2.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91307/spark-mllib-local_2.11-2.0.1.jar" "__spark_libs__/spark-mllib-local_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91330/jersey-container-servlet-2.22.2.jar" "__spark_libs__/jersey-container-servlet-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91465/lift-json_2.11-2.6.3.jar" "__spark_libs__/lift-json_2.11-2.6.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91277/avro-mapred-1.7.7-hadoop2.jar" "__spark_libs__/avro-mapred-1.7.7-hadoop2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91353/hive-exec-1.2.1.spark2.jar" "__spark_libs__/hive-exec-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91392/jersey-guava-2.22.2.jar" "__spark_libs__/jersey-guava-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91411/commons-httpclient-3.1.jar" "__spark_libs__/commons-httpclient-3.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91421/hive-beeline-1.2.1.spark2.jar" "__spark_libs__/hive-beeline-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91416/spark-core_2.11-2.0.1.jar" "__spark_libs__/spark-core_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91426/jsr305-1.3.9.jar" "__spark_libs__/jsr305-1.3.9.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91445/jackson-jaxrs-1.9.13.jar" "__spark_libs__/jackson-jaxrs-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91304/pmml-model-1.2.15.jar" "__spark_libs__/pmml-model-1.2.15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91315/JavaEWAH-0.3.2.jar" "__spark_libs__/JavaEWAH-0.3.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91320/curator-recipes-2.6.0.jar" "__spark_libs__/curator-recipes-2.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91339/jackson-mapper-asl-1.9.13.jar" "__spark_libs__/jackson-mapper-asl-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91351/ST4-4.0.4.jar" "__spark_libs__/ST4-4.0.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91402/hadoop-auth-2.7.1.jar" "__spark_libs__/hadoop-auth-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91419/spark-sql_2.11-2.0.1.jar" "__spark_libs__/spark-sql_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91417/xercesImpl-2.9.1.jar" "__spark_libs__/xercesImpl-2.9.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91418/commons-net-2.2.jar" "__spark_libs__/commons-net-2.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91337/slf4j-api-1.7.16.jar" "__spark_libs__/slf4j-api-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91401/hive-jdbc-1.2.1.spark2.jar" "__spark_libs__/hive-jdbc-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91331/janino-2.7.8.jar" "__spark_libs__/janino-2.7.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91422/spark-network-common_2.11-2.0.1.jar" "__spark_libs__/spark-network-common_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91466/hk2-utils-2.4.0-b34.jar" "__spark_libs__/hk2-utils-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91446/jackson-xc-1.9.13.jar" "__spark_libs__/jackson-xc-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91383/jsp-api-2.1.jar" "__spark_libs__/jsp-api-2.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91370/antlr-runtime-3.4.jar" "__spark_libs__/antlr-runtime-3.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91281/calcite-core-1.2.0-incubating.jar" "__spark_libs__/calcite-core-1.2.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91430/kryo-shaded-3.0.3.jar" "__spark_libs__/kryo-shaded-3.0.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91314/mysql-connector-java-5.0.8-bin.jar" "__spark_libs__/mysql-connector-java-5.0.8-bin.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/85715/jersey-client-2.22.2.jar" "__spark_libs__/jersey-client-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91348/javax.inject-1.jar" "__spark_libs__/javax.inject-1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/85718/pmml-schema-1.2.15.jar" "__spark_libs__/pmml-schema-1.2.15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91354/aopalliance-1.0.jar" "__spark_libs__/aopalliance-1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91313/paranamer-2.3.jar" "__spark_libs__/paranamer-2.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91291/javax.servlet-api-3.1.0.jar" "__spark_libs__/javax.servlet-api-3.1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91285/spark-network-shuffle_2.11-2.0.1.jar" "__spark_libs__/spark-network-shuffle_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91338/spark-yarn_2.11-2.0.1.jar" "__spark_libs__/spark-yarn_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91385/spark-unsafe_2.11-2.0.1.jar" "__spark_libs__/spark-unsafe_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91457/guava-14.0.1.jar" "__spark_libs__/guava-14.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91438/metrics-graphite-3.1.2.jar" "__spark_libs__/metrics-graphite-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91321/jackson-module-paranamer-2.6.5.jar" "__spark_libs__/jackson-module-paranamer-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91350/compress-lzf-1.0.3.jar" "__spark_libs__/compress-lzf-1.0.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91404/parquet-common-1.7.0.jar" "__spark_libs__/parquet-common-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91395/hadoop-yarn-common-2.7.1.jar" "__spark_libs__/hadoop-yarn-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91364/scala-reflect-2.11.8.jar" "__spark_libs__/scala-reflect-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91311/metrics-jvm-3.1.2.jar" "__spark_libs__/metrics-jvm-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91427/xmlenc-0.52.jar" "__spark_libs__/xmlenc-0.52.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91306/json4s-core_2.11-3.2.11.jar" "__spark_libs__/json4s-core_2.11-3.2.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91391/snappy-0.2.jar" "__spark_libs__/snappy-0.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91308/jersey-container-servlet-core-2.22.2.jar" "__spark_libs__/jersey-container-servlet-core-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91273/metrics-core-3.1.2.jar" "__spark_libs__/metrics-core-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91282/guice-servlet-3.0.jar" "__spark_libs__/guice-servlet-3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91365/spark-catalyst_2.11-2.0.1.jar" "__spark_libs__/spark-catalyst_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91299/spark-streaming_2.11-2.0.1.jar" "__spark_libs__/spark-streaming_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91440/commons-logging-1.1.3.jar" "__spark_libs__/commons-logging-1.1.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91274/hadoop-mapreduce-client-core-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-core-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91336/commons-codec-1.10.jar" "__spark_libs__/commons-codec-1.10.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91360/calcite-linq4j-1.2.0-incubating.jar" "__spark_libs__/calcite-linq4j-1.2.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91396/py4j-0.10.3.jar" "__spark_libs__/py4j-0.10.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91374/spire-macros_2.11-0.7.4.jar" "__spark_libs__/spire-macros_2.11-0.7.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91403/commons-lang3-3.3.2.jar" "__spark_libs__/commons-lang3-3.3.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91444/xz-1.0.jar" "__spark_libs__/xz-1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91380/aopalliance-repackaged-2.4.0-b34.jar" "__spark_libs__/aopalliance-repackaged-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91435/snappy-java-1.1.2.6.jar" "__spark_libs__/snappy-java-1.1.2.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/usercache/fairy/filecache/38959/stats-etlr-1.0.jar" "__app__.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91298/hadoop-hdfs-2.7.1.jar" "__spark_libs__/hadoop-hdfs-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91447/hadoop-mapreduce-client-shuffle-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-shuffle-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91453/jackson-module-scala_2.11-2.6.5.jar" "__spark_libs__/jackson-module-scala_2.11-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91324/univocity-parsers-2.1.1.jar" "__spark_libs__/univocity-parsers-2.1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91400/jpam-1.1.jar" "__spark_libs__/jpam-1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91373/bcprov-jdk15on-1.51.jar" "__spark_libs__/bcprov-jdk15on-1.51.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91434/eigenbase-properties-1.1.5.jar" "__spark_libs__/eigenbase-properties-1.1.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91326/scalap-2.11.8.jar" "__spark_libs__/scalap-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91287/hive-cli-1.2.1.spark2.jar" "__spark_libs__/hive-cli-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91389/jersey-common-2.22.2.jar" "__spark_libs__/jersey-common-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91432/oro-2.0.8.jar" "__spark_libs__/oro-2.0.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91448/spark-graphx_2.11-2.0.1.jar" "__spark_libs__/spark-graphx_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91459/activation-1.1.1.jar" "__spark_libs__/activation-1.1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91470/stax-api-1.0.1.jar" "__spark_libs__/stax-api-1.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91319/jersey-server-2.22.2.jar" "__spark_libs__/jersey-server-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91372/jaxb-api-2.2.2.jar" "__spark_libs__/jaxb-api-2.2.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91301/avro-1.7.7.jar" "__spark_libs__/avro-1.7.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91388/jtransforms-2.4.0.jar" "__spark_libs__/jtransforms-2.4.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91473/stream-2.7.0.jar" "__spark_libs__/stream-2.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91386/scala-parser-combinators_2.11-1.0.4.jar" "__spark_libs__/scala-parser-combinators_2.11-1.0.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91302/jcl-over-slf4j-1.7.16.jar" "__spark_libs__/jcl-over-slf4j-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91359/httpclient-4.5.2.jar" "__spark_libs__/httpclient-4.5.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91384/hive-metastore-1.2.1.spark2.jar" "__spark_libs__/hive-metastore-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91436/httpcore-4.4.4.jar" "__spark_libs__/httpcore-4.4.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91368/protobuf-java-2.5.0.jar" "__spark_libs__/protobuf-java-2.5.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91362/commons-compress-1.4.1.jar" "__spark_libs__/commons-compress-1.4.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91275/libthrift-0.9.2.jar" "__spark_libs__/libthrift-0.9.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91352/leveldbjni-all-1.8.jar" "__spark_libs__/leveldbjni-all-1.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91456/apache-log4j-extras-1.2.17.jar" "__spark_libs__/apache-log4j-extras-1.2.17.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91437/osgi-resource-locator-1.0.1.jar" "__spark_libs__/osgi-resource-locator-1.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91455/hadoop-mapreduce-client-jobclient-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-jobclient-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91289/hadoop-client-2.7.1.jar" "__spark_libs__/hadoop-client-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91341/spark-launcher_2.11-2.0.1.jar" "__spark_libs__/spark-launcher_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91375/mesos-0.21.1-shaded-protobuf.jar" "__spark_libs__/mesos-0.21.1-shaded-protobuf.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91329/opencsv-2.3.jar" "__spark_libs__/opencsv-2.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91335/datanucleus-core-3.2.10.jar" "__spark_libs__/datanucleus-core-3.2.10.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91349/parquet-hadoop-1.7.0.jar" "__spark_libs__/parquet-hadoop-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91475/libfb303-0.9.2.jar" "__spark_libs__/libfb303-0.9.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91472/parquet-generator-1.7.0.jar" "__spark_libs__/parquet-generator-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91295/spark-tags_2.11-2.0.1.jar" "__spark_libs__/spark-tags_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91297/validation-api-1.1.0.Final.jar" "__spark_libs__/validation-api-1.1.0.Final.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91387/parquet-encoding-1.7.0.jar" "__spark_libs__/parquet-encoding-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91278/jetty-util-6.1.26.jar" "__spark_libs__/jetty-util-6.1.26.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/usercache/fairy/filecache/38960/__spark_conf__.zip" "__spark_conf__"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91358/avro-ipc-1.7.7.jar" "__spark_libs__/avro-ipc-1.7.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91363/calcite-avatica-1.2.0-incubating.jar" "__spark_libs__/calcite-avatica-1.2.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91410/spark-mllib_2.11-2.0.1.jar" "__spark_libs__/spark-mllib_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91276/commons-beanutils-core-1.8.0.jar" "__spark_libs__/commons-beanutils-core-1.8.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91382/stringtemplate-3.2.1.jar" "__spark_libs__/stringtemplate-3.2.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91355/base64-2.3.8.jar" "__spark_libs__/base64-2.3.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91303/json4s-jackson_2.11-3.2.11.jar" "__spark_libs__/json4s-jackson_2.11-3.2.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91425/joda-time-2.9.3.jar" "__spark_libs__/joda-time-2.9.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91371/commons-io-2.4.jar" "__spark_libs__/commons-io-2.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91408/apacheds-kerberos-codec-2.0.0-M15.jar" "__spark_libs__/apacheds-kerberos-codec-2.0.0-M15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91468/mx4j-3.0.2.jar" "__spark_libs__/mx4j-3.0.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91290/scala-xml_2.11-1.0.2.jar" "__spark_libs__/scala-xml_2.11-1.0.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91464/RoaringBitmap-0.5.11.jar" "__spark_libs__/RoaringBitmap-0.5.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91442/parquet-column-1.7.0.jar" "__spark_libs__/parquet-column-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91469/spark-hive-thriftserver_2.11-2.0.1.jar" "__spark_libs__/spark-hive-thriftserver_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91429/java-xmlbuilder-1.0.jar" "__spark_libs__/java-xmlbuilder-1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91415/javax.annotation-api-1.2.jar" "__spark_libs__/javax.annotation-api-1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91317/jackson-core-2.6.5.jar" "__spark_libs__/jackson-core-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91305/commons-beanutils-1.7.0.jar" "__spark_libs__/commons-beanutils-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91325/hk2-api-2.4.0-b34.jar" "__spark_libs__/hk2-api-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91344/ivy-2.4.0.jar" "__spark_libs__/ivy-2.4.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91323/commons-dbcp-1.4.jar" "__spark_libs__/commons-dbcp-1.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91328/commons-digester-1.8.jar" "__spark_libs__/commons-digester-1.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91378/hadoop-annotations-2.7.1.jar" "__spark_libs__/hadoop-annotations-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91322/jets3t-0.9.3.jar" "__spark_libs__/jets3t-0.9.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91474/antlr4-runtime-4.5.3.jar" "__spark_libs__/antlr4-runtime-4.5.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91398/log4j-1.2.17.jar" "__spark_libs__/log4j-1.2.17.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
# Creating copy of launch script
cp "launch_container.sh" "/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000005/launch_container.sh"
chmod 640 "/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000005/launch_container.sh"
# Determining directory contents
echo "ls -l:" 1>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000005/directory.info"
ls -l 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000005/directory.info"
echo "find -L . -maxdepth 5 -ls:" 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000005/directory.info"
find -L . -maxdepth 5 -ls 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000005/directory.info"
echo "broken symlinks(find -L . -maxdepth 5 -type l -ls):" 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000005/directory.info"
find -L . -maxdepth 5 -type l -ls 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000005/directory.info"
exec /bin/bash -c "$JAVA_HOME/bin/java -server -Xmx3072m '-Dhdp.version=2.4.2.0-258' -Djava.io.tmpdir=$PWD/tmp '-Dspark.history.ui.port=18080' '-Dspark.ui.port=0' '-Dspark.driver.port=19210' -Dspark.yarn.app.container.log.dir=/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000005 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.60.43.33:19210 --executor-id 4 --hostname c414.hadoop.gda.lo --cores 1 --app-id application_1494395298335_303847 --user-class-path file:$PWD/__app__.jar 1> /hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000005/stdout 2> /hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000005/stderr"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
End of LogType:launch_container.sh

LogType:stderr
Log Upload Time:Tue May 16 11:12:18 +0700 2017
LogLength:43442
Log Contents:
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/hadoop/yarn/local/filecache/91343/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/hdp/2.4.2.0-258/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
17/05/16 11:10:35 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 23152@c414.hadoop.gda.lo
17/05/16 11:10:35 INFO util.SignalUtils: Registered signal handler for TERM
17/05/16 11:10:35 INFO util.SignalUtils: Registered signal handler for HUP
17/05/16 11:10:35 INFO util.SignalUtils: Registered signal handler for INT
17/05/16 11:10:36 INFO spark.SecurityManager: Changing view acls to: yarn,fairy
17/05/16 11:10:36 INFO spark.SecurityManager: Changing modify acls to: yarn,fairy
17/05/16 11:10:36 INFO spark.SecurityManager: Changing view acls groups to: 
17/05/16 11:10:36 INFO spark.SecurityManager: Changing modify acls groups to: 
17/05/16 11:10:36 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(yarn, fairy); groups with view permissions: Set(); users  with modify permissions: Set(yarn, fairy); groups with modify permissions: Set()
17/05/16 11:10:36 INFO client.TransportClientFactory: Successfully created connection to /10.60.43.33:19210 after 84 ms (0 ms spent in bootstraps)
17/05/16 11:10:37 INFO spark.SecurityManager: Changing view acls to: yarn,fairy
17/05/16 11:10:37 INFO spark.SecurityManager: Changing modify acls to: yarn,fairy
17/05/16 11:10:37 INFO spark.SecurityManager: Changing view acls groups to: 
17/05/16 11:10:37 INFO spark.SecurityManager: Changing modify acls groups to: 
17/05/16 11:10:37 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(yarn, fairy); groups with view permissions: Set(); users  with modify permissions: Set(yarn, fairy); groups with modify permissions: Set()
17/05/16 11:10:37 INFO client.TransportClientFactory: Successfully created connection to /10.60.43.33:19210 after 1 ms (0 ms spent in bootstraps)
17/05/16 11:10:37 INFO storage.DiskBlockManager: Created local directory at /hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/blockmgr-f4363b19-f652-417e-afc2-bde7a9892bfb
17/05/16 11:10:37 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MB
17/05/16 11:10:38 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@10.60.43.33:19210
17/05/16 11:10:38 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver
17/05/16 11:10:38 INFO executor.Executor: Starting executor ID 4 on host c414.hadoop.gda.lo
17/05/16 11:10:38 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34712.
17/05/16 11:10:38 INFO netty.NettyBlockTransferService: Server created on c414.hadoop.gda.lo:34712
17/05/16 11:10:38 INFO storage.BlockManager: external shuffle service port = 7337
17/05/16 11:10:38 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(4, c414.hadoop.gda.lo, 34712)
17/05/16 11:10:38 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(4, c414.hadoop.gda.lo, 34712)
17/05/16 11:10:38 INFO storage.BlockManager: Registering executor with local external shuffle service.
17/05/16 11:10:38 INFO client.TransportClientFactory: Successfully created connection to c414.hadoop.gda.lo/10.60.43.14:7337 after 1 ms (0 ms spent in bootstraps)
17/05/16 11:10:51 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 0
17/05/16 11:10:51 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 0)
17/05/16 11:10:51 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 0
17/05/16 11:10:51 INFO client.TransportClientFactory: Successfully created connection to /10.60.43.33:5141 after 3 ms (0 ms spent in bootstraps)
17/05/16 11:10:51 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.2 KB, free 1458.6 MB)
17/05/16 11:10:51 INFO broadcast.TorrentBroadcast: Reading broadcast variable 0 took 147 ms
17/05/16 11:10:51 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 30.3 KB, free 1458.6 MB)
17/05/16 11:10:51 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
17/05/16 11:10:51 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.60.43.33:19210)
17/05/16 11:10:51 INFO spark.MapOutputTrackerWorker: Got the output locations
17/05/16 11:10:51 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:51 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
17/05/16 11:10:52 INFO codegen.CodeGenerator: Code generated in 347.535636 ms
17/05/16 11:10:52 INFO codegen.CodeGenerator: Code generated in 12.570026 ms
17/05/16 11:10:52 INFO codegen.CodeGenerator: Code generated in 11.538928 ms
17/05/16 11:10:52 INFO codegen.CodeGenerator: Code generated in 14.442593 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 0). 4292 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 5
17/05/16 11:10:52 INFO executor.Executor: Running task 5.0 in stage 1.0 (TID 5)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 5.0 in stage 1.0 (TID 5). 3414 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 7
17/05/16 11:10:52 INFO executor.Executor: Running task 7.0 in stage 1.0 (TID 7)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 7.0 in stage 1.0 (TID 7). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 8
17/05/16 11:10:52 INFO executor.Executor: Running task 8.0 in stage 1.0 (TID 8)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 8.0 in stage 1.0 (TID 8). 3414 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 10
17/05/16 11:10:52 INFO executor.Executor: Running task 10.0 in stage 1.0 (TID 10)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 10.0 in stage 1.0 (TID 10). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 12
17/05/16 11:10:52 INFO executor.Executor: Running task 12.0 in stage 1.0 (TID 12)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 12.0 in stage 1.0 (TID 12). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 14
17/05/16 11:10:52 INFO executor.Executor: Running task 14.0 in stage 1.0 (TID 14)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 14.0 in stage 1.0 (TID 14). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 16
17/05/16 11:10:52 INFO executor.Executor: Running task 16.0 in stage 1.0 (TID 16)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 16.0 in stage 1.0 (TID 16). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 19
17/05/16 11:10:52 INFO executor.Executor: Running task 19.0 in stage 1.0 (TID 19)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 19.0 in stage 1.0 (TID 19). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 22
17/05/16 11:10:52 INFO executor.Executor: Running task 22.0 in stage 1.0 (TID 22)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 22.0 in stage 1.0 (TID 22). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 25
17/05/16 11:10:52 INFO executor.Executor: Running task 25.0 in stage 1.0 (TID 25)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 25.0 in stage 1.0 (TID 25). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 27
17/05/16 11:10:52 INFO executor.Executor: Running task 27.0 in stage 1.0 (TID 27)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 27.0 in stage 1.0 (TID 27). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 29
17/05/16 11:10:52 INFO executor.Executor: Running task 29.0 in stage 1.0 (TID 29)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 29.0 in stage 1.0 (TID 29). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 32
17/05/16 11:10:52 INFO executor.Executor: Running task 32.0 in stage 1.0 (TID 32)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 32.0 in stage 1.0 (TID 32). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 36
17/05/16 11:10:52 INFO executor.Executor: Running task 36.0 in stage 1.0 (TID 36)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 36.0 in stage 1.0 (TID 36). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 39
17/05/16 11:10:52 INFO executor.Executor: Running task 39.0 in stage 1.0 (TID 39)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 39.0 in stage 1.0 (TID 39). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 41
17/05/16 11:10:52 INFO executor.Executor: Running task 41.0 in stage 1.0 (TID 41)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 41.0 in stage 1.0 (TID 41). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 44
17/05/16 11:10:52 INFO executor.Executor: Running task 44.0 in stage 1.0 (TID 44)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 44.0 in stage 1.0 (TID 44). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 46
17/05/16 11:10:52 INFO executor.Executor: Running task 46.0 in stage 1.0 (TID 46)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 46.0 in stage 1.0 (TID 46). 3414 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 48
17/05/16 11:10:52 INFO executor.Executor: Running task 48.0 in stage 1.0 (TID 48)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 48.0 in stage 1.0 (TID 48). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 50
17/05/16 11:10:52 INFO executor.Executor: Running task 50.0 in stage 1.0 (TID 50)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 50.0 in stage 1.0 (TID 50). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 53
17/05/16 11:10:52 INFO executor.Executor: Running task 53.0 in stage 1.0 (TID 53)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 53.0 in stage 1.0 (TID 53). 3414 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 56
17/05/16 11:10:52 INFO executor.Executor: Running task 56.0 in stage 1.0 (TID 56)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 56.0 in stage 1.0 (TID 56). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 58
17/05/16 11:10:52 INFO executor.Executor: Running task 58.0 in stage 1.0 (TID 58)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 58.0 in stage 1.0 (TID 58). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 61
17/05/16 11:10:52 INFO executor.Executor: Running task 61.0 in stage 1.0 (TID 61)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 61.0 in stage 1.0 (TID 61). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 64
17/05/16 11:10:52 INFO executor.Executor: Running task 64.0 in stage 1.0 (TID 64)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 64.0 in stage 1.0 (TID 64). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 66
17/05/16 11:10:52 INFO executor.Executor: Running task 66.0 in stage 1.0 (TID 66)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 66.0 in stage 1.0 (TID 66). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 68
17/05/16 11:10:52 INFO executor.Executor: Running task 68.0 in stage 1.0 (TID 68)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 68.0 in stage 1.0 (TID 68). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 71
17/05/16 11:10:52 INFO executor.Executor: Running task 71.0 in stage 1.0 (TID 71)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 71.0 in stage 1.0 (TID 71). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 73
17/05/16 11:10:52 INFO executor.Executor: Running task 73.0 in stage 1.0 (TID 73)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 73.0 in stage 1.0 (TID 73). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 75
17/05/16 11:10:52 INFO executor.Executor: Running task 75.0 in stage 1.0 (TID 75)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 75.0 in stage 1.0 (TID 75). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 77
17/05/16 11:10:52 INFO executor.Executor: Running task 77.0 in stage 1.0 (TID 77)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 77.0 in stage 1.0 (TID 77). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 80
17/05/16 11:10:52 INFO executor.Executor: Running task 80.0 in stage 1.0 (TID 80)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 80.0 in stage 1.0 (TID 80). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 83
17/05/16 11:10:53 INFO executor.Executor: Running task 83.0 in stage 1.0 (TID 83)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 83.0 in stage 1.0 (TID 83). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 85
17/05/16 11:10:53 INFO executor.Executor: Running task 85.0 in stage 1.0 (TID 85)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 85.0 in stage 1.0 (TID 85). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 87
17/05/16 11:10:53 INFO executor.Executor: Running task 87.0 in stage 1.0 (TID 87)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 87.0 in stage 1.0 (TID 87). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 91
17/05/16 11:10:53 INFO executor.Executor: Running task 91.0 in stage 1.0 (TID 91)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 91.0 in stage 1.0 (TID 91). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 93
17/05/16 11:10:53 INFO executor.Executor: Running task 93.0 in stage 1.0 (TID 93)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 93.0 in stage 1.0 (TID 93). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 95
17/05/16 11:10:53 INFO executor.Executor: Running task 95.0 in stage 1.0 (TID 95)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 95.0 in stage 1.0 (TID 95). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 98
17/05/16 11:10:53 INFO executor.Executor: Running task 98.0 in stage 1.0 (TID 98)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 98.0 in stage 1.0 (TID 98). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 101
17/05/16 11:10:53 INFO executor.Executor: Running task 101.0 in stage 1.0 (TID 101)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 101.0 in stage 1.0 (TID 101). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 104
17/05/16 11:10:53 INFO executor.Executor: Running task 104.0 in stage 1.0 (TID 104)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 104.0 in stage 1.0 (TID 104). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 106
17/05/16 11:10:53 INFO executor.Executor: Running task 106.0 in stage 1.0 (TID 106)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 106.0 in stage 1.0 (TID 106). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 109
17/05/16 11:10:53 INFO executor.Executor: Running task 109.0 in stage 1.0 (TID 109)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 109.0 in stage 1.0 (TID 109). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 112
17/05/16 11:10:53 INFO executor.Executor: Running task 112.0 in stage 1.0 (TID 112)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 112.0 in stage 1.0 (TID 112). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 114
17/05/16 11:10:53 INFO executor.Executor: Running task 114.0 in stage 1.0 (TID 114)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 114.0 in stage 1.0 (TID 114). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 116
17/05/16 11:10:53 INFO executor.Executor: Running task 116.0 in stage 1.0 (TID 116)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 116.0 in stage 1.0 (TID 116). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 119
17/05/16 11:10:53 INFO executor.Executor: Running task 119.0 in stage 1.0 (TID 119)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 119.0 in stage 1.0 (TID 119). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 122
17/05/16 11:10:53 INFO executor.Executor: Running task 122.0 in stage 1.0 (TID 122)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 122.0 in stage 1.0 (TID 122). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 123
17/05/16 11:10:53 INFO executor.Executor: Running task 123.0 in stage 1.0 (TID 123)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 123.0 in stage 1.0 (TID 123). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 127
17/05/16 11:10:53 INFO executor.Executor: Running task 127.0 in stage 1.0 (TID 127)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 127.0 in stage 1.0 (TID 127). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 128
17/05/16 11:10:53 INFO executor.Executor: Running task 128.0 in stage 1.0 (TID 128)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 128.0 in stage 1.0 (TID 128). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 131
17/05/16 11:10:53 INFO executor.Executor: Running task 131.0 in stage 1.0 (TID 131)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 131.0 in stage 1.0 (TID 131). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 133
17/05/16 11:10:53 INFO executor.Executor: Running task 133.0 in stage 1.0 (TID 133)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 133.0 in stage 1.0 (TID 133). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 135
17/05/16 11:10:53 INFO executor.Executor: Running task 135.0 in stage 1.0 (TID 135)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 135.0 in stage 1.0 (TID 135). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 137
17/05/16 11:10:53 INFO executor.Executor: Running task 137.0 in stage 1.0 (TID 137)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 137.0 in stage 1.0 (TID 137). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 140
17/05/16 11:10:53 INFO executor.Executor: Running task 140.0 in stage 1.0 (TID 140)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 140.0 in stage 1.0 (TID 140). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 142
17/05/16 11:10:53 INFO executor.Executor: Running task 142.0 in stage 1.0 (TID 142)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 142.0 in stage 1.0 (TID 142). 3414 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 146
17/05/16 11:10:53 INFO executor.Executor: Running task 146.0 in stage 1.0 (TID 146)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 146.0 in stage 1.0 (TID 146). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 149
17/05/16 11:10:53 INFO executor.Executor: Running task 149.0 in stage 1.0 (TID 149)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 149.0 in stage 1.0 (TID 149). 3414 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 152
17/05/16 11:10:53 INFO executor.Executor: Running task 152.0 in stage 1.0 (TID 152)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 152.0 in stage 1.0 (TID 152). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 154
17/05/16 11:10:53 INFO executor.Executor: Running task 154.0 in stage 1.0 (TID 154)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 154.0 in stage 1.0 (TID 154). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 158
17/05/16 11:10:53 INFO executor.Executor: Running task 158.0 in stage 1.0 (TID 158)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 158.0 in stage 1.0 (TID 158). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 160
17/05/16 11:10:53 INFO executor.Executor: Running task 160.0 in stage 1.0 (TID 160)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 160.0 in stage 1.0 (TID 160). 3414 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 164
17/05/16 11:10:53 INFO executor.Executor: Running task 164.0 in stage 1.0 (TID 164)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 164.0 in stage 1.0 (TID 164). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 166
17/05/16 11:10:53 INFO executor.Executor: Running task 166.0 in stage 1.0 (TID 166)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 166.0 in stage 1.0 (TID 166). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 169
17/05/16 11:10:53 INFO executor.Executor: Running task 169.0 in stage 1.0 (TID 169)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 169.0 in stage 1.0 (TID 169). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 173
17/05/16 11:10:53 INFO executor.Executor: Running task 173.0 in stage 1.0 (TID 173)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 173.0 in stage 1.0 (TID 173). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 174
17/05/16 11:10:53 INFO executor.Executor: Running task 174.0 in stage 1.0 (TID 174)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 174.0 in stage 1.0 (TID 174). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 177
17/05/16 11:10:53 INFO executor.Executor: Running task 177.0 in stage 1.0 (TID 177)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 177.0 in stage 1.0 (TID 177). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 180
17/05/16 11:10:53 INFO executor.Executor: Running task 180.0 in stage 1.0 (TID 180)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 180.0 in stage 1.0 (TID 180). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 181
17/05/16 11:10:53 INFO executor.Executor: Running task 181.0 in stage 1.0 (TID 181)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 181.0 in stage 1.0 (TID 181). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 184
17/05/16 11:10:53 INFO executor.Executor: Running task 184.0 in stage 1.0 (TID 184)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 184.0 in stage 1.0 (TID 184). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 185
17/05/16 11:10:53 INFO executor.Executor: Running task 185.0 in stage 1.0 (TID 185)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 185.0 in stage 1.0 (TID 185). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 187
17/05/16 11:10:53 INFO executor.Executor: Running task 187.0 in stage 1.0 (TID 187)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 187.0 in stage 1.0 (TID 187). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 190
17/05/16 11:10:53 INFO executor.Executor: Running task 190.0 in stage 1.0 (TID 190)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 190.0 in stage 1.0 (TID 190). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 193
17/05/16 11:10:53 INFO executor.Executor: Running task 193.0 in stage 1.0 (TID 193)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 193.0 in stage 1.0 (TID 193). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 194
17/05/16 11:10:53 INFO executor.Executor: Running task 194.0 in stage 1.0 (TID 194)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 194.0 in stage 1.0 (TID 194). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 198
17/05/16 11:10:53 INFO executor.Executor: Running task 198.0 in stage 1.0 (TID 198)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 198.0 in stage 1.0 (TID 198). 3327 bytes result sent to driver
17/05/16 11:10:54 INFO executor.CoarseGrainedExecutorBackend: Driver commanded a shutdown
17/05/16 11:10:54 INFO memory.MemoryStore: MemoryStore cleared
17/05/16 11:10:54 INFO executor.CoarseGrainedExecutorBackend: Driver from 10.60.43.33:19210 disconnected during shutdown
17/05/16 11:10:54 INFO executor.CoarseGrainedExecutorBackend: Driver from 10.60.43.33:19210 disconnected during shutdown
17/05/16 11:10:54 INFO storage.BlockManager: BlockManager stopped
17/05/16 11:10:54 INFO util.ShutdownHookManager: Shutdown hook called
End of LogType:stderr

LogType:stdout
Log Upload Time:Tue May 16 11:12:18 +0700 2017
LogLength:0
Log Contents:
End of LogType:stdout



Container: container_e81_1494395298335_303847_02_000001 on c416.hadoop.gda.lo_45454
=====================================================================================
LogType:directory.info
Log Upload Time:Tue May 16 11:12:21 +0700 2017
LogLength:28883
Log Contents:
ls -l:
total 112
lrwxrwxrwx 1 yarn hadoop    69 May 16 11:10 __app__.jar -> /hadoop/yarn/local/usercache/fairy/filecache/41321/stats-etlr-1.0.jar
-rw-r--r-- 1 yarn hadoop    71 May 16 11:10 container_tokens
-rwx------ 1 yarn hadoop   676 May 16 11:10 default_container_executor_session.sh
-rwx------ 1 yarn hadoop   730 May 16 11:10 default_container_executor.sh
-rwx------ 1 yarn hadoop 75925 May 16 11:10 launch_container.sh
lrwxrwxrwx 1 yarn hadoop    69 May 16 11:10 __spark_conf__ -> /hadoop/yarn/local/usercache/fairy/filecache/41322/__spark_conf__.zip
drwxr-xr-x 2 yarn hadoop 12288 May 16 11:10 __spark_libs__
drwx--x--- 2 yarn hadoop  4096 May 16 11:10 tmp
find -L . -maxdepth 5 -ls:
16908294    4 drwx--x---   4 yarn     hadoop       4096 May 16 11:10 .
13897787    4 drwx------   2 yarn     hadoop       4096 May 16 11:10 ./__spark_conf__
13897790   12 -r-x------   1 yarn     hadoop       8699 May 16 11:10 ./__spark_conf__/log4j.properties
13897794    0 -r-x------   1 yarn     hadoop          0 May 16 11:10 ./__spark_conf__/yarn.exclude
13897854    4 -r-x------   1 yarn     hadoop       1602 May 16 11:10 ./__spark_conf__/health_check
13897799    4 -r-x------   1 yarn     hadoop        884 May 16 11:10 ./__spark_conf__/ssl-client.xml
13897855    4 -r-x------   1 yarn     hadoop       2316 May 16 11:10 ./__spark_conf__/ssl-client.xml.example
13897853    4 -r-x------   1 yarn     hadoop        690 May 16 11:10 ./__spark_conf__/mapred-env.sh
13897864    4 -r-x------   1 yarn     hadoop       1000 May 16 11:10 ./__spark_conf__/ssl-server.xml
13897869   36 -r-x------   1 yarn     hadoop      34061 May 16 11:10 ./__spark_conf__/__spark_conf__.properties
13897861    4 -r-x------   1 yarn     hadoop        951 May 16 11:10 ./__spark_conf__/mapred-env.cmd
13897793    4 -r-x------   1 yarn     hadoop       3979 May 16 11:10 ./__spark_conf__/hadoop-env.cmd
13897809    4 -r-x------   1 yarn     hadoop       1019 May 16 11:10 ./__spark_conf__/container-executor.cfg
13897804    4 -r-x------   1 yarn     hadoop       2358 May 16 11:10 ./__spark_conf__/topology_script.py
13897792   20 -r-x------   1 yarn     hadoop      16530 May 16 11:10 ./__spark_conf__/yarn-site.xml
13897803    4 -r-x------   1 yarn     hadoop        758 May 16 11:10 ./__spark_conf__/mapred-site.xml.template
13897849    4 -r-x------   1 yarn     hadoop       2490 May 16 11:10 ./__spark_conf__/hadoop-metrics.properties
13897865    4 -r-x------   1 yarn     hadoop       2268 May 16 11:10 ./__spark_conf__/ssl-server.xml.example
13897805    4 -r-x------   1 yarn     hadoop       1335 May 16 11:10 ./__spark_conf__/configuration.xsl
13897857    4 -r-x------   1 yarn     hadoop       1308 May 16 11:10 ./__spark_conf__/hadoop-policy.xml
13897788    8 -r-x------   1 yarn     hadoop       6836 May 16 11:10 ./__spark_conf__/mapred-site.xml
13897856    4 -r-x------   1 yarn     hadoop       1527 May 16 11:10 ./__spark_conf__/kms-env.sh
13897801    4 -r-x------   1 yarn     hadoop       3518 May 16 11:10 ./__spark_conf__/kms-acls.xml
13897806    8 -r-x------   1 yarn     hadoop       5006 May 16 11:10 ./__spark_conf__/yarn-env.sh
13897800    8 -r-x------   1 yarn     hadoop       5404 May 16 11:10 ./__spark_conf__/capacity-scheduler.xml
13897789    8 -r-x------   1 yarn     hadoop       5467 May 16 11:10 ./__spark_conf__/hadoop-env.sh
13897808    8 -r-x------   1 yarn     hadoop       6572 May 16 11:10 ./__spark_conf__/hdfs-site.xml
13897795    8 -r-x------   1 yarn     hadoop       4197 May 16 11:10 ./__spark_conf__/core-site.xml
13897868    4 -r-x------   1 yarn     hadoop        945 May 16 11:10 ./__spark_conf__/taskcontroller.cfg
13897859    4 -r-x------   1 yarn     hadoop        724 May 16 11:10 ./__spark_conf__/topology_mappings.data
13897867    8 -r-x------   1 yarn     hadoop       5511 May 16 11:10 ./__spark_conf__/kms-site.xml
13897866    8 -r-x------   1 yarn     hadoop       4113 May 16 11:10 ./__spark_conf__/mapred-queues.xml.template
13897791    4 -r-x------   1 yarn     hadoop       2302 May 16 11:10 ./__spark_conf__/hadoop-metrics2.properties
13897797    4 -r-x------   1 yarn     hadoop       2250 May 16 11:10 ./__spark_conf__/yarn-env.cmd
13897851    8 -r-x------   1 yarn     hadoop       4221 May 16 11:10 ./__spark_conf__/task-log4j.properties
13897815    4 -r-x------   1 yarn     hadoop       1020 May 16 11:10 ./__spark_conf__/commons-logging.properties
13897796    4 -r-x------   1 yarn     hadoop       1631 May 16 11:10 ./__spark_conf__/kms-log4j.properties
13897858    4 -r-x------   1 yarn     hadoop        229 May 16 11:10 ./__spark_conf__/slaves
16908297    4 -rw-r--r--   1 yarn     hadoop         71 May 16 11:10 ./container_tokens
16908303    4 -rwx------   1 yarn     hadoop        730 May 16 11:10 ./default_container_executor.sh
16908301    4 -rwx------   1 yarn     hadoop        676 May 16 11:10 ./default_container_executor_session.sh
16908302    4 -rw-r--r--   1 yarn     hadoop         16 May 16 11:10 ./.default_container_executor_session.sh.crc
16908300    4 -rw-r--r--   1 yarn     hadoop        604 May 16 11:10 ./.launch_container.sh.crc
16908305   12 drwxr-xr-x   2 yarn     hadoop      12288 May 16 11:10 ./__spark_libs__
14418394  280 -rwxr-xr-x   1 yarn     hadoop     284184 Mar 27 13:36 ./__spark_libs__/commons-codec-1.10.jar
14418437  112 -rwxr-xr-x   1 yarn     hadoop     110600 Mar 27 13:36 ./__spark_libs__/bonecp-0.8.0.RELEASE.jar
14419019 1144 -rwxr-xr-x   1 yarn     hadoop    1171380 Mar 27 13:37 ./__spark_libs__/jackson-databind-2.6.5.jar
14418898 3152 -rwxr-xr-x   1 yarn     hadoop    3224708 Mar 27 13:37 ./__spark_libs__/derby-10.12.1.1.jar
14418584 1204 -rwxr-xr-x   1 yarn     hadoop    1230201 Mar 27 13:37 ./__spark_libs__/netty-3.8.0.Final.jar
14418454  208 -rwxr-xr-x   1 yarn     hadoop     209622 Mar 27 13:36 ./__spark_libs__/parquet-hadoop-1.7.0.jar
14418514  256 -rwxr-xr-x   1 yarn     hadoop     258370 Mar 27 13:37 ./__spark_libs__/calcite-avatica-1.2.0-incubating.jar
14418397   40 -rwxr-xr-x   1 yarn     hadoop      40509 Mar 27 13:36 ./__spark_libs__/slf4j-api-1.7.16.jar
14289127   44 -rwxr-xr-x   1 yarn     hadoop      41755 Mar 27 13:36 ./__spark_libs__/objenesis-2.1.jar
14289076    4 -rwxr-xr-x   1 yarn     hadoop       2545 Mar 27 13:36 ./__spark_libs__/hadoop-client-2.7.1.jar
14418472    8 -rwxr-xr-x   1 yarn     hadoop       4467 Mar 27 13:36 ./__spark_libs__/aopalliance-1.0.jar
14418483  180 -rwxr-xr-x   1 yarn     hadoop     181271 Mar 27 13:37 ./__spark_libs__/hk2-locator-2.4.0-b34.jar
14289071  776 -rwxr-xr-x   1 yarn     hadoop     792964 Mar 27 13:36 ./__spark_libs__/zookeeper-3.4.6.jar
14419049  120 -rwxr-xr-x   1 yarn     hadoop     118973 Mar 27 13:37 ./__spark_libs__/hk2-utils-2.4.0-b34.jar
14418942   20 -rwxr-xr-x   1 yarn     hadoop      20235 Mar 27 13:37 ./__spark_libs__/osgi-resource-locator-1.0.1.jar
14418778  116 -rwxr-xr-x   1 yarn     hadoop     115534 Mar 27 13:37 ./__spark_libs__/javax.ws.rs-api-2.0.1.jar
14418810  404 -rwxr-xr-x   1 yarn     hadoop     412739 Mar 27 13:37 ./__spark_libs__/commons-lang3-3.3.2.jar
14418847  300 -rwxr-xr-x   1 yarn     hadoop     305001 Mar 27 13:37 ./__spark_libs__/commons-httpclient-3.1.jar
14418450    4 -rwxr-xr-x   1 yarn     hadoop       2497 Mar 27 13:36 ./__spark_libs__/javax.inject-1.jar
14419110    8 -rwxr-xr-x   1 yarn     hadoop       5310 Mar 27 13:37 ./__spark_libs__/pmml-schema-1.2.15.jar
14418460   80 -rwxr-xr-x   1 yarn     hadoop      79845 Mar 27 13:36 ./__spark_libs__/compress-lzf-1.0.3.jar
14418916   12 -rwxr-xr-x   1 yarn     hadoop      10023 Mar 27 13:37 ./__spark_libs__/java-xmlbuilder-1.0.jar
14289176 1988 -rwxr-xr-x   1 yarn     hadoop    2035066 Mar 27 13:36 ./__spark_libs__/commons-math3-3.4.1.jar
14289106   96 -rwxr-xr-x   1 yarn     hadoop      95806 Mar 27 13:36 ./__spark_libs__/javax.servlet-api-3.1.0.jar
14418914   32 -rwxr-xr-x   1 yarn     hadoop      30595 Mar 27 13:37 ./__spark_libs__/commons-compiler-2.7.6.jar
14419004 1168 -rwxr-xr-x   1 yarn     hadoop    1194003 Mar 27 13:37 ./__spark_libs__/arpack_combined_all-0.1.jar
14418799   44 -rwxr-xr-x   1 yarn     hadoop      45015 Mar 27 13:37 ./__spark_libs__/hadoop-mapreduce-client-shuffle-2.7.1.jar
14418730   20 -rwxr-xr-x   1 yarn     hadoop      16560 Mar 27 13:37 ./__spark_libs__/api-asn1-api-1.0.0-M20.jar
14418503  436 -rwxr-xr-x   1 yarn     hadoop     442406 Mar 27 13:37 ./__spark_libs__/calcite-linq4j-1.2.0-incubating.jar
14419040  200 -rwxr-xr-x   1 yarn     hadoop     201928 Mar 27 13:37 ./__spark_libs__/RoaringBitmap-0.5.11.jar
14418491  720 -rwxr-xr-x   1 yarn     hadoop     736658 Mar 27 13:37 ./__spark_libs__/httpclient-4.5.2.jar
14418883   80 -rwxr-xr-x   1 yarn     hadoop      79912 Mar 27 13:37 ./__spark_libs__/api-util-1.0.0-M20.jar
14418551  184 -rwxr-xr-x   1 yarn     hadoop     185140 Mar 27 13:37 ./__spark_libs__/commons-io-2.4.jar
14289193  188 -rwxr-xr-x   1 yarn     hadoop     190432 Mar 27 13:36 ./__spark_libs__/gson-2.2.4.jar
14418545  164 -rwxr-xr-x   1 yarn     hadoop     164368 Mar 27 13:37 ./__spark_libs__/antlr-runtime-3.4.jar
14418621  332 -rwxr-xr-x   1 yarn     hadoop     339666 Mar 27 13:37 ./__spark_libs__/datanucleus-api-jdo-3.2.6.jar
14418932   20 -rwxr-xr-x   1 yarn     hadoop      18482 Mar 27 13:37 ./__spark_libs__/eigenbase-properties-1.1.5.jar
14418892  136 -rwxr-xr-x   1 yarn     hadoop     138464 Mar 27 13:37 ./__spark_libs__/hive-beeline-1.2.1.spark2.jar
14418945   24 -rwxr-xr-x   1 yarn     hadoop      20852 Mar 27 13:37 ./__spark_libs__/metrics-graphite-3.1.2.jar
14289102  636 -rwxr-xr-x   1 yarn     hadoop     648678 Mar 27 13:36 ./__spark_libs__/scala-xml_2.11-1.0.2.jar
14418958  896 -rwxr-xr-x   1 yarn     hadoop     917052 Mar 27 13:37 ./__spark_libs__/parquet-column-1.7.0.jar
14419032  140 -rwxr-xr-x   1 yarn     hadoop     142631 Mar 27 13:37 ./__spark_libs__/hadoop-yarn-client-2.7.1.jar
14418608   20 -rwxr-xr-x   1 yarn     hadoop      17385 Mar 27 13:37 ./__spark_libs__/hadoop-annotations-2.7.1.jar
14418782  116 -rwxr-xr-x   1 yarn     hadoop     114913 Mar 27 13:37 ./__spark_libs__/py4j-0.10.3.jar
14418667  100 -rwxr-xr-x   1 yarn     hadoop     100636 Mar 27 13:37 ./__spark_libs__/jsp-api-2.1.jar
14419079   24 -rwxr-xr-x   1 yarn     hadoop      21243 Mar 27 13:37 ./__spark_libs__/parquet-generator-1.7.0.jar
14419083  296 -rwxr-xr-x   1 yarn     hadoop     302248 Mar 27 13:37 ./__spark_libs__/antlr4-runtime-4.5.3.jar
14419100  508 -rwxr-xr-x   1 yarn     hadoop     516127 Mar 27 13:37 ./__spark_libs__/hadoop-mapreduce-client-app-2.7.1.jar
14418823  356 -rwxr-xr-x   1 yarn     hadoop     363908 Mar 27 13:37 ./__spark_libs__/hadoop-yarn-server-common-2.7.1.jar
14418686   44 -rwxr-xr-x   1 yarn     hadoop      41070 Mar 27 13:37 ./__spark_libs__/spark-unsafe_2.11-2.0.1.jar
14418793  100 -rwxr-xr-x   1 yarn     hadoop     100680 Mar 27 13:37 ./__spark_libs__/hive-jdbc-1.2.1.spark2.jar
14418432 1256 -rwxr-xr-x   1 yarn     hadoop    1282424 Mar 27 13:36 ./__spark_libs__/ivy-2.4.0.jar
14418984 1972 -rwxr-xr-x   1 yarn     hadoop    2015514 Mar 27 13:37 ./__spark_libs__/hadoop-yarn-api-2.7.1.jar
14419074   44 -rwxr-xr-x   1 yarn     hadoop      41123 Mar 27 13:37 ./__spark_libs__/commons-cli-1.2.jar
14289087   16 -rwxr-xr-x   1 yarn     hadoop      15827 Mar 27 13:36 ./__spark_libs__/metrics-json-3.1.2.jar
14289084   44 -rwxr-xr-x   1 yarn     hadoop      44925 Mar 27 13:36 ./__spark_libs__/apacheds-i18n-2.0.0-M15.jar
14418464 1024 -rwxr-xr-x   1 yarn     hadoop    1045744 Mar 27 13:36 ./__spark_libs__/leveldbjni-all-1.8.jar
14289240   20 -rwxr-xr-x   1 yarn     hadoop      18098 Mar 27 13:36 ./__spark_libs__/jersey-container-servlet-2.22.2.jar
14419096  164 -rwxr-xr-x   1 yarn     hadoop     167421 Mar 27 13:37 ./__spark_libs__/jersey-client-2.22.2.jar
14418791  380 -rwxr-xr-x   1 yarn     hadoop     387188 Mar 27 13:37 ./__spark_libs__/parquet-format-2.3.0-incubating.jar
14418479  576 -rwxr-xr-x   1 yarn     hadoop     588337 Mar 27 13:37 ./__spark_libs__/commons-collections-3.2.2.jar
14418830  220 -rwxr-xr-x   1 yarn     hadoop     223573 Mar 27 13:37 ./__spark_libs__/chill_2.11-0.8.0.jar
14289190   20 -rwxr-xr-x   1 yarn     hadoop      16993 Mar 27 13:36 ./__spark_libs__/JavaEWAH-0.3.2.jar
14418954  512 -rwxr-xr-x   1 yarn     hadoop     521157 Mar 27 13:37 ./__spark_libs__/mail-1.4.7.jar
14418604   68 -rwxr-xr-x   1 yarn     hadoop      68866 Mar 27 13:37 ./__spark_libs__/curator-client-2.6.0.jar
14289147  428 -rwxr-xr-x   1 yarn     hadoop     436303 Mar 27 13:36 ./__spark_libs__/avro-1.7.7.jar
14418570 1248 -rwxr-xr-x   1 yarn     hadoop    1277883 Mar 27 13:37 ./__spark_libs__/mesos-0.21.1-shaded-protobuf.jar
14289058  224 -rwxr-xr-x   1 yarn     hadoop     227712 Mar 27 13:36 ./__spark_libs__/libthrift-0.9.2.jar
14289074   92 -rwxr-xr-x   1 yarn     hadoop      93210 Mar 27 13:36 ./__spark_libs__/super-csv-2.2.0.jar
14289195  256 -rwxr-xr-x   1 yarn     hadoop     258876 Mar 27 13:36 ./__spark_libs__/jackson-core-2.6.5.jar
14419091 7108 -rwxr-xr-x   1 yarn     hadoop    7276083 Mar 27 13:37 ./__spark_libs__/spire_2.11-0.7.4.jar
14418662  148 -rwxr-xr-x   1 yarn     hadoop     148627 Mar 27 13:37 ./__spark_libs__/stringtemplate-3.2.1.jar
14418351   92 -rwxr-xr-x   1 yarn     hadoop      93407 Mar 27 13:36 ./__spark_libs__/pyrolite-4.9.jar
14418904  616 -rwxr-xr-x   1 yarn     hadoop     627814 Mar 27 13:37 ./__spark_libs__/joda-time-2.9.3.jar
14418653  280 -rwxr-xr-x   1 yarn     hadoop     284220 Mar 27 13:37 ./__spark_libs__/commons-lang-2.6.jar
14289207  244 -rwxr-xr-x   1 yarn     hadoop     248171 Mar 27 13:36 ./__spark_libs__/curator-recipes-2.6.0.jar
14289134 2044 -rwxr-xr-x   1 yarn     hadoop    2090370 Mar 27 13:36 ./__spark_libs__/spark-streaming_2.11-2.0.1.jar
14418389 1848 -rwxr-xr-x   1 yarn     hadoop    1890075 Mar 27 13:36 ./__spark_libs__/datanucleus-core-3.2.10.jar
14289224  176 -rwxr-xr-x   1 yarn     hadoop     178947 Mar 27 13:36 ./__spark_libs__/hk2-api-2.4.0-b34.jar
14418480  528 -rwxr-xr-x   1 yarn     hadoop     539912 Mar 27 13:37 ./__spark_libs__/jetty-6.1.26.jar
14289089   56 -rwxr-xr-x   1 yarn     hadoop      55511 Mar 27 13:36 ./__spark_libs__/spark-network-shuffle_2.11-2.0.1.jar
14419035   16 -rwxr-xr-x   1 yarn     hadoop      15071 Mar 27 13:37 ./__spark_libs__/jta-1.1.jar
14418925   64 -rwxr-xr-x   1 yarn     hadoop      65261 Mar 27 13:37 ./__spark_libs__/oro-2.0.8.jar
14418413 13136 -rwxr-xr-x   1 yarn     hadoop   13448966 Mar 27 13:36 ./__spark_libs__/breeze_2.11-0.11.2.jar
14418826 5712 -rwxr-xr-x   1 yarn     hadoop    5847591 Mar 27 13:37 ./__spark_libs__/spark-mllib_2.11-2.0.1.jar
14418862   28 -rwxr-xr-x   1 yarn     hadoop      26366 Mar 27 13:37 ./__spark_libs__/javax.annotation-api-1.2.jar
14419089  308 -rwxr-xr-x   1 yarn     hadoop     313686 Mar 27 13:37 ./__spark_libs__/libfb303-0.9.2.jar
14289131 8068 -rwxr-xr-x   1 yarn     hadoop    8260573 Mar 27 13:36 ./__spark_libs__/hadoop-hdfs-2.7.1.jar
14418563 2780 -rwxr-xr-x   1 yarn     hadoop    2842667 Mar 27 13:37 ./__spark_libs__/bcprov-jdk15on-1.51.jar
14418715   48 -rwxr-xr-x   1 yarn     hadoop      48720 Mar 27 13:37 ./__spark_libs__/snappy-0.2.jar
14289081   64 -rwxr-xr-x   1 yarn     hadoop      65012 Mar 27 13:36 ./__spark_libs__/guice-servlet-3.0.jar
14289067  176 -rwxr-xr-x   1 yarn     hadoop     177131 Mar 27 13:36 ./__spark_libs__/jetty-util-6.1.26.jar
14289149   20 -rwxr-xr-x   1 yarn     hadoop      16430 Mar 27 13:36 ./__spark_libs__/jcl-over-slf4j-1.7.16.jar
14418929  700 -rwxr-xr-x   1 yarn     hadoop     714194 Mar 27 13:37 ./__spark_libs__/javassist-3.18.1-GA.jar
14419010  440 -rwxr-xr-x   1 yarn     hadoop     448794 Mar 27 13:37 ./__spark_libs__/apache-log4j-extras-1.2.17.jar
14418531   32 -rwxr-xr-x   1 yarn     hadoop      29540 Mar 27 13:37 ./__spark_libs__/spark-sketch_2.11-2.0.1.jar
14417921  164 -rwxr-xr-x   1 yarn     hadoop     164422 Mar 27 13:36 ./__spark_libs__/core-1.1.2.jar
14289118   96 -rwxr-xr-x   1 yarn     hadoop      96221 Mar 27 13:36 ./__spark_libs__/commons-pool-1.5.4.jar
14419015 2140 -rwxr-xr-x   1 yarn     hadoop    2189117 Mar 27 13:37 ./__spark_libs__/guava-14.0.1.jar
14418912   16 -rwxr-xr-x   1 yarn     hadoop      15010 Mar 27 13:37 ./__spark_libs__/xmlenc-0.52.jar
14418832  676 -rwxr-xr-x   1 yarn     hadoop     691479 Mar 27 13:37 ./__spark_libs__/apacheds-kerberos-codec-2.0.0-M15.jar
14289202  932 -rwxr-xr-x   1 yarn     hadoop     951701 Mar 27 13:36 ./__spark_libs__/jersey-server-2.22.2.jar
14289051  112 -rwxr-xr-x   1 yarn     hadoop     112558 Mar 27 13:36 ./__spark_libs__/metrics-core-3.1.2.jar
14418688  416 -rwxr-xr-x   1 yarn     hadoop     423753 Mar 27 13:37 ./__spark_libs__/scala-parser-combinators_2.11-1.0.4.jar
14418391  764 -rwxr-xr-x   1 yarn     hadoop     780664 Mar 27 13:36 ./__spark_libs__/jackson-mapper-asl-1.9.13.jar
14418638   16 -rwxr-xr-x   1 yarn     hadoop      14766 Mar 27 13:37 ./__spark_libs__/aopalliance-repackaged-2.4.0-b34.jar
14289187  532 -rwxr-xr-x   1 yarn     hadoop     540852 Mar 27 13:36 ./__spark_libs__/mysql-connector-java-5.0.8-bin.jar
14418528   72 -rwxr-xr-x   1 yarn     hadoop      72733 Mar 27 13:37 ./__spark_libs__/jersey-media-jaxb-2.22.2.jar
14418874  208 -rwxr-xr-x   1 yarn     hadoop     212453 Mar 27 13:37 ./__spark_libs__/commons-net-2.2.jar
14418819   24 -rwxr-xr-x   1 yarn     hadoop      21575 Mar 27 13:37 ./__spark_libs__/parquet-common-1.7.0.jar
14418540  436 -rwxr-xr-x   1 yarn     hadoop     445288 Mar 27 13:37 ./__spark_libs__/antlr-2.7.7.jar
14289181 2732 -rwxr-xr-x   1 yarn     hadoop    2796935 Mar 27 13:36 ./__spark_libs__/parquet-hadoop-bundle-1.6.0.jar
14418556  104 -rwxr-xr-x   1 yarn     hadoop     105134 Mar 27 13:37 ./__spark_libs__/jaxb-api-2.2.2.jar
14418505 5612 -rwxr-xr-x   1 yarn     hadoop    5744974 Mar 27 13:37 ./__spark_libs__/scala-library-2.11.8.jar
14418440   52 -rwxr-xr-x   1 yarn     hadoop      50619 Mar 27 13:36 ./__spark_libs__/chill-java-0.8.0.jar
14418982   84 -rwxr-xr-x   1 yarn     hadoop      82421 Mar 27 13:37 ./__spark_libs__/json4s-ast_2.11-3.2.11.jar
14289062  204 -rwxr-xr-x   1 yarn     hadoop     206035 Mar 27 13:36 ./__spark_libs__/commons-beanutils-core-1.8.0.jar
14418921  352 -rwxr-xr-x   1 yarn     hadoop     358390 Mar 27 13:37 ./__spark_libs__/kryo-shaded-3.0.3.jar
14289168   40 -rwxr-xr-x   1 yarn     hadoop      39280 Mar 27 13:36 ./__spark_libs__/metrics-jvm-3.1.2.jar
14419022   68 -rwxr-xr-x   1 yarn     hadoop      69409 Mar 27 13:37 ./__spark_libs__/activation-1.1.1.jar
14418520 6716 -rwxr-xr-x   1 yarn     hadoop    6873892 Mar 27 13:37 ./__spark_libs__/spark-catalyst_2.11-2.0.1.jar
14289211   44 -rwxr-xr-x   1 yarn     hadoop      41263 Mar 27 13:36 ./__spark_libs__/jackson-module-paranamer-2.6.5.jar
14419104  736 -rwxr-xr-x   1 yarn     hadoop     753012 Mar 27 13:37 ./__spark_libs__/hadoop-mapreduce-client-common-2.7.1.jar
14418975  668 -rwxr-xr-x   1 yarn     hadoop     680106 Mar 27 13:37 ./__spark_libs__/spark-graphx_2.11-2.0.1.jar
14418736  952 -rwxr-xr-x   1 yarn     hadoop     971310 Mar 27 13:37 ./__spark_libs__/jersey-guava-2.22.2.jar
14418462  232 -rwxr-xr-x   1 yarn     hadoop     236660 Mar 27 13:36 ./__spark_libs__/ST4-4.0.4.jar
14418759   24 -rwxr-xr-x   1 yarn     hadoop      23346 Mar 27 13:37 ./__spark_libs__/stax-api-1.0-2.jar
14418901 1768 -rwxr-xr-x   1 yarn     hadoop    1809447 Mar 27 13:37 ./__spark_libs__/datanucleus-rdbms-3.2.9.jar
14418536  524 -rwxr-xr-x   1 yarn     hadoop     533455 Mar 27 13:37 ./__spark_libs__/protobuf-java-2.5.0.jar
14418871 1204 -rwxr-xr-x   1 yarn     hadoop    1229125 Mar 27 13:37 ./__spark_libs__/xercesImpl-2.9.1.jar
14418476   20 -rwxr-xr-x   1 yarn     hadoop      17008 Mar 27 13:36 ./__spark_libs__/base64-2.3.8.jar
14418836  232 -rwxr-xr-x   1 yarn     hadoop     236880 Mar 27 13:37 ./__spark_libs__/lz4-1.3.0.jar
14418487  192 -rwxr-xr-x   1 yarn     hadoop     192993 Mar 27 13:37 ./__spark_libs__/avro-ipc-1.7.7.jar
14289166  176 -rwxr-xr-x   1 yarn     hadoop     177832 Mar 27 13:36 ./__spark_libs__/spark-mllib-local_2.11-2.0.1.jar
14418951   64 -rwxr-xr-x   1 yarn     hadoop      62050 Mar 27 13:37 ./__spark_libs__/commons-logging-1.1.3.jar
14289231  228 -rwxr-xr-x   1 yarn     hadoop     232248 Mar 27 13:36 ./__spark_libs__/jackson-core-asl-1.9.13.jar
14289219  284 -rwxr-xr-x   1 yarn     hadoop     290506 Mar 27 13:36 ./__spark_libs__/univocity-parsers-2.1.1.jar
14419012  388 -rwxr-xr-x   1 yarn     hadoop     395195 Mar 27 13:37 ./__spark_libs__/javolution-5.5.1.jar
14289161  188 -rwxr-xr-x   1 yarn     hadoop     188671 Mar 27 13:36 ./__spark_libs__/commons-beanutils-1.7.0.jar
14418785  292 -rwxr-xr-x   1 yarn     hadoop     298829 Mar 27 13:37 ./__spark_libs__/commons-configuration-1.6.jar
14418788  480 -rwxr-xr-x   1 yarn     hadoop     489884 Mar 27 13:37 ./__spark_libs__/log4j-1.2.17.jar
14289120   64 -rwxr-xr-x   1 yarn     hadoop      63777 Mar 27 13:36 ./__spark_libs__/validation-api-1.1.0.Final.jar
14418947  184 -rwxr-xr-x   1 yarn     hadoop     185245 Mar 27 13:37 ./__spark_libs__/curator-framework-2.6.0.jar
14418965 1024 -rwxr-xr-x   1 yarn     hadoop    1044511 Mar 27 13:37 ./__spark_libs__/spark-hive_2.11-2.0.1.jar
14418709  280 -rwxr-xr-x   1 yarn     hadoop     285447 Mar 27 13:37 ./__spark_libs__/parquet-encoding-1.7.0.jar
14289172  144 -rwxr-xr-x   1 yarn     hadoop     144660 Mar 27 13:36 ./__spark_libs__/xbean-asm5-shaded-4.4.jar
14419076   28 -rwxr-xr-x   1 yarn     hadoop      26514 Mar 27 13:37 ./__spark_libs__/stax-api-1.0.1.jar
14419029  420 -rwxr-xr-x   1 yarn     hadoop     427780 Mar 27 13:37 ./__spark_libs__/jodd-core-3.5.2.jar
14418936 1032 -rwxr-xr-x   1 yarn     hadoop    1056168 Mar 27 13:37 ./__spark_libs__/snappy-java-1.1.2.6.jar
14418510  236 -rwxr-xr-x   1 yarn     hadoop     241367 Mar 27 13:37 ./__spark_libs__/commons-compress-1.4.1.jar
14289170   68 -rwxr-xr-x   1 yarn     hadoop      66270 Mar 27 13:36 ./__spark_libs__/jersey-container-servlet-core-2.22.2.jar
14418443 1592 -rwxr-xr-x   1 yarn     hadoop    1627065 Mar 27 13:36 ./__spark_libs__/jersey-bundle-1.19.1.jar
14419094 2008 -rwxr-xr-x   1 yarn     hadoop    2054931 Mar 27 13:37 ./__spark_libs__/netty-all-4.0.29.Final.jar
14418795   12 -rwxr-xr-x   1 yarn     hadoop      12131 Mar 27 13:37 ./__spark_libs__/jpam-1.1.jar
14419001  504 -rwxr-xr-x   1 yarn     hadoop     515604 Mar 27 13:37 ./__spark_libs__/jackson-module-scala_2.11-2.6.5.jar
14418949   28 -rwxr-xr-x   1 yarn     hadoop      27084 Mar 27 13:37 ./__spark_libs__/jackson-xc-1.9.13.jar
14418993 3352 -rwxr-xr-x   1 yarn     hadoop    3431544 Mar 27 13:37 ./__spark_libs__/hadoop-common-2.7.1.jar
14418885   36 -rwxr-xr-x   1 yarn     hadoop      33015 Mar 27 13:37 ./__spark_libs__/jsr305-1.3.9.jar
14418991  212 -rwxr-xr-x   1 yarn     hadoop     213911 Mar 27 13:37 ./__spark_libs__/jline-2.12.1.jar
14418517 4468 -rwxr-xr-x   1 yarn     hadoop    4573750 Mar 27 13:37 ./__spark_libs__/scala-reflect-2.11.8.jar
14289153   40 -rwxr-xr-x   1 yarn     hadoop      40341 Mar 27 13:36 ./__spark_libs__/json4s-jackson_2.11-3.2.11.jar
14418938  320 -rwxr-xr-x   1 yarn     hadoop     326724 Mar 27 13:37 ./__spark_libs__/httpcore-4.4.4.jar
14418879 6144 -rwxr-xr-x   1 yarn     hadoop    6290315 Mar 27 13:37 ./__spark_libs__/spark-sql_2.11-2.0.1.jar
14419045  468 -rwxr-xr-x   1 yarn     hadoop     477970 Mar 27 13:37 ./__spark_libs__/lift-json_2.11-2.6.3.jar
14289215  160 -rwxr-xr-x   1 yarn     hadoop     160519 Mar 27 13:36 ./__spark_libs__/commons-dbcp-1.4.jar
14417954  136 -rwxr-xr-x   1 yarn     hadoop     135552 Mar 27 13:36 ./__spark_libs__/breeze-macros_2.11-0.11.2.jar
14418856    8 -rwxr-xr-x   1 yarn     hadoop       4596 Mar 27 13:37 ./__spark_libs__/jul-to-slf4j-1.7.16.jar
14289234  144 -rwxr-xr-x   1 yarn     hadoop     143602 Mar 27 13:36 ./__spark_libs__/commons-digester-1.8.jar
14289113    8 -rwxr-xr-x   1 yarn     hadoop       5950 Mar 27 13:36 ./__spark_libs__/javax.inject-2.4.0-b34.jar
14418522   88 -rwxr-xr-x   1 yarn     hadoop      86811 Mar 27 13:37 ./__spark_libs__/spire-macros_2.11-0.7.4.jar
14418777 1616 -rwxr-xr-x   1 yarn     hadoop    1654097 Mar 27 13:37 ./__spark_libs__/hadoop-yarn-common-2.7.1.jar
14418670 5380 -rwxr-xr-x   1 yarn     hadoop    5505200 Mar 27 13:37 ./__spark_libs__/hive-metastore-1.2.1.spark2.jar
14418406   72 -rwxr-xr-x   1 yarn     hadoop      70688 Mar 27 13:37 ./__spark_libs__/hadoop-auth-2.7.1.jar
14418967   20 -rwxr-xr-x   1 yarn     hadoop      18336 Mar 27 13:37 ./__spark_libs__/jackson-jaxrs-1.9.13.jar
14419065 1772 -rwxr-xr-x   1 yarn     hadoop    1814309 Mar 27 13:37 ./__spark_libs__/spark-hive-thriftserver_2.11-2.0.1.jar
14289229  788 -rwxr-xr-x   1 yarn     hadoop     802818 Mar 27 13:36 ./__spark_libs__/scalap-2.11.8.jar
14289198   64 -rwxr-xr-x   1 yarn     hadoop      63316 Mar 27 13:36 ./__spark_libs__/spark-repl_2.11-2.0.1.jar
14289156  640 -rwxr-xr-x   1 yarn     hadoop     654216 Mar 27 13:36 ./__spark_libs__/pmml-model-1.2.15.jar
14289201 1996 -rwxr-xr-x   1 yarn     hadoop    2041628 Mar 27 13:36 ./__spark_libs__/jets3t-0.9.3.jar
14289122   16 -rwxr-xr-x   1 yarn     hadoop      15305 Mar 27 13:36 ./__spark_libs__/spark-tags_2.11-2.0.1.jar
14418927  696 -rwxr-xr-x   1 yarn     hadoop     710492 Mar 27 13:37 ./__spark_libs__/guice-3.0.jar
14419067  400 -rwxr-xr-x   1 yarn     hadoop     409467 Mar 27 13:37 ./__spark_libs__/mx4j-3.0.2.jar
14289236   20 -rwxr-xr-x   1 yarn     hadoop      19827 Mar 27 13:36 ./__spark_libs__/opencsv-2.3.jar
14418859    8 -rwxr-xr-x   1 yarn     hadoop       5711 Mar 27 13:37 ./__spark_libs__/minlog-1.3.0.jar
14289242  600 -rwxr-xr-x   1 yarn     hadoop     613299 Mar 27 13:36 ./__spark_libs__/janino-2.7.8.jar
14289139 15128 -rwxr-xr-x   1 yarn     hadoop   15487351 Mar 27 13:36 ./__spark_libs__/scala-compiler-2.11.8.jar
14289078 3440 -rwxr-xr-x   1 yarn     hadoop    3519262 Mar 27 13:36 ./__spark_libs__/calcite-core-1.2.0-incubating.jar
14289095   40 -rwxr-xr-x   1 yarn     hadoop      40817 Mar 27 13:36 ./__spark_libs__/hive-cli-1.2.1.spark2.jar
14289099   48 -rwxr-xr-x   1 yarn     hadoop      45944 Mar 27 13:36 ./__spark_libs__/json-20090211.jar
14419008   40 -rwxr-xr-x   1 yarn     hadoop      38134 Mar 27 13:37 ./__spark_libs__/hadoop-mapreduce-client-jobclient-2.7.1.jar
14418961 1024 -rwxr-xr-x   1 yarn     hadoop    1048110 Mar 27 13:37 ./__spark_libs__/parquet-jackson-1.7.0.jar
14289054 1512 -rwxr-xr-x   1 yarn     hadoop    1544875 Mar 27 13:36 ./__spark_libs__/hadoop-mapreduce-client-core-2.7.1.jar
14418420   32 -rwxr-xr-x   1 yarn     hadoop      32145 Mar 27 13:36 ./__spark_libs__/hadoop-yarn-server-web-proxy-2.7.1.jar
14418469 11232 -rwxr-xr-x   1 yarn     hadoop   11498852 Mar 27 13:37 ./__spark_libs__/hive-exec-1.2.1.spark2.jar
14289151  576 -rwxr-xr-x   1 yarn     hadoop     589462 Mar 27 13:36 ./__spark_libs__/json4s-core_2.11-3.2.11.jar
14418416   68 -rwxr-xr-x   1 yarn     hadoop      65653 Mar 27 13:36 ./__spark_libs__/spark-launcher_2.11-2.0.1.jar
14419070  172 -rwxr-xr-x   1 yarn     hadoop     174351 Mar 27 13:37 ./__spark_libs__/stream-2.7.0.jar
14418866 11452 -rwxr-xr-x   1 yarn     hadoop   11723537 Mar 27 13:37 ./__spark_libs__/spark-core_2.11-2.0.1.jar
14418403  656 -rwxr-xr-x   1 yarn     hadoop     669589 Mar 27 13:36 ./__spark_libs__/spark-yarn_2.11-2.0.1.jar
14289115 1444 -rwxr-xr-x   1 yarn     hadoop    1475955 Mar 27 13:36 ./__spark_libs__/htrace-core-3.1.0-incubating.jar
14289184   32 -rwxr-xr-x   1 yarn     hadoop      29555 Mar 27 13:36 ./__spark_libs__/paranamer-2.3.jar
14418721  748 -rwxr-xr-x   1 yarn     hadoop     764569 Mar 27 13:37 ./__spark_libs__/jtransforms-2.4.0.jar
14289065  180 -rwxr-xr-x   1 yarn     hadoop     180736 Mar 27 13:36 ./__spark_libs__/avro-mapred-1.7.7-hadoop2.jar
14289093   48 -rwxr-xr-x   1 yarn     hadoop      46983 Mar 27 13:36 ./__spark_libs__/jackson-annotations-2.6.5.jar
14418424   12 -rwxr-xr-x   1 yarn     hadoop       9939 Mar 27 13:36 ./__spark_libs__/slf4j-log4j12-1.7.16.jar
14418908   96 -rwxr-xr-x   1 yarn     hadoop      94672 Mar 27 13:37 ./__spark_libs__/xz-1.0.jar
14418725  684 -rwxr-xr-x   1 yarn     hadoop     698375 Mar 27 13:37 ./__spark_libs__/jersey-common-2.22.2.jar
14418895 2304 -rwxr-xr-x   1 yarn     hadoop    2355465 Mar 27 13:37 ./__spark_libs__/spark-network-common_2.11-2.0.1.jar
14418853  200 -rwxr-xr-x   1 yarn     hadoop     201124 Mar 27 13:37 ./__spark_libs__/jdo-api-3.0.1.jar
13897695 1152 -r-x------   1 yarn     hadoop    1178454 May 16 11:10 ./__app__.jar
16908298    4 -rw-r--r--   1 yarn     hadoop         12 May 16 11:10 ./.container_tokens.crc
16908295    4 drwx--x---   2 yarn     hadoop       4096 May 16 11:10 ./tmp
16908304    4 -rw-r--r--   1 yarn     hadoop         16 May 16 11:10 ./.default_container_executor.sh.crc
16908299   76 -rwx------   1 yarn     hadoop      75925 May 16 11:10 ./launch_container.sh
broken symlinks(find -L . -maxdepth 5 -type l -ls):
End of LogType:directory.info

LogType:launch_container.sh
Log Upload Time:Tue May 16 11:12:21 +0700 2017
LogLength:75925
Log Contents:
#!/bin/bash

export SPARK_YARN_STAGING_DIR="hdfs://c408.hadoop.gda.lo:8020/user/fairy/.sparkStaging/application_1494395298335_303847"
export HADOOP_CONF_DIR="/usr/hdp/current/hadoop-client/conf"
export MAX_APP_ATTEMPTS="2"
export JAVA_HOME="/usr/jdk64/jdk1.8.0_92/"
export APP_SUBMIT_TIME_ENV="1494907824987"
export NM_HOST="c416.hadoop.gda.lo"
export LOGNAME="fairy"
export JVM_PID="$$"
export PWD="/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_02_000001"
export LOCAL_DIRS="/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847"
export APPLICATION_WEB_PROXY_BASE="/proxy/application_1494395298335_303847"
export NM_HTTP_PORT="8042"
export LOG_DIRS="/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000001"
export NM_AUX_SERVICE_mapreduce_shuffle="AAA0+gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=
"
export NM_PORT="45454"
export USER="fairy"
export HADOOP_YARN_HOME="/usr/hdp/current/hadoop-yarn-nodemanager"
export CLASSPATH="$PWD:$PWD/__spark_conf__:$PWD/__spark_libs__/*:$HADOOP_CONF_DIR:/usr/hdp/current/hadoop-client/*:/usr/hdp/current/hadoop-client/lib/*:/usr/hdp/current/hadoop-hdfs-client/*:/usr/hdp/current/hadoop-hdfs-client/lib/*:/usr/hdp/current/hadoop-yarn-client/*:/usr/hdp/current/hadoop-yarn-client/lib/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.4.2.0-258/hadoop/lib/hadoop-lzo-0.6.0.2.4.2.0-258.jar:/etc/hadoop/conf/secure"
export SPARK_YARN_MODE="true"
export HADOOP_TOKEN_FILE_LOCATION="/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_02_000001/container_tokens"
export NM_AUX_SERVICE_spark_shuffle=""
export SPARK_USER="fairy"
export HOME="/home/"
export CONTAINER_ID="container_e81_1494395298335_303847_02_000001"
export MALLOC_ARENA_MAX="4"
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226327/jackson-module-scala_2.11-2.6.5.jar" "__spark_libs__/jackson-module-scala_2.11-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226279/hadoop-yarn-server-common-2.7.1.jar" "__spark_libs__/hadoop-yarn-server-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226295/hive-beeline-1.2.1.spark2.jar" "__spark_libs__/hive-beeline-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226237/calcite-avatica-1.2.0-incubating.jar" "__spark_libs__/calcite-avatica-1.2.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226351/netty-all-4.0.29.Final.jar" "__spark_libs__/netty-all-4.0.29.Final.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226266/jersey-guava-2.22.2.jar" "__spark_libs__/jersey-guava-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226226/leveldbjni-all-1.8.jar" "__spark_libs__/leveldbjni-all-1.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226340/hk2-utils-2.4.0-b34.jar" "__spark_libs__/hk2-utils-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226156/guice-servlet-3.0.jar" "__spark_libs__/guice-servlet-3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226168/commons-pool-1.5.4.jar" "__spark_libs__/commons-pool-1.5.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226288/minlog-1.3.0.jar" "__spark_libs__/minlog-1.3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226212/spark-yarn_2.11-2.0.1.jar" "__spark_libs__/spark-yarn_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226269/hadoop-yarn-common-2.7.1.jar" "__spark_libs__/hadoop-yarn-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226307/javassist-3.18.1-GA.jar" "__spark_libs__/javassist-3.18.1-GA.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226155/calcite-core-1.2.0-incubating.jar" "__spark_libs__/calcite-core-1.2.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226334/jodd-core-3.5.2.jar" "__spark_libs__/jodd-core-3.5.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226152/jetty-util-6.1.26.jar" "__spark_libs__/jetty-util-6.1.26.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226177/json4s-jackson_2.11-3.2.11.jar" "__spark_libs__/json4s-jackson_2.11-3.2.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226179/commons-beanutils-1.7.0.jar" "__spark_libs__/commons-beanutils-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226314/commons-logging-1.1.3.jar" "__spark_libs__/commons-logging-1.1.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226169/spark-tags_2.11-2.0.1.jar" "__spark_libs__/spark-tags_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226225/ST4-4.0.4.jar" "__spark_libs__/ST4-4.0.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226275/hive-jdbc-1.2.1.spark2.jar" "__spark_libs__/hive-jdbc-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226234/calcite-linq4j-1.2.0-incubating.jar" "__spark_libs__/calcite-linq4j-1.2.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226313/curator-framework-2.6.0.jar" "__spark_libs__/curator-framework-2.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226352/jersey-client-2.22.2.jar" "__spark_libs__/jersey-client-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226170/objenesis-2.1.jar" "__spark_libs__/objenesis-2.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226278/parquet-common-1.7.0.jar" "__spark_libs__/parquet-common-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226246/jaxb-api-2.2.2.jar" "__spark_libs__/jaxb-api-2.2.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226249/mesos-0.21.1-shaded-protobuf.jar" "__spark_libs__/mesos-0.21.1-shaded-protobuf.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226253/datanucleus-api-jdo-3.2.6.jar" "__spark_libs__/datanucleus-api-jdo-3.2.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226224/compress-lzf-1.0.3.jar" "__spark_libs__/compress-lzf-1.0.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226243/antlr-2.7.7.jar" "__spark_libs__/antlr-2.7.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226252/hadoop-annotations-2.7.1.jar" "__spark_libs__/hadoop-annotations-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226326/hadoop-common-2.7.1.jar" "__spark_libs__/hadoop-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226178/pmml-model-1.2.15.jar" "__spark_libs__/pmml-model-1.2.15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226244/antlr-runtime-3.4.jar" "__spark_libs__/antlr-runtime-3.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226223/parquet-hadoop-1.7.0.jar" "__spark_libs__/parquet-hadoop-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226189/JavaEWAH-0.3.2.jar" "__spark_libs__/JavaEWAH-0.3.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226355/pmml-schema-1.2.15.jar" "__spark_libs__/pmml-schema-1.2.15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226164/scala-xml_2.11-1.0.2.jar" "__spark_libs__/scala-xml_2.11-1.0.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226228/aopalliance-1.0.jar" "__spark_libs__/aopalliance-1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226316/parquet-column-1.7.0.jar" "__spark_libs__/parquet-column-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226318/xz-1.0.jar" "__spark_libs__/xz-1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226251/curator-client-2.6.0.jar" "__spark_libs__/curator-client-2.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226339/lift-json_2.11-2.6.3.jar" "__spark_libs__/lift-json_2.11-2.6.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226222/javax.inject-1.jar" "__spark_libs__/javax.inject-1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226149/libthrift-0.9.2.jar" "__spark_libs__/libthrift-0.9.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226157/apacheds-i18n-2.0.0-M15.jar" "__spark_libs__/apacheds-i18n-2.0.0-M15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226171/validation-api-1.1.0.Final.jar" "__spark_libs__/validation-api-1.1.0.Final.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226211/slf4j-api-1.7.16.jar" "__spark_libs__/slf4j-api-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226347/stream-2.7.0.jar" "__spark_libs__/stream-2.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226245/commons-io-2.4.jar" "__spark_libs__/commons-io-2.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226232/avro-ipc-1.7.7.jar" "__spark_libs__/avro-ipc-1.7.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226283/lz4-1.3.0.jar" "__spark_libs__/lz4-1.3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226181/spark-mllib-local_2.11-2.0.1.jar" "__spark_libs__/spark-mllib-local_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226348/antlr4-runtime-4.5.3.jar" "__spark_libs__/antlr4-runtime-4.5.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226306/oro-2.0.8.jar" "__spark_libs__/oro-2.0.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226227/hive-exec-1.2.1.spark2.jar" "__spark_libs__/hive-exec-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226198/univocity-parsers-2.1.1.jar" "__spark_libs__/univocity-parsers-2.1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226216/hadoop-yarn-server-web-proxy-2.7.1.jar" "__spark_libs__/hadoop-yarn-server-web-proxy-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226353/hadoop-mapreduce-client-app-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-app-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226277/commons-lang3-3.3.2.jar" "__spark_libs__/commons-lang3-3.3.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226285/commons-httpclient-3.1.jar" "__spark_libs__/commons-httpclient-3.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226264/api-asn1-api-1.0.0-M20.jar" "__spark_libs__/api-asn1-api-1.0.0-M20.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226256/stringtemplate-3.2.1.jar" "__spark_libs__/stringtemplate-3.2.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226282/apacheds-kerberos-codec-2.0.0-M15.jar" "__spark_libs__/apacheds-kerberos-codec-2.0.0-M15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226215/spark-launcher_2.11-2.0.1.jar" "__spark_libs__/spark-launcher_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226349/libfb303-0.9.2.jar" "__spark_libs__/libfb303-0.9.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226338/RoaringBitmap-0.5.11.jar" "__spark_libs__/RoaringBitmap-0.5.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226161/hive-cli-1.2.1.spark2.jar" "__spark_libs__/hive-cli-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226259/spark-unsafe_2.11-2.0.1.jar" "__spark_libs__/spark-unsafe_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226315/mail-1.4.7.jar" "__spark_libs__/mail-1.4.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226166/javax.inject-2.4.0-b34.jar" "__spark_libs__/javax.inject-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226185/metrics-jvm-3.1.2.jar" "__spark_libs__/metrics-jvm-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226174/scala-compiler-2.11.8.jar" "__spark_libs__/scala-compiler-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226272/log4j-1.2.17.jar" "__spark_libs__/log4j-1.2.17.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226159/spark-network-shuffle_2.11-2.0.1.jar" "__spark_libs__/spark-network-shuffle_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226271/commons-configuration-1.6.jar" "__spark_libs__/commons-configuration-1.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226200/scalap-2.11.8.jar" "__spark_libs__/scalap-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226290/spark-core_2.11-2.0.1.jar" "__spark_libs__/spark-core_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226242/protobuf-java-2.5.0.jar" "__spark_libs__/protobuf-java-2.5.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226262/jtransforms-2.4.0.jar" "__spark_libs__/jtransforms-2.4.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226218/ivy-2.4.0.jar" "__spark_libs__/ivy-2.4.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226250/netty-3.8.0.Final.jar" "__spark_libs__/netty-3.8.0.Final.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226190/gson-2.2.4.jar" "__spark_libs__/gson-2.2.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226208/core-1.1.2.jar" "__spark_libs__/core-1.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226304/kryo-shaded-3.0.3.jar" "__spark_libs__/kryo-shaded-3.0.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226154/super-csv-2.2.0.jar" "__spark_libs__/super-csv-2.2.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226183/xbean-asm5-shaded-4.4.jar" "__spark_libs__/xbean-asm5-shaded-4.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226163/hadoop-client-2.7.1.jar" "__spark_libs__/hadoop-client-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226248/spire-macros_2.11-0.7.4.jar" "__spark_libs__/spire-macros_2.11-0.7.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226324/hadoop-yarn-api-2.7.1.jar" "__spark_libs__/hadoop-yarn-api-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226350/spire_2.11-0.7.4.jar" "__spark_libs__/spire_2.11-0.7.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226289/javax.annotation-api-1.2.jar" "__spark_libs__/javax.annotation-api-1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226298/datanucleus-rdbms-3.2.9.jar" "__spark_libs__/datanucleus-rdbms-3.2.9.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226230/jetty-6.1.26.jar" "__spark_libs__/jetty-6.1.26.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226221/jersey-bundle-1.19.1.jar" "__spark_libs__/jersey-bundle-1.19.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226320/jackson-xc-1.9.13.jar" "__spark_libs__/jackson-xc-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226317/parquet-jackson-1.7.0.jar" "__spark_libs__/parquet-jackson-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226297/derby-10.12.1.1.jar" "__spark_libs__/derby-10.12.1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226153/zookeeper-3.4.6.jar" "__spark_libs__/zookeeper-3.4.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226323/json4s-ast_2.11-3.2.11.jar" "__spark_libs__/json4s-ast_2.11-3.2.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226331/guava-14.0.1.jar" "__spark_libs__/guava-14.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226267/stax-api-1.0-2.jar" "__spark_libs__/stax-api-1.0-2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226303/java-xmlbuilder-1.0.jar" "__spark_libs__/java-xmlbuilder-1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226345/commons-cli-1.2.jar" "__spark_libs__/commons-cli-1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226287/jul-to-slf4j-1.7.16.jar" "__spark_libs__/jul-to-slf4j-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226302/commons-compiler-2.7.6.jar" "__spark_libs__/commons-compiler-2.7.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226293/spark-sql_2.11-2.0.1.jar" "__spark_libs__/spark-sql_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226322/spark-graphx_2.11-2.0.1.jar" "__spark_libs__/spark-graphx_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226301/xmlenc-0.52.jar" "__spark_libs__/xmlenc-0.52.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226150/commons-beanutils-core-1.8.0.jar" "__spark_libs__/commons-beanutils-core-1.8.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226310/httpcore-4.4.4.jar" "__spark_libs__/httpcore-4.4.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226321/hadoop-mapreduce-client-shuffle-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-shuffle-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226254/aopalliance-repackaged-2.4.0-b34.jar" "__spark_libs__/aopalliance-repackaged-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226241/spark-sketch_2.11-2.0.1.jar" "__spark_libs__/spark-sketch_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226336/jta-1.1.jar" "__spark_libs__/jta-1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226202/commons-digester-1.8.jar" "__spark_libs__/commons-digester-1.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226284/spark-mllib_2.11-2.0.1.jar" "__spark_libs__/spark-mllib_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226229/base64-2.3.8.jar" "__spark_libs__/base64-2.3.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226148/hadoop-mapreduce-client-core-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-core-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226258/hive-metastore-1.2.1.spark2.jar" "__spark_libs__/hive-metastore-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226219/bonecp-0.8.0.RELEASE.jar" "__spark_libs__/bonecp-0.8.0.RELEASE.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226217/slf4j-log4j12-1.7.16.jar" "__spark_libs__/slf4j-log4j12-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226194/curator-recipes-2.6.0.jar" "__spark_libs__/curator-recipes-2.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226343/spark-hive-thriftserver_2.11-2.0.1.jar" "__spark_libs__/spark-hive-thriftserver_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226182/jersey-container-servlet-core-2.22.2.jar" "__spark_libs__/jersey-container-servlet-core-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226188/mysql-connector-java-5.0.8-bin.jar" "__spark_libs__/mysql-connector-java-5.0.8-bin.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226175/avro-1.7.7.jar" "__spark_libs__/avro-1.7.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226329/hadoop-mapreduce-client-jobclient-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-jobclient-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226193/jersey-server-2.22.2.jar" "__spark_libs__/jersey-server-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226213/jackson-mapper-asl-1.9.13.jar" "__spark_libs__/jackson-mapper-asl-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226341/spark-hive_2.11-2.0.1.jar" "__spark_libs__/spark-hive_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226239/spark-catalyst_2.11-2.0.1.jar" "__spark_libs__/spark-catalyst_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226184/commons-math3-3.4.1.jar" "__spark_libs__/commons-math3-3.4.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226257/jsp-api-2.1.jar" "__spark_libs__/jsp-api-2.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226344/stax-api-1.0.1.jar" "__spark_libs__/stax-api-1.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226195/jackson-module-paranamer-2.6.5.jar" "__spark_libs__/jackson-module-paranamer-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226206/breeze-macros_2.11-0.11.2.jar" "__spark_libs__/breeze-macros_2.11-0.11.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226220/chill-java-0.8.0.jar" "__spark_libs__/chill-java-0.8.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226335/hadoop-yarn-client-2.7.1.jar" "__spark_libs__/hadoop-yarn-client-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226173/spark-streaming_2.11-2.0.1.jar" "__spark_libs__/spark-streaming_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226199/hk2-api-2.4.0-b34.jar" "__spark_libs__/hk2-api-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226209/datanucleus-core-3.2.10.jar" "__spark_libs__/datanucleus-core-3.2.10.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226235/scala-library-2.11.8.jar" "__spark_libs__/scala-library-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226214/breeze_2.11-0.11.2.jar" "__spark_libs__/breeze_2.11-0.11.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226286/jdo-api-3.0.1.jar" "__spark_libs__/jdo-api-3.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226332/jackson-databind-2.6.5.jar" "__spark_libs__/jackson-databind-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226308/eigenbase-properties-1.1.5.jar" "__spark_libs__/eigenbase-properties-1.1.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226292/commons-net-2.2.jar" "__spark_libs__/commons-net-2.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226312/metrics-graphite-3.1.2.jar" "__spark_libs__/metrics-graphite-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226319/jackson-jaxrs-1.9.13.jar" "__spark_libs__/jackson-jaxrs-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226346/parquet-generator-1.7.0.jar" "__spark_libs__/parquet-generator-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226204/jersey-container-servlet-2.22.2.jar" "__spark_libs__/jersey-container-servlet-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226255/commons-lang-2.6.jar" "__spark_libs__/commons-lang-2.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226187/paranamer-2.3.jar" "__spark_libs__/paranamer-2.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226296/spark-network-common_2.11-2.0.1.jar" "__spark_libs__/spark-network-common_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226197/commons-dbcp-1.4.jar" "__spark_libs__/commons-dbcp-1.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226191/jackson-core-2.6.5.jar" "__spark_libs__/jackson-core-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226196/jets3t-0.9.3.jar" "__spark_libs__/jets3t-0.9.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226309/snappy-java-1.1.2.6.jar" "__spark_libs__/snappy-java-1.1.2.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226167/htrace-core-3.1.0-incubating.jar" "__spark_libs__/htrace-core-3.1.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226268/javax.ws.rs-api-2.0.1.jar" "__spark_libs__/javax.ws.rs-api-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226276/hadoop-auth-2.7.1.jar" "__spark_libs__/hadoop-auth-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226291/xercesImpl-2.9.1.jar" "__spark_libs__/xercesImpl-2.9.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226265/snappy-0.2.jar" "__spark_libs__/snappy-0.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226238/scala-reflect-2.11.8.jar" "__spark_libs__/scala-reflect-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226192/spark-repl_2.11-2.0.1.jar" "__spark_libs__/spark-repl_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226333/activation-1.1.1.jar" "__spark_libs__/activation-1.1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226274/jpam-1.1.jar" "__spark_libs__/jpam-1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226328/arpack_combined_all-0.1.jar" "__spark_libs__/arpack_combined_all-0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226205/janino-2.7.8.jar" "__spark_libs__/janino-2.7.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226300/jsr305-1.3.9.jar" "__spark_libs__/jsr305-1.3.9.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/usercache/fairy/filecache/41322/__spark_conf__.zip" "__spark_conf__"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226160/jackson-annotations-2.6.5.jar" "__spark_libs__/jackson-annotations-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226354/hadoop-mapreduce-client-common-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226311/osgi-resource-locator-1.0.1.jar" "__spark_libs__/osgi-resource-locator-1.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226236/commons-compress-1.4.1.jar" "__spark_libs__/commons-compress-1.4.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226247/bcprov-jdk15on-1.51.jar" "__spark_libs__/bcprov-jdk15on-1.51.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226273/parquet-format-2.3.0-incubating.jar" "__spark_libs__/parquet-format-2.3.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226147/metrics-core-3.1.2.jar" "__spark_libs__/metrics-core-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226176/jcl-over-slf4j-1.7.16.jar" "__spark_libs__/jcl-over-slf4j-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226162/json-20090211.jar" "__spark_libs__/json-20090211.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226233/httpclient-4.5.2.jar" "__spark_libs__/httpclient-4.5.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226337/javolution-5.5.1.jar" "__spark_libs__/javolution-5.5.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226231/hk2-locator-2.4.0-b34.jar" "__spark_libs__/hk2-locator-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226281/chill_2.11-0.8.0.jar" "__spark_libs__/chill_2.11-0.8.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226165/javax.servlet-api-3.1.0.jar" "__spark_libs__/javax.servlet-api-3.1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226325/jline-2.12.1.jar" "__spark_libs__/jline-2.12.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226270/py4j-0.10.3.jar" "__spark_libs__/py4j-0.10.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226260/scala-parser-combinators_2.11-1.0.4.jar" "__spark_libs__/scala-parser-combinators_2.11-1.0.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226299/joda-time-2.9.3.jar" "__spark_libs__/joda-time-2.9.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226180/json4s-core_2.11-3.2.11.jar" "__spark_libs__/json4s-core_2.11-3.2.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226151/avro-mapred-1.7.7-hadoop2.jar" "__spark_libs__/avro-mapred-1.7.7-hadoop2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226305/guice-3.0.jar" "__spark_libs__/guice-3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226172/hadoop-hdfs-2.7.1.jar" "__spark_libs__/hadoop-hdfs-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226240/jersey-media-jaxb-2.22.2.jar" "__spark_libs__/jersey-media-jaxb-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226342/mx4j-3.0.2.jar" "__spark_libs__/mx4j-3.0.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226203/opencsv-2.3.jar" "__spark_libs__/opencsv-2.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226210/commons-codec-1.10.jar" "__spark_libs__/commons-codec-1.10.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226330/apache-log4j-extras-1.2.17.jar" "__spark_libs__/apache-log4j-extras-1.2.17.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/usercache/fairy/filecache/41321/stats-etlr-1.0.jar" "__app__.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226263/jersey-common-2.22.2.jar" "__spark_libs__/jersey-common-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226158/metrics-json-3.1.2.jar" "__spark_libs__/metrics-json-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226201/jackson-core-asl-1.9.13.jar" "__spark_libs__/jackson-core-asl-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226186/parquet-hadoop-bundle-1.6.0.jar" "__spark_libs__/parquet-hadoop-bundle-1.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226261/parquet-encoding-1.7.0.jar" "__spark_libs__/parquet-encoding-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226280/commons-collections-3.2.2.jar" "__spark_libs__/commons-collections-3.2.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226207/pyrolite-4.9.jar" "__spark_libs__/pyrolite-4.9.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/226294/api-util-1.0.0-M20.jar" "__spark_libs__/api-util-1.0.0-M20.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
# Creating copy of launch script
cp "launch_container.sh" "/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000001/launch_container.sh"
chmod 640 "/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000001/launch_container.sh"
# Determining directory contents
echo "ls -l:" 1>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000001/directory.info"
ls -l 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000001/directory.info"
echo "find -L . -maxdepth 5 -ls:" 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000001/directory.info"
find -L . -maxdepth 5 -ls 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000001/directory.info"
echo "broken symlinks(find -L . -maxdepth 5 -type l -ls):" 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000001/directory.info"
find -L . -maxdepth 5 -type l -ls 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000001/directory.info"
exec /bin/bash -c "$JAVA_HOME/bin/java -server -Xmx3072m -Djava.io.tmpdir=$PWD/tmp '-Dhdp.version=2.4.2.0-258' -Dspark.yarn.app.container.log.dir=/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000001 org.apache.spark.deploy.yarn.ApplicationMaster --class 'vng.ge.stats.report.job.Runner' --jar file:/home/fairy/quangctn/spark_submit/report/lib/stats-etlr-1.0.jar --arg 'game_code=cack' --arg 'log_date=2017-05-06' --arg 'calc_id=id' --arg 'source=sdk' --arg 'group_id=game' --arg 'job_name=cack' --arg 'report_number=1-2-3-4-5-6-7-8-9' --arg 'run_timing=a1,a3,a7,a14,a30,ac7,ac30' --arg 'logDir=/ge/warehouse' --properties-file $PWD/__spark_conf__/__spark_conf__.properties 1> /hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000001/stdout 2> /hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000001/stderr"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
End of LogType:launch_container.sh

LogType:stderr
Log Upload Time:Tue May 16 11:12:21 +0700 2017
LogLength:186873
Log Contents:
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/hadoop/yarn/local/filecache/226217/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/hdp/2.4.2.0-258/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
17/05/16 11:10:58 INFO util.SignalUtils: Registered signal handler for TERM
17/05/16 11:10:58 INFO util.SignalUtils: Registered signal handler for HUP
17/05/16 11:10:58 INFO util.SignalUtils: Registered signal handler for INT
17/05/16 11:10:58 INFO yarn.ApplicationMaster: Preparing Local resources
17/05/16 11:10:59 INFO yarn.ApplicationMaster: Prepared Local resources Map(__spark_libs__/guava-14.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/guava-14.0.1.jar" } size: 2189117 timestamp: 1479114351167 type: FILE visibility: PUBLIC, __spark_libs__/base64-2.3.8.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/base64-2.3.8.jar" } size: 17008 timestamp: 1479114347360 type: FILE visibility: PUBLIC, __spark_libs__/xbean-asm5-shaded-4.4.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/xbean-asm5-shaded-4.4.jar" } size: 144660 timestamp: 1479114365576 type: FILE visibility: PUBLIC, __spark_libs__/hive-beeline-1.2.1.spark2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hive-beeline-1.2.1.spark2.jar" } size: 138464 timestamp: 1479114353007 type: FILE visibility: PUBLIC, __spark_libs__/hive-cli-1.2.1.spark2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hive-cli-1.2.1.spark2.jar" } size: 40817 timestamp: 1479114353066 type: FILE visibility: PUBLIC, __spark_libs__/parquet-column-1.7.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/parquet-column-1.7.0.jar" } size: 917052 timestamp: 1479114359648 type: FILE visibility: PUBLIC, __spark_libs__/commons-codec-1.10.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/commons-codec-1.10.jar" } size: 284184 timestamp: 1479114348618 type: FILE visibility: PUBLIC, __spark_libs__/spire_2.11-0.7.4.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/spire_2.11-0.7.4.jar" } size: 7276083 timestamp: 1479114364780 type: FILE visibility: PUBLIC, __spark_libs__/datanucleus-rdbms-3.2.9.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/datanucleus-rdbms-3.2.9.jar" } size: 1809447 timestamp: 1479114350477 type: FILE visibility: PUBLIC, __spark_libs__/javolution-5.5.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/javolution-5.5.1.jar" } size: 395195 timestamp: 1479114356133 type: FILE visibility: PUBLIC, __spark_libs__/spark-sketch_2.11-2.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/spark-sketch_2.11-2.0.1.jar" } size: 29540 timestamp: 1479114363697 type: FILE visibility: PUBLIC, __spark_libs__/parquet-format-2.3.0-incubating.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/parquet-format-2.3.0-incubating.jar" } size: 387188 timestamp: 1479114359848 type: FILE visibility: PUBLIC, __spark_libs__/hadoop-auth-2.7.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hadoop-auth-2.7.1.jar" } size: 70688 timestamp: 1479114351488 type: FILE visibility: PUBLIC, __spark_libs__/snappy-java-1.1.2.6.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/snappy-java-1.1.2.6.jar" } size: 1056168 timestamp: 1479114361925 type: FILE visibility: PUBLIC, __spark_libs__/ivy-2.4.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/ivy-2.4.0.jar" } size: 1282424 timestamp: 1479114354167 type: FILE visibility: PUBLIC, __spark_libs__/jersey-client-2.22.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jersey-client-2.22.2.jar" } size: 167421 timestamp: 1479114356604 type: FILE visibility: PUBLIC, __spark_libs__/pyrolite-4.9.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/pyrolite-4.9.jar" } size: 93407 timestamp: 1479114360574 type: FILE visibility: PUBLIC, __spark_libs__/jetty-util-6.1.26.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jetty-util-6.1.26.jar" } size: 177131 timestamp: 1479114357309 type: FILE visibility: PUBLIC, __spark_libs__/scala-xml_2.11-1.0.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/scala-xml_2.11-1.0.2.jar" } size: 648678 timestamp: 1479114361648 type: FILE visibility: PUBLIC, __app__.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/.sparkStaging/application_1494395298335_303847/stats-etlr-1.0.jar" } size: 1178454 timestamp: 1494907824716 type: FILE visibility: PRIVATE, __spark_libs__/spark-launcher_2.11-2.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/spark-launcher_2.11-2.0.1.jar" } size: 65653 timestamp: 1479114362864 type: FILE visibility: PUBLIC, __spark_libs__/commons-compiler-2.7.6.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/commons-compiler-2.7.6.jar" } size: 30595 timestamp: 1479114348818 type: FILE visibility: PUBLIC, __spark_libs__/commons-io-2.4.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/commons-io-2.4.jar" } size: 185140 timestamp: 1479114349176 type: FILE visibility: PUBLIC, __spark_libs__/objenesis-2.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/objenesis-2.1.jar" } size: 41755 timestamp: 1479114359355 type: FILE visibility: PUBLIC, __spark_libs__/jersey-media-jaxb-2.22.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jersey-media-jaxb-2.22.2.jar" } size: 72733 timestamp: 1479114356914 type: FILE visibility: PUBLIC, __spark_libs__/parquet-jackson-1.7.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/parquet-jackson-1.7.0.jar" } size: 1048110 timestamp: 1479114360177 type: FILE visibility: PUBLIC, __spark_libs__/mx4j-3.0.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/mx4j-3.0.2.jar" } size: 409467 timestamp: 1479114359115 type: FILE visibility: PUBLIC, __spark_libs__/javax.servlet-api-3.1.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/javax.servlet-api-3.1.0.jar" } size: 95806 timestamp: 1479114355903 type: FILE visibility: PUBLIC, __spark_libs__/spire-macros_2.11-0.7.4.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/spire-macros_2.11-0.7.4.jar" } size: 86811 timestamp: 1479114364854 type: FILE visibility: PUBLIC, __spark_libs__/apacheds-kerberos-codec-2.0.0-M15.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/apacheds-kerberos-codec-2.0.0-M15.jar" } size: 691479 timestamp: 1479114346581 type: FILE visibility: PUBLIC, __spark_libs__/scala-parser-combinators_2.11-1.0.4.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/scala-parser-combinators_2.11-1.0.4.jar" } size: 423753 timestamp: 1479114361442 type: FILE visibility: PUBLIC, __spark_libs__/hadoop-client-2.7.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hadoop-client-2.7.1.jar" } size: 2545 timestamp: 1479114351539 type: FILE visibility: PUBLIC, __spark_libs__/spark-repl_2.11-2.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/spark-repl_2.11-2.0.1.jar" } size: 63316 timestamp: 1479114363593 type: FILE visibility: PUBLIC, __spark_libs__/commons-logging-1.1.3.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/commons-logging-1.1.3.jar" } size: 62050 timestamp: 1479114349329 type: FILE visibility: PUBLIC, __spark_libs__/calcite-avatica-1.2.0-incubating.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/calcite-avatica-1.2.0-incubating.jar" } size: 258370 timestamp: 1479114347988 type: FILE visibility: PUBLIC, __spark_libs__/scala-library-2.11.8.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/scala-library-2.11.8.jar" } size: 5744974 timestamp: 1479114361256 type: FILE visibility: PUBLIC, __spark_libs__/stream-2.7.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/stream-2.7.0.jar" } size: 174351 timestamp: 1479114365186 type: FILE visibility: PUBLIC, __spark_libs__/javax.annotation-api-1.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/javax.annotation-api-1.2.jar" } size: 26366 timestamp: 1479114355501 type: FILE visibility: PUBLIC, __spark_libs__/parquet-common-1.7.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/parquet-common-1.7.0.jar" } size: 21575 timestamp: 1479114359702 type: FILE visibility: PUBLIC, __spark_libs__/opencsv-2.3.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/opencsv-2.3.jar" } size: 19827 timestamp: 1479114359401 type: FILE visibility: PUBLIC, __spark_libs__/commons-lang-2.6.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/commons-lang-2.6.jar" } size: 284220 timestamp: 1479114349235 type: FILE visibility: PUBLIC, __spark_libs__/spark-graphx_2.11-2.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/spark-graphx_2.11-2.0.1.jar" } size: 680106 timestamp: 1479114362466 type: FILE visibility: PUBLIC, __spark_libs__/guice-servlet-3.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/guice-servlet-3.0.jar" } size: 65012 timestamp: 1479114351350 type: FILE visibility: PUBLIC, __spark_libs__/metrics-graphite-3.1.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/metrics-graphite-3.1.2.jar" } size: 20852 timestamp: 1479114358871 type: FILE visibility: PUBLIC, __spark_libs__/hadoop-mapreduce-client-common-2.7.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hadoop-mapreduce-client-common-2.7.1.jar" } size: 753012 timestamp: 1479114352328 type: FILE visibility: PUBLIC, __spark_libs__/hadoop-mapreduce-client-shuffle-2.7.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hadoop-mapreduce-client-shuffle-2.7.1.jar" } size: 45015 timestamp: 1479114352553 type: FILE visibility: PUBLIC, __spark_libs__/spark-network-common_2.11-2.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/spark-network-common_2.11-2.0.1.jar" } size: 2355465 timestamp: 1479114363447 type: FILE visibility: PUBLIC, __spark_libs__/commons-math3-3.4.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/commons-math3-3.4.1.jar" } size: 2035066 timestamp: 1479114349475 type: FILE visibility: PUBLIC, __spark_libs__/jersey-bundle-1.19.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jersey-bundle-1.19.1.jar" } size: 1627065 timestamp: 1479114356469 type: FILE visibility: PUBLIC, __spark_libs__/hk2-locator-2.4.0-b34.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hk2-locator-2.4.0-b34.jar" } size: 181271 timestamp: 1479114353738 type: FILE visibility: PUBLIC, __spark_libs__/commons-collections-3.2.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/commons-collections-3.2.2.jar" } size: 588337 timestamp: 1479114348763 type: FILE visibility: PUBLIC, __spark_libs__/kryo-shaded-3.0.3.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/kryo-shaded-3.0.3.jar" } size: 358390 timestamp: 1479114358300 type: FILE visibility: PUBLIC, __spark_libs__/arpack_combined_all-0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/arpack_combined_all-0.1.jar" } size: 1194003 timestamp: 1479114347079 type: FILE visibility: PUBLIC, __spark_libs__/jackson-module-paranamer-2.6.5.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jackson-module-paranamer-2.6.5.jar" } size: 41263 timestamp: 1479114354895 type: FILE visibility: PUBLIC, __spark_libs__/curator-client-2.6.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/curator-client-2.6.0.jar" } size: 68866 timestamp: 1479114350136 type: FILE visibility: PUBLIC, __spark_libs__/hadoop-yarn-client-2.7.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hadoop-yarn-client-2.7.1.jar" } size: 142631 timestamp: 1479114352719 type: FILE visibility: PUBLIC, __spark_libs__/lz4-1.3.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/lz4-1.3.0.jar" } size: 236880 timestamp: 1479114358635 type: FILE visibility: PUBLIC, __spark_libs__/RoaringBitmap-0.5.11.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/RoaringBitmap-0.5.11.jar" } size: 201928 timestamp: 1479114360629 type: FILE visibility: PUBLIC, __spark_libs__/javax.inject-1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/javax.inject-1.jar" } size: 2497 timestamp: 1479114355639 type: FILE visibility: PUBLIC, __spark_libs__/spark-streaming_2.11-2.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/spark-streaming_2.11-2.0.1.jar" } size: 2090370 timestamp: 1479114364106 type: FILE visibility: PUBLIC, __spark_libs__/datanucleus-core-3.2.10.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/datanucleus-core-3.2.10.jar" } size: 1890075 timestamp: 1479114350395 type: FILE visibility: PUBLIC, __spark_libs__/commons-lang3-3.3.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/commons-lang3-3.3.2.jar" } size: 412739 timestamp: 1479114349296 type: FILE visibility: PUBLIC, __spark_libs__/parquet-hadoop-bundle-1.6.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/parquet-hadoop-bundle-1.6.0.jar" } size: 2796935 timestamp: 1479114360096 type: FILE visibility: PUBLIC, __spark_libs__/javax.ws.rs-api-2.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/javax.ws.rs-api-2.0.1.jar" } size: 115534 timestamp: 1479114355988 type: FILE visibility: PUBLIC, __spark_libs__/validation-api-1.1.0.Final.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/validation-api-1.1.0.Final.jar" } size: 63777 timestamp: 1479114365455 type: FILE visibility: PUBLIC, __spark_libs__/jersey-container-servlet-2.22.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jersey-container-servlet-2.22.2.jar" } size: 18098 timestamp: 1479114356728 type: FILE visibility: PUBLIC, __spark_libs__/parquet-encoding-1.7.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/parquet-encoding-1.7.0.jar" } size: 285447 timestamp: 1479114359763 type: FILE visibility: PUBLIC, __spark_libs__/derby-10.12.1.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/derby-10.12.1.1.jar" } size: 3224708 timestamp: 1479114350586 type: FILE visibility: PUBLIC, __spark_libs__/jpam-1.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jpam-1.1.jar" } size: 12131 timestamp: 1479114357692 type: FILE visibility: PUBLIC, __spark_libs__/oro-2.0.8.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/oro-2.0.8.jar" } size: 65261 timestamp: 1479114359455 type: FILE visibility: PUBLIC, __spark_libs__/jackson-databind-2.6.5.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jackson-databind-2.6.5.jar" } size: 1171380 timestamp: 1479114354688 type: FILE visibility: PUBLIC, __spark_libs__/parquet-hadoop-1.7.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/parquet-hadoop-1.7.0.jar" } size: 209622 timestamp: 1479114359964 type: FILE visibility: PUBLIC, __spark_libs__/java-xmlbuilder-1.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/java-xmlbuilder-1.0.jar" } size: 10023 timestamp: 1479114355815 type: FILE visibility: PUBLIC, __spark_libs__/netty-3.8.0.Final.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/netty-3.8.0.Final.jar" } size: 1230201 timestamp: 1479114359200 type: FILE visibility: PUBLIC, __spark_libs__/snappy-0.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/snappy-0.2.jar" } size: 48720 timestamp: 1479114361854 type: FILE visibility: PUBLIC, __spark_libs__/ST4-4.0.4.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/ST4-4.0.4.jar" } size: 236660 timestamp: 1479114364959 type: FILE visibility: PUBLIC, __spark_libs__/calcite-linq4j-1.2.0-incubating.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/calcite-linq4j-1.2.0-incubating.jar" } size: 442406 timestamp: 1479114348202 type: FILE visibility: PUBLIC, __spark_libs__/avro-1.7.7.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/avro-1.7.7.jar" } size: 436303 timestamp: 1479114347141 type: FILE visibility: PUBLIC, __spark_libs__/apache-log4j-extras-1.2.17.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/apache-log4j-extras-1.2.17.jar" } size: 448794 timestamp: 1479114346658 type: FILE visibility: PUBLIC, __spark_libs__/metrics-json-3.1.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/metrics-json-3.1.2.jar" } size: 15827 timestamp: 1479114358921 type: FILE visibility: PUBLIC, __spark_libs__/spark-core_2.11-2.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/spark-core_2.11-2.0.1.jar" } size: 11723537 timestamp: 1479114362385 type: FILE visibility: PUBLIC, __spark_libs__/json4s-jackson_2.11-3.2.11.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/json4s-jackson_2.11-3.2.11.jar" } size: 40341 timestamp: 1479114357958 type: FILE visibility: PUBLIC, __spark_libs__/avro-ipc-1.7.7.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/avro-ipc-1.7.7.jar" } size: 192993 timestamp: 1479114347272 type: FILE visibility: PUBLIC, __spark_libs__/commons-pool-1.5.4.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/commons-pool-1.5.4.jar" } size: 96221 timestamp: 1479114349599 type: FILE visibility: PUBLIC, __spark_libs__/hive-metastore-1.2.1.spark2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hive-metastore-1.2.1.spark2.jar" } size: 5505200 timestamp: 1479114353604 type: FILE visibility: PUBLIC, __spark_libs__/py4j-0.10.3.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/py4j-0.10.3.jar" } size: 114913 timestamp: 1479114360491 type: FILE visibility: PUBLIC, __spark_libs__/scala-reflect-2.11.8.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/scala-reflect-2.11.8.jar" } size: 4573750 timestamp: 1479114361581 type: FILE visibility: PUBLIC, __spark_libs__/super-csv-2.2.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/super-csv-2.2.0.jar" } size: 93210 timestamp: 1479114365297 type: FILE visibility: PUBLIC, __spark_libs__/protobuf-java-2.5.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/protobuf-java-2.5.0.jar" } size: 533455 timestamp: 1479114360414 type: FILE visibility: PUBLIC, __spark_libs__/antlr-2.7.7.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/antlr-2.7.7.jar" } size: 445288 timestamp: 1479114346112 type: FILE visibility: PUBLIC, __spark_libs__/hadoop-mapreduce-client-app-2.7.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hadoop-mapreduce-client-app-2.7.1.jar" } size: 516127 timestamp: 1479114352239 type: FILE visibility: PUBLIC, __spark_libs__/curator-recipes-2.6.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/curator-recipes-2.6.0.jar" } size: 248171 timestamp: 1479114350263 type: FILE visibility: PUBLIC, __spark_libs__/jsp-api-2.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jsp-api-2.1.jar" } size: 100636 timestamp: 1479114358008 type: FILE visibility: PUBLIC, __spark_libs__/breeze_2.11-0.11.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/breeze_2.11-0.11.2.jar" } size: 13448966 timestamp: 1479114347866 type: FILE visibility: PUBLIC, __spark_libs__/commons-beanutils-core-1.8.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/commons-beanutils-core-1.8.0.jar" } size: 206035 timestamp: 1479114348473 type: FILE visibility: PUBLIC, __spark_libs__/metrics-core-3.1.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/metrics-core-3.1.2.jar" } size: 112558 timestamp: 1479114358822 type: FILE visibility: PUBLIC, __spark_libs__/jackson-mapper-asl-1.9.13.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jackson-mapper-asl-1.9.13.jar" } size: 780664 timestamp: 1479114354829 type: FILE visibility: PUBLIC, __spark_libs__/antlr-runtime-3.4.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/antlr-runtime-3.4.jar" } size: 164368 timestamp: 1479114346266 type: FILE visibility: PUBLIC, __spark_libs__/hadoop-hdfs-2.7.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hadoop-hdfs-2.7.1.jar" } size: 8260573 timestamp: 1479114352007 type: FILE visibility: PUBLIC, __spark_libs__/jackson-annotations-2.6.5.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jackson-annotations-2.6.5.jar" } size: 46983 timestamp: 1479114354226 type: FILE visibility: PUBLIC, __spark_libs__/libthrift-0.9.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/libthrift-0.9.2.jar" } size: 227712 timestamp: 1479114358512 type: FILE visibility: PUBLIC, __spark_libs__/spark-mllib-local_2.11-2.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/spark-mllib-local_2.11-2.0.1.jar" } size: 177832 timestamp: 1479114363111 type: FILE visibility: PUBLIC, __spark_libs__/jackson-jaxrs-1.9.13.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jackson-jaxrs-1.9.13.jar" } size: 18336 timestamp: 1479114354733 type: FILE visibility: PUBLIC, __spark_libs__/jersey-guava-2.22.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jersey-guava-2.22.2.jar" } size: 971310 timestamp: 1479114356860 type: FILE visibility: PUBLIC, __spark_libs__/minlog-1.3.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/minlog-1.3.0.jar" } size: 5711 timestamp: 1479114359046 type: FILE visibility: PUBLIC, __spark_libs__/httpclient-4.5.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/httpclient-4.5.2.jar" } size: 736658 timestamp: 1479114354023 type: FILE visibility: PUBLIC, __spark_libs__/paranamer-2.3.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/paranamer-2.3.jar" } size: 29555 timestamp: 1479114359589 type: FILE visibility: PUBLIC, __spark_libs__/hive-exec-1.2.1.spark2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hive-exec-1.2.1.spark2.jar" } size: 11498852 timestamp: 1479114353361 type: FILE visibility: PUBLIC, __spark_libs__/mysql-connector-java-5.0.8-bin.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/mysql-connector-java-5.0.8-bin.jar" } size: 540852 timestamp: 1487839787847 type: FILE visibility: PUBLIC, __spark_libs__/libfb303-0.9.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/libfb303-0.9.2.jar" } size: 313686 timestamp: 1479114358457 type: FILE visibility: PUBLIC, __spark_libs__/hadoop-yarn-api-2.7.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hadoop-yarn-api-2.7.1.jar" } size: 2015514 timestamp: 1479114352670 type: FILE visibility: PUBLIC, __spark_libs__/parquet-generator-1.7.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/parquet-generator-1.7.0.jar" } size: 21243 timestamp: 1479114359903 type: FILE visibility: PUBLIC, __spark_libs__/hadoop-mapreduce-client-core-2.7.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hadoop-mapreduce-client-core-2.7.1.jar" } size: 1544875 timestamp: 1479114352416 type: FILE visibility: PUBLIC, __spark_libs__/jodd-core-3.5.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jodd-core-3.5.2.jar" } size: 427780 timestamp: 1479114357600 type: FILE visibility: PUBLIC, __spark_libs__/spark-hive-thriftserver_2.11-2.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/spark-hive-thriftserver_2.11-2.0.1.jar" } size: 1814309 timestamp: 1479114362781 type: FILE visibility: PUBLIC, __spark_libs__/jackson-xc-1.9.13.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jackson-xc-1.9.13.jar" } size: 27084 timestamp: 1479114355045 type: FILE visibility: PUBLIC, __spark_libs__/javassist-3.18.1-GA.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/javassist-3.18.1-GA.jar" } size: 714194 timestamp: 1479114355425 type: FILE visibility: PUBLIC, __spark_libs__/jersey-common-2.22.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jersey-common-2.22.2.jar" } size: 698375 timestamp: 1479114356678 type: FILE visibility: PUBLIC, __spark_libs__/avro-mapred-1.7.7-hadoop2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/avro-mapred-1.7.7-hadoop2.jar" } size: 180736 timestamp: 1479114347317 type: FILE visibility: PUBLIC, __spark_libs__/spark-catalyst_2.11-2.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/spark-catalyst_2.11-2.0.1.jar" } size: 6873892 timestamp: 1479114362096 type: FILE visibility: PUBLIC, __spark_libs__/log4j-1.2.17.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/log4j-1.2.17.jar" } size: 489884 timestamp: 1479114358563 type: FILE visibility: PUBLIC, __spark_libs__/spark-tags_2.11-2.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/spark-tags_2.11-2.0.1.jar" } size: 15305 timestamp: 1479114364213 type: FILE visibility: PUBLIC, __spark_libs__/metrics-jvm-3.1.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/metrics-jvm-3.1.2.jar" } size: 39280 timestamp: 1479114358993 type: FILE visibility: PUBLIC, __spark_libs__/spark-unsafe_2.11-2.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/spark-unsafe_2.11-2.0.1.jar" } size: 41070 timestamp: 1479114364270 type: FILE visibility: PUBLIC, __spark_libs__/api-asn1-api-1.0.0-M20.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/api-asn1-api-1.0.0-M20.jar" } size: 16560 timestamp: 1479114346748 type: FILE visibility: PUBLIC, __spark_libs__/htrace-core-3.1.0-incubating.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/htrace-core-3.1.0-incubating.jar" } size: 1475955 timestamp: 1479114353943 type: FILE visibility: PUBLIC, __spark_libs__/aopalliance-repackaged-2.4.0-b34.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/aopalliance-repackaged-2.4.0-b34.jar" } size: 14766 timestamp: 1479114346380 type: FILE visibility: PUBLIC, __spark_conf__ -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/.sparkStaging/application_1494395298335_303847/__spark_conf__.zip" } size: 148464 timestamp: 1494907824876 type: ARCHIVE visibility: PRIVATE, __spark_libs__/netty-all-4.0.29.Final.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/netty-all-4.0.29.Final.jar" } size: 2054931 timestamp: 1479114359303 type: FILE visibility: PUBLIC, __spark_libs__/scala-compiler-2.11.8.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/scala-compiler-2.11.8.jar" } size: 15487351 timestamp: 1479114361118 type: FILE visibility: PUBLIC, __spark_libs__/core-1.1.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/core-1.1.2.jar" } size: 164422 timestamp: 1479114350080 type: FILE visibility: PUBLIC, __spark_libs__/mail-1.4.7.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/mail-1.4.7.jar" } size: 521157 timestamp: 1479114358695 type: FILE visibility: PUBLIC, __spark_libs__/jul-to-slf4j-1.7.16.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jul-to-slf4j-1.7.16.jar" } size: 4596 timestamp: 1479114358231 type: FILE visibility: PUBLIC, __spark_libs__/xmlenc-0.52.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/xmlenc-0.52.jar" } size: 15010 timestamp: 1479114366089 type: FILE visibility: PUBLIC, __spark_libs__/guice-3.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/guice-3.0.jar" } size: 710492 timestamp: 1479114351281 type: FILE visibility: PUBLIC, __spark_libs__/calcite-core-1.2.0-incubating.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/calcite-core-1.2.0-incubating.jar" } size: 3519262 timestamp: 1479114348109 type: FILE visibility: PUBLIC, __spark_libs__/xercesImpl-2.9.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/xercesImpl-2.9.1.jar" } size: 1229125 timestamp: 1479114366034 type: FILE visibility: PUBLIC, __spark_libs__/jets3t-0.9.3.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jets3t-0.9.3.jar" } size: 2041628 timestamp: 1479114357121 type: FILE visibility: PUBLIC, __spark_libs__/httpcore-4.4.4.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/httpcore-4.4.4.jar" } size: 326724 timestamp: 1479114354102 type: FILE visibility: PUBLIC, __spark_libs__/jline-2.12.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jline-2.12.1.jar" } size: 213911 timestamp: 1479114357395 type: FILE visibility: PUBLIC, __spark_libs__/json4s-ast_2.11-3.2.11.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/json4s-ast_2.11-3.2.11.jar" } size: 82421 timestamp: 1479114357826 type: FILE visibility: PUBLIC, __spark_libs__/slf4j-log4j12-1.7.16.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/slf4j-log4j12-1.7.16.jar" } size: 9939 timestamp: 1479114361780 type: FILE visibility: PUBLIC, __spark_libs__/jackson-core-2.6.5.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jackson-core-2.6.5.jar" } size: 258876 timestamp: 1479114354282 type: FILE visibility: PUBLIC, __spark_libs__/hadoop-common-2.7.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hadoop-common-2.7.1.jar" } size: 3431544 timestamp: 1479114351722 type: FILE visibility: PUBLIC, __spark_libs__/jaxb-api-2.2.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jaxb-api-2.2.2.jar" } size: 105134 timestamp: 1479114356207 type: FILE visibility: PUBLIC, __spark_libs__/pmml-model-1.2.15.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/pmml-model-1.2.15.jar" } size: 654216 timestamp: 1479114360268 type: FILE visibility: PUBLIC, __spark_libs__/jcl-over-slf4j-1.7.16.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jcl-over-slf4j-1.7.16.jar" } size: 16430 timestamp: 1479114356280 type: FILE visibility: PUBLIC, __spark_libs__/datanucleus-api-jdo-3.2.6.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/datanucleus-api-jdo-3.2.6.jar" } size: 339666 timestamp: 1479114350317 type: FILE visibility: PUBLIC, __spark_libs__/jdo-api-3.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jdo-api-3.0.1.jar" } size: 201124 timestamp: 1479114356360 type: FILE visibility: PUBLIC, __spark_libs__/curator-framework-2.6.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/curator-framework-2.6.0.jar" } size: 185245 timestamp: 1479114350187 type: FILE visibility: PUBLIC, __spark_libs__/jackson-module-scala_2.11-2.6.5.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jackson-module-scala_2.11-2.6.5.jar" } size: 515604 timestamp: 1479114355002 type: FILE visibility: PUBLIC, __spark_libs__/commons-beanutils-1.7.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/commons-beanutils-1.7.0.jar" } size: 188671 timestamp: 1479114348411 type: FILE visibility: PUBLIC, __spark_libs__/jta-1.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jta-1.1.jar" } size: 15071 timestamp: 1479114358099 type: FILE visibility: PUBLIC, __spark_libs__/aopalliance-1.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/aopalliance-1.0.jar" } size: 4467 timestamp: 1479114346315 type: FILE visibility: PUBLIC, __spark_libs__/bcprov-jdk15on-1.51.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/bcprov-jdk15on-1.51.jar" } size: 2842667 timestamp: 1479114347496 type: FILE visibility: PUBLIC, __spark_libs__/slf4j-api-1.7.16.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/slf4j-api-1.7.16.jar" } size: 40509 timestamp: 1479114361709 type: FILE visibility: PUBLIC, __spark_libs__/commons-dbcp-1.4.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/commons-dbcp-1.4.jar" } size: 160519 timestamp: 1479114348991 type: FILE visibility: PUBLIC, __spark_libs__/commons-digester-1.8.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/commons-digester-1.8.jar" } size: 143602 timestamp: 1479114349054 type: FILE visibility: PUBLIC, __spark_libs__/activation-1.1.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/activation-1.1.1.jar" } size: 69409 timestamp: 1479114345985 type: FILE visibility: PUBLIC, __spark_libs__/scalap-2.11.8.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/scalap-2.11.8.jar" } size: 802818 timestamp: 1479114361370 type: FILE visibility: PUBLIC, __spark_libs__/jersey-server-2.22.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jersey-server-2.22.2.jar" } size: 951701 timestamp: 1479114357031 type: FILE visibility: PUBLIC, __spark_libs__/janino-2.7.8.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/janino-2.7.8.jar" } size: 613299 timestamp: 1479114355178 type: FILE visibility: PUBLIC, __spark_libs__/jersey-container-servlet-core-2.22.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jersey-container-servlet-core-2.22.2.jar" } size: 66270 timestamp: 1479114356792 type: FILE visibility: PUBLIC, __spark_libs__/apacheds-i18n-2.0.0-M15.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/apacheds-i18n-2.0.0-M15.jar" } size: 44925 timestamp: 1479114346454 type: FILE visibility: PUBLIC, __spark_libs__/breeze-macros_2.11-0.11.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/breeze-macros_2.11-0.11.2.jar" } size: 135552 timestamp: 1479114347922 type: FILE visibility: PUBLIC, __spark_libs__/javax.inject-2.4.0-b34.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/javax.inject-2.4.0-b34.jar" } size: 5950 timestamp: 1479114355715 type: FILE visibility: PUBLIC, __spark_libs__/univocity-parsers-2.1.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/univocity-parsers-2.1.1.jar" } size: 290506 timestamp: 1479114365337 type: FILE visibility: PUBLIC, __spark_libs__/spark-yarn_2.11-2.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/spark-yarn_2.11-2.0.1.jar" } size: 669589 timestamp: 1479114364455 type: FILE visibility: PUBLIC, __spark_libs__/spark-hive_2.11-2.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/spark-hive_2.11-2.0.1.jar" } size: 1044511 timestamp: 1479114362531 type: FILE visibility: PUBLIC, __spark_libs__/chill-java-0.8.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/chill-java-0.8.0.jar" } size: 50619 timestamp: 1479114348335 type: FILE visibility: PUBLIC, __spark_libs__/jetty-6.1.26.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jetty-6.1.26.jar" } size: 539912 timestamp: 1479114357208 type: FILE visibility: PUBLIC, __spark_libs__/osgi-resource-locator-1.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/osgi-resource-locator-1.0.1.jar" } size: 20235 timestamp: 1479114359529 type: FILE visibility: PUBLIC, __spark_libs__/commons-httpclient-3.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/commons-httpclient-3.1.jar" } size: 305001 timestamp: 1479114349123 type: FILE visibility: PUBLIC, __spark_libs__/commons-compress-1.4.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/commons-compress-1.4.1.jar" } size: 241367 timestamp: 1479114348885 type: FILE visibility: PUBLIC, __spark_libs__/compress-lzf-1.0.3.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/compress-lzf-1.0.3.jar" } size: 79845 timestamp: 1479114349705 type: FILE visibility: PUBLIC, __spark_libs__/gson-2.2.4.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/gson-2.2.4.jar" } size: 190432 timestamp: 1479114351077 type: FILE visibility: PUBLIC, __spark_libs__/spark-mllib_2.11-2.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/spark-mllib_2.11-2.0.1.jar" } size: 5847591 timestamp: 1479114363045 type: FILE visibility: PUBLIC, __spark_libs__/commons-net-2.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/commons-net-2.2.jar" } size: 212453 timestamp: 1479114349538 type: FILE visibility: PUBLIC, __spark_libs__/stax-api-1.0-2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/stax-api-1.0-2.jar" } size: 23346 timestamp: 1479114365093 type: FILE visibility: PUBLIC, __spark_libs__/hadoop-annotations-2.7.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hadoop-annotations-2.7.1.jar" } size: 17385 timestamp: 1479114351396 type: FILE visibility: PUBLIC, __spark_libs__/mesos-0.21.1-shaded-protobuf.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/mesos-0.21.1-shaded-protobuf.jar" } size: 1277883 timestamp: 1479114358770 type: FILE visibility: PUBLIC, __spark_libs__/eigenbase-properties-1.1.5.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/eigenbase-properties-1.1.5.jar" } size: 18482 timestamp: 1479114350658 type: FILE visibility: PUBLIC, __spark_libs__/jtransforms-2.4.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jtransforms-2.4.0.jar" } size: 764569 timestamp: 1479114358157 type: FILE visibility: PUBLIC, __spark_libs__/chill_2.11-0.8.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/chill_2.11-0.8.0.jar" } size: 223573 timestamp: 1479114348273 type: FILE visibility: PUBLIC, __spark_libs__/api-util-1.0.0-M20.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/api-util-1.0.0-M20.jar" } size: 79912 timestamp: 1479114346966 type: FILE visibility: PUBLIC, __spark_libs__/hk2-api-2.4.0-b34.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hk2-api-2.4.0-b34.jar" } size: 178947 timestamp: 1479114353676 type: FILE visibility: PUBLIC, __spark_libs__/hk2-utils-2.4.0-b34.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hk2-utils-2.4.0-b34.jar" } size: 118973 timestamp: 1479114353838 type: FILE visibility: PUBLIC, __spark_libs__/stringtemplate-3.2.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/stringtemplate-3.2.1.jar" } size: 148627 timestamp: 1479114365251 type: FILE visibility: PUBLIC, __spark_libs__/antlr4-runtime-4.5.3.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/antlr4-runtime-4.5.3.jar" } size: 302248 timestamp: 1479114346180 type: FILE visibility: PUBLIC, __spark_libs__/json-20090211.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/json-20090211.jar" } size: 45944 timestamp: 1479114357767 type: FILE visibility: PUBLIC, __spark_libs__/stax-api-1.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/stax-api-1.0.1.jar" } size: 26514 timestamp: 1479114365021 type: FILE visibility: PUBLIC, __spark_libs__/JavaEWAH-0.3.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/JavaEWAH-0.3.2.jar" } size: 16993 timestamp: 1479114355289 type: FILE visibility: PUBLIC, __spark_libs__/bonecp-0.8.0.RELEASE.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/bonecp-0.8.0.RELEASE.jar" } size: 110600 timestamp: 1479114347572 type: FILE visibility: PUBLIC, __spark_libs__/json4s-core_2.11-3.2.11.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/json4s-core_2.11-3.2.11.jar" } size: 589462 timestamp: 1479114357905 type: FILE visibility: PUBLIC, __spark_libs__/jackson-core-asl-1.9.13.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jackson-core-asl-1.9.13.jar" } size: 232248 timestamp: 1479114354339 type: FILE visibility: PUBLIC, __spark_libs__/hadoop-yarn-server-web-proxy-2.7.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hadoop-yarn-server-web-proxy-2.7.1.jar" } size: 32145 timestamp: 1479114352953 type: FILE visibility: PUBLIC, __spark_libs__/spark-network-shuffle_2.11-2.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/spark-network-shuffle_2.11-2.0.1.jar" } size: 55511 timestamp: 1479114363500 type: FILE visibility: PUBLIC, __spark_libs__/zookeeper-3.4.6.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/zookeeper-3.4.6.jar" } size: 792964 timestamp: 1479114366194 type: FILE visibility: PUBLIC, __spark_libs__/pmml-schema-1.2.15.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/pmml-schema-1.2.15.jar" } size: 5310 timestamp: 1479114360327 type: FILE visibility: PUBLIC, __spark_libs__/hadoop-yarn-common-2.7.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hadoop-yarn-common-2.7.1.jar" } size: 1654097 timestamp: 1479114352811 type: FILE visibility: PUBLIC, __spark_libs__/xz-1.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/xz-1.0.jar" } size: 94672 timestamp: 1479114366136 type: FILE visibility: PUBLIC, __spark_libs__/commons-cli-1.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/commons-cli-1.2.jar" } size: 41123 timestamp: 1479114348550 type: FILE visibility: PUBLIC, __spark_libs__/spark-sql_2.11-2.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/spark-sql_2.11-2.0.1.jar" } size: 6290315 timestamp: 1479114363954 type: FILE visibility: PUBLIC, __spark_libs__/hadoop-mapreduce-client-jobclient-2.7.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hadoop-mapreduce-client-jobclient-2.7.1.jar" } size: 38134 timestamp: 1479114352478 type: FILE visibility: PUBLIC, __spark_libs__/joda-time-2.9.3.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/joda-time-2.9.3.jar" } size: 627814 timestamp: 1479114357501 type: FILE visibility: PUBLIC, __spark_libs__/hadoop-yarn-server-common-2.7.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hadoop-yarn-server-common-2.7.1.jar" } size: 363908 timestamp: 1479114352894 type: FILE visibility: PUBLIC, __spark_libs__/commons-configuration-1.6.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/commons-configuration-1.6.jar" } size: 298829 timestamp: 1479114348940 type: FILE visibility: PUBLIC, __spark_libs__/lift-json_2.11-2.6.3.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/lift-json_2.11-2.6.3.jar" } size: 477970 timestamp: 1487839577149 type: FILE visibility: PUBLIC, __spark_libs__/leveldbjni-all-1.8.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/leveldbjni-all-1.8.jar" } size: 1045744 timestamp: 1479114358383 type: FILE visibility: PUBLIC, __spark_libs__/hive-jdbc-1.2.1.spark2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hive-jdbc-1.2.1.spark2.jar" } size: 100680 timestamp: 1479114353442 type: FILE visibility: PUBLIC, __spark_libs__/jsr305-1.3.9.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jsr305-1.3.9.jar" } size: 33015 timestamp: 1479114358053 type: FILE visibility: PUBLIC)
17/05/16 11:10:59 INFO yarn.ApplicationMaster: ApplicationAttemptId: appattempt_1494395298335_303847_000002
17/05/16 11:10:59 INFO spark.SecurityManager: Changing view acls to: yarn,fairy
17/05/16 11:10:59 INFO spark.SecurityManager: Changing modify acls to: yarn,fairy
17/05/16 11:10:59 INFO spark.SecurityManager: Changing view acls groups to: 
17/05/16 11:10:59 INFO spark.SecurityManager: Changing modify acls groups to: 
17/05/16 11:10:59 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(yarn, fairy); groups with view permissions: Set(); users  with modify permissions: Set(yarn, fairy); groups with modify permissions: Set()
17/05/16 11:10:59 INFO yarn.ApplicationMaster: Starting the user application in a separate Thread
17/05/16 11:10:59 INFO yarn.ApplicationMaster: Waiting for spark context initialization
17/05/16 11:10:59 INFO yarn.ApplicationMaster: Waiting for spark context initialization ... 
17/05/16 11:10:59 INFO spark.SparkContext: Running Spark version 2.0.1
17/05/16 11:11:00 INFO spark.SecurityManager: Changing view acls to: yarn,fairy
17/05/16 11:11:00 INFO spark.SecurityManager: Changing modify acls to: yarn,fairy
17/05/16 11:11:00 INFO spark.SecurityManager: Changing view acls groups to: 
17/05/16 11:11:00 INFO spark.SecurityManager: Changing modify acls groups to: 
17/05/16 11:11:00 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(yarn, fairy); groups with view permissions: Set(); users  with modify permissions: Set(yarn, fairy); groups with modify permissions: Set()
17/05/16 11:11:00 INFO util.Utils: Successfully started service 'sparkDriver' on port 39032.
17/05/16 11:11:00 INFO spark.SparkEnv: Registering MapOutputTracker
17/05/16 11:11:00 INFO spark.SparkEnv: Registering BlockManagerMaster
17/05/16 11:11:00 INFO storage.DiskBlockManager: Created local directory at /hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/blockmgr-42cc5517-bcd4-447a-8021-4fe3861ed755
17/05/16 11:11:00 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MB
17/05/16 11:11:00 INFO spark.SparkEnv: Registering OutputCommitCoordinator
17/05/16 11:11:00 INFO util.log: Logging initialized @3037ms
17/05/16 11:11:00 INFO ui.JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
17/05/16 11:11:00 INFO server.Server: jetty-9.2.z-SNAPSHOT
17/05/16 11:11:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b2bc5f6{/jobs,null,AVAILABLE}
17/05/16 11:11:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c19b835{/jobs/json,null,AVAILABLE}
17/05/16 11:11:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@76d8bb44{/jobs/job,null,AVAILABLE}
17/05/16 11:11:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3ec843e4{/jobs/job/json,null,AVAILABLE}
17/05/16 11:11:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2b5935be{/stages,null,AVAILABLE}
17/05/16 11:11:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@aa0e8f0{/stages/json,null,AVAILABLE}
17/05/16 11:11:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3911607b{/stages/stage,null,AVAILABLE}
17/05/16 11:11:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@45bad3de{/stages/stage/json,null,AVAILABLE}
17/05/16 11:11:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@272cea12{/stages/pool,null,AVAILABLE}
17/05/16 11:11:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2a46616f{/stages/pool/json,null,AVAILABLE}
17/05/16 11:11:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@18e6dee4{/storage,null,AVAILABLE}
17/05/16 11:11:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@b51da05{/storage/json,null,AVAILABLE}
17/05/16 11:11:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b6dcd07{/storage/rdd,null,AVAILABLE}
17/05/16 11:11:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@52b9801c{/storage/rdd/json,null,AVAILABLE}
17/05/16 11:11:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7c866551{/environment,null,AVAILABLE}
17/05/16 11:11:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@79021936{/environment/json,null,AVAILABLE}
17/05/16 11:11:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ca2e6e4{/executors,null,AVAILABLE}
17/05/16 11:11:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4285240c{/executors/json,null,AVAILABLE}
17/05/16 11:11:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@d666de0{/executors/threadDump,null,AVAILABLE}
17/05/16 11:11:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64c40ed3{/executors/threadDump/json,null,AVAILABLE}
17/05/16 11:11:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f9a5169{/static,null,AVAILABLE}
17/05/16 11:11:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4454a5d2{/,null,AVAILABLE}
17/05/16 11:11:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7a63d9d5{/api,null,AVAILABLE}
17/05/16 11:11:00 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e95e2dc{/stages/stage/kill,null,AVAILABLE}
17/05/16 11:11:00 INFO server.ServerConnector: Started ServerConnector@38325bf0{HTTP/1.1}{0.0.0.0:39509}
17/05/16 11:11:00 INFO server.Server: Started @3221ms
17/05/16 11:11:00 INFO util.Utils: Successfully started service 'SparkUI' on port 39509.
17/05/16 11:11:00 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.60.43.16:39509
17/05/16 11:11:01 INFO cluster.YarnClusterScheduler: Created YarnClusterScheduler
17/05/16 11:11:01 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1494395298335_303847 and attemptId Some(appattempt_1494395298335_303847_000002)
17/05/16 11:11:01 INFO util.Utils: Using initial executors = 4, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances
17/05/16 11:11:01 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33694.
17/05/16 11:11:01 INFO netty.NettyBlockTransferService: Server created on 10.60.43.16:33694
17/05/16 11:11:01 INFO storage.BlockManager: external shuffle service port = 7337
17/05/16 11:11:01 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.60.43.16, 33694)
17/05/16 11:11:01 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.60.43.16:33694 with 1458.6 MB RAM, BlockManagerId(driver, 10.60.43.16, 33694)
17/05/16 11:11:01 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.60.43.16, 33694)
17/05/16 11:11:01 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1588a201{/metrics/json,null,AVAILABLE}
17/05/16 11:11:01 INFO scheduler.EventLoggingListener: Logging events to hdfs:///spark-history/application_1494395298335_303847_2
17/05/16 11:11:01 INFO util.Utils: Using initial executors = 4, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances
17/05/16 11:11:01 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!
17/05/16 11:11:01 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark://YarnAM@10.60.43.16:39032)
17/05/16 11:11:01 INFO client.RMProxy: Connecting to ResourceManager at c428.hadoop.gda.lo/10.60.43.28:8030
17/05/16 11:11:01 INFO yarn.YarnRMClient: Registering the ApplicationMaster
17/05/16 11:11:02 INFO util.Utils: Using initial executors = 4, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances
17/05/16 11:11:02 INFO yarn.YarnAllocator: Will request 4 executor containers, each with 1 cores and 3456 MB memory including 384 MB overhead
17/05/16 11:11:02 INFO yarn.YarnAllocator: Canceled 0 container requests (locality no longer needed)
17/05/16 11:11:02 INFO yarn.YarnAllocator: Submitted container request (host: Any, capability: <memory:3456, vCores:1>)
17/05/16 11:11:02 INFO yarn.YarnAllocator: Submitted container request (host: Any, capability: <memory:3456, vCores:1>)
17/05/16 11:11:02 INFO yarn.YarnAllocator: Submitted container request (host: Any, capability: <memory:3456, vCores:1>)
17/05/16 11:11:02 INFO yarn.YarnAllocator: Submitted container request (host: Any, capability: <memory:3456, vCores:1>)
17/05/16 11:11:02 INFO yarn.ApplicationMaster: Started progress reporter thread with (heartbeat : 5000, initial allocation : 200) intervals
17/05/16 11:11:02 INFO impl.AMRMClientImpl: Received new token for : c412.hadoop.gda.lo:45454
17/05/16 11:11:02 INFO impl.AMRMClientImpl: Received new token for : c402.hadoop.gda.lo:45454
17/05/16 11:11:02 INFO impl.AMRMClientImpl: Received new token for : c411.hadoop.gda.lo:45454
17/05/16 11:11:02 INFO impl.AMRMClientImpl: Received new token for : c419.hadoop.gda.lo:45454
17/05/16 11:11:02 INFO yarn.YarnAllocator: Launching container container_e81_1494395298335_303847_02_000002 for on host c412.hadoop.gda.lo
17/05/16 11:11:02 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@10.60.43.16:39032,  executorHostname: c412.hadoop.gda.lo
17/05/16 11:11:02 INFO yarn.YarnAllocator: Launching container container_e81_1494395298335_303847_02_000003 for on host c402.hadoop.gda.lo
17/05/16 11:11:02 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@10.60.43.16:39032,  executorHostname: c402.hadoop.gda.lo
17/05/16 11:11:02 INFO yarn.YarnAllocator: Launching container container_e81_1494395298335_303847_02_000004 for on host c411.hadoop.gda.lo
17/05/16 11:11:02 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@10.60.43.16:39032,  executorHostname: c411.hadoop.gda.lo
17/05/16 11:11:02 INFO yarn.YarnAllocator: Launching container container_e81_1494395298335_303847_02_000005 for on host c419.hadoop.gda.lo
17/05/16 11:11:02 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@10.60.43.16:39032,  executorHostname: c419.hadoop.gda.lo
17/05/16 11:11:02 INFO yarn.YarnAllocator: Received 4 containers from YARN, launching executors on 4 of them.
17/05/16 11:11:02 INFO yarn.ExecutorRunnable: Starting Executor Container
17/05/16 11:11:02 INFO yarn.ExecutorRunnable: Starting Executor Container
17/05/16 11:11:02 INFO yarn.ExecutorRunnable: Starting Executor Container
17/05/16 11:11:02 INFO yarn.ExecutorRunnable: Starting Executor Container
17/05/16 11:11:02 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
17/05/16 11:11:02 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
17/05/16 11:11:02 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
17/05/16 11:11:02 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
17/05/16 11:11:02 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
17/05/16 11:11:02 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
17/05/16 11:11:02 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
17/05/16 11:11:02 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
17/05/16 11:11:02 INFO yarn.ExecutorRunnable: 
===============================================================================
YARN executor launch context:
  env:
    CLASSPATH -> {{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark_libs__/*<CPS>$HADOOP_CONF_DIR<CPS>/usr/hdp/current/hadoop-client/*<CPS>/usr/hdp/current/hadoop-client/lib/*<CPS>/usr/hdp/current/hadoop-hdfs-client/*<CPS>/usr/hdp/current/hadoop-hdfs-client/lib/*<CPS>/usr/hdp/current/hadoop-yarn-client/*<CPS>/usr/hdp/current/hadoop-yarn-client/lib/*<CPS>$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.4.2.0-258/hadoop/lib/hadoop-lzo-0.6.0.2.4.2.0-258.jar:/etc/hadoop/conf/secure
    SPARK_LOG_URL_STDERR -> http://c412.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_02_000002/fairy/stderr?start=-4096
    SPARK_YARN_STAGING_DIR -> hdfs://c408.hadoop.gda.lo:8020/user/fairy/.sparkStaging/application_1494395298335_303847
    SPARK_USER -> fairy
    SPARK_YARN_MODE -> true
    SPARK_LOG_URL_STDOUT -> http://c412.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_02_000002/fairy/stdout?start=-4096

  command:
    {{JAVA_HOME}}/bin/java -server -Xmx3072m '-Dhdp.version=2.4.2.0-258' -Djava.io.tmpdir={{PWD}}/tmp '-Dspark.driver.port=39032' '-Dspark.history.ui.port=18080' '-Dspark.ui.port=0' -Dspark.yarn.app.container.log.dir=<LOG_DIR> -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.60.43.16:39032 --executor-id 1 --hostname c412.hadoop.gda.lo --cores 1 --app-id application_1494395298335_303847 --user-class-path file:$PWD/__app__.jar 1> <LOG_DIR>/stdout 2> <LOG_DIR>/stderr
===============================================================================
      
17/05/16 11:11:02 INFO yarn.ExecutorRunnable: 
===============================================================================
YARN executor launch context:
  env:
    CLASSPATH -> {{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark_libs__/*<CPS>$HADOOP_CONF_DIR<CPS>/usr/hdp/current/hadoop-client/*<CPS>/usr/hdp/current/hadoop-client/lib/*<CPS>/usr/hdp/current/hadoop-hdfs-client/*<CPS>/usr/hdp/current/hadoop-hdfs-client/lib/*<CPS>/usr/hdp/current/hadoop-yarn-client/*<CPS>/usr/hdp/current/hadoop-yarn-client/lib/*<CPS>$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.4.2.0-258/hadoop/lib/hadoop-lzo-0.6.0.2.4.2.0-258.jar:/etc/hadoop/conf/secure
    SPARK_LOG_URL_STDERR -> http://c419.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_02_000005/fairy/stderr?start=-4096
    SPARK_YARN_STAGING_DIR -> hdfs://c408.hadoop.gda.lo:8020/user/fairy/.sparkStaging/application_1494395298335_303847
    SPARK_USER -> fairy
    SPARK_YARN_MODE -> true
    SPARK_LOG_URL_STDOUT -> http://c419.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_02_000005/fairy/stdout?start=-4096

  command:
    {{JAVA_HOME}}/bin/java -server -Xmx3072m '-Dhdp.version=2.4.2.0-258' -Djava.io.tmpdir={{PWD}}/tmp '-Dspark.driver.port=39032' '-Dspark.history.ui.port=18080' '-Dspark.ui.port=0' -Dspark.yarn.app.container.log.dir=<LOG_DIR> -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.60.43.16:39032 --executor-id 4 --hostname c419.hadoop.gda.lo --cores 1 --app-id application_1494395298335_303847 --user-class-path file:$PWD/__app__.jar 1> <LOG_DIR>/stdout 2> <LOG_DIR>/stderr
===============================================================================
      
17/05/16 11:11:02 INFO yarn.ExecutorRunnable: 
===============================================================================
YARN executor launch context:
  env:
    CLASSPATH -> {{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark_libs__/*<CPS>$HADOOP_CONF_DIR<CPS>/usr/hdp/current/hadoop-client/*<CPS>/usr/hdp/current/hadoop-client/lib/*<CPS>/usr/hdp/current/hadoop-hdfs-client/*<CPS>/usr/hdp/current/hadoop-hdfs-client/lib/*<CPS>/usr/hdp/current/hadoop-yarn-client/*<CPS>/usr/hdp/current/hadoop-yarn-client/lib/*<CPS>$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.4.2.0-258/hadoop/lib/hadoop-lzo-0.6.0.2.4.2.0-258.jar:/etc/hadoop/conf/secure
    SPARK_LOG_URL_STDERR -> http://c402.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_02_000003/fairy/stderr?start=-4096
    SPARK_YARN_STAGING_DIR -> hdfs://c408.hadoop.gda.lo:8020/user/fairy/.sparkStaging/application_1494395298335_303847
    SPARK_USER -> fairy
    SPARK_YARN_MODE -> true
    SPARK_LOG_URL_STDOUT -> http://c402.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_02_000003/fairy/stdout?start=-4096

  command:
    {{JAVA_HOME}}/bin/java -server -Xmx3072m '-Dhdp.version=2.4.2.0-258' -Djava.io.tmpdir={{PWD}}/tmp '-Dspark.driver.port=39032' '-Dspark.history.ui.port=18080' '-Dspark.ui.port=0' -Dspark.yarn.app.container.log.dir=<LOG_DIR> -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.60.43.16:39032 --executor-id 2 --hostname c402.hadoop.gda.lo --cores 1 --app-id application_1494395298335_303847 --user-class-path file:$PWD/__app__.jar 1> <LOG_DIR>/stdout 2> <LOG_DIR>/stderr
===============================================================================
      
17/05/16 11:11:02 INFO yarn.ExecutorRunnable: 
===============================================================================
YARN executor launch context:
  env:
    CLASSPATH -> {{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark_libs__/*<CPS>$HADOOP_CONF_DIR<CPS>/usr/hdp/current/hadoop-client/*<CPS>/usr/hdp/current/hadoop-client/lib/*<CPS>/usr/hdp/current/hadoop-hdfs-client/*<CPS>/usr/hdp/current/hadoop-hdfs-client/lib/*<CPS>/usr/hdp/current/hadoop-yarn-client/*<CPS>/usr/hdp/current/hadoop-yarn-client/lib/*<CPS>$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.4.2.0-258/hadoop/lib/hadoop-lzo-0.6.0.2.4.2.0-258.jar:/etc/hadoop/conf/secure
    SPARK_LOG_URL_STDERR -> http://c411.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_02_000004/fairy/stderr?start=-4096
    SPARK_YARN_STAGING_DIR -> hdfs://c408.hadoop.gda.lo:8020/user/fairy/.sparkStaging/application_1494395298335_303847
    SPARK_USER -> fairy
    SPARK_YARN_MODE -> true
    SPARK_LOG_URL_STDOUT -> http://c411.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_02_000004/fairy/stdout?start=-4096

  command:
    {{JAVA_HOME}}/bin/java -server -Xmx3072m '-Dhdp.version=2.4.2.0-258' -Djava.io.tmpdir={{PWD}}/tmp '-Dspark.driver.port=39032' '-Dspark.history.ui.port=18080' '-Dspark.ui.port=0' -Dspark.yarn.app.container.log.dir=<LOG_DIR> -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.60.43.16:39032 --executor-id 3 --hostname c411.hadoop.gda.lo --cores 1 --app-id application_1494395298335_303847 --user-class-path file:$PWD/__app__.jar 1> <LOG_DIR>/stdout 2> <LOG_DIR>/stderr
===============================================================================
      
17/05/16 11:11:02 INFO impl.ContainerManagementProtocolProxy: Opening proxy : c419.hadoop.gda.lo:45454
17/05/16 11:11:02 INFO impl.ContainerManagementProtocolProxy: Opening proxy : c402.hadoop.gda.lo:45454
17/05/16 11:11:02 INFO impl.ContainerManagementProtocolProxy: Opening proxy : c411.hadoop.gda.lo:45454
17/05/16 11:11:02 INFO impl.ContainerManagementProtocolProxy: Opening proxy : c412.hadoop.gda.lo:45454
17/05/16 11:11:05 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(null) (10.60.43.19:22817) with ID 4
17/05/16 11:11:06 INFO storage.BlockManagerMasterEndpoint: Registering block manager c419.hadoop.gda.lo:27876 with 1458.6 MB RAM, BlockManagerId(4, c419.hadoop.gda.lo, 27876)
17/05/16 11:11:06 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(null) (10.60.43.11:48413) with ID 3
17/05/16 11:11:06 INFO storage.BlockManagerMasterEndpoint: Registering block manager c411.hadoop.gda.lo:19196 with 1458.6 MB RAM, BlockManagerId(3, c411.hadoop.gda.lo, 19196)
17/05/16 11:11:07 INFO spark.ExecutorAllocationManager: New executor 4 has registered (new total is 1)
17/05/16 11:11:07 INFO spark.ExecutorAllocationManager: New executor 3 has registered (new total is 2)
17/05/16 11:11:07 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(null) (10.60.43.2:47145) with ID 2
17/05/16 11:11:07 INFO spark.ExecutorAllocationManager: New executor 2 has registered (new total is 3)
17/05/16 11:11:07 INFO storage.BlockManagerMasterEndpoint: Registering block manager c402.hadoop.gda.lo:20069 with 1458.6 MB RAM, BlockManagerId(2, c402.hadoop.gda.lo, 20069)
17/05/16 11:11:07 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(null) (10.60.43.12:54583) with ID 1
17/05/16 11:11:07 INFO spark.ExecutorAllocationManager: New executor 1 has registered (new total is 4)
17/05/16 11:11:08 INFO cluster.YarnClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
17/05/16 11:11:08 INFO cluster.YarnClusterScheduler: YarnClusterScheduler.postStartHook done
17/05/16 11:11:08 WARN spark.SparkContext: Use an existing SparkContext, some configuration may not take effect.
17/05/16 11:11:08 INFO storage.BlockManagerMasterEndpoint: Registering block manager c412.hadoop.gda.lo:29206 with 1458.6 MB RAM, BlockManagerId(1, c412.hadoop.gda.lo, 29206)
17/05/16 11:11:08 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5b99e0ea{/SQL,null,AVAILABLE}
17/05/16 11:11:08 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4a8d6ac2{/SQL/json,null,AVAILABLE}
17/05/16 11:11:08 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@68657b99{/SQL/execution,null,AVAILABLE}
17/05/16 11:11:08 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2afbfe70{/SQL/execution/json,null,AVAILABLE}
17/05/16 11:11:08 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6cfc89a8{/static/sql,null,AVAILABLE}
17/05/16 11:11:08 INFO hive.HiveSharedState: Warehouse path is '/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_02_000001/spark-warehouse'.
17/05/16 11:11:10 INFO hive.HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17/05/16 11:11:11 INFO metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/05/16 11:11:11 INFO metastore.ObjectStore: ObjectStore, initialize called
17/05/16 11:11:11 INFO DataNucleus.Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/05/16 11:11:11 INFO DataNucleus.Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/05/16 11:11:13 INFO metastore.ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/05/16 11:11:14 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/05/16 11:11:14 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/05/16 11:11:14 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/05/16 11:11:14 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/05/16 11:11:15 INFO metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/05/16 11:11:15 INFO metastore.ObjectStore: Initialized ObjectStore
17/05/16 11:11:15 WARN metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/05/16 11:11:15 WARN metastore.ObjectStore: Failed to get database default, returning NoSuchObjectException
17/05/16 11:11:15 INFO metastore.HiveMetaStore: Added admin role in metastore
17/05/16 11:11:15 INFO metastore.HiveMetaStore: Added public role in metastore
17/05/16 11:11:15 INFO metastore.HiveMetaStore: No user is added in admin role, since config is empty
17/05/16 11:11:15 INFO metastore.HiveMetaStore: 0: get_all_databases
17/05/16 11:11:15 INFO HiveMetaStore.audit: ugi=fairy	ip=unknown-ip-addr	cmd=get_all_databases	
17/05/16 11:11:15 INFO metastore.HiveMetaStore: 0: get_functions: db=default pat=*
17/05/16 11:11:15 INFO HiveMetaStore.audit: ugi=fairy	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/05/16 11:11:15 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/05/16 11:11:15 INFO session.SessionState: Created local directory: /hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_02_000001/tmp/yarn
17/05/16 11:11:15 INFO session.SessionState: Created local directory: /hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_02_000001/tmp/236acf3d-5253-4156-844b-320b9755c465_resources
17/05/16 11:11:15 INFO session.SessionState: Created HDFS directory: /tmp/hive/fairy/236acf3d-5253-4156-844b-320b9755c465
17/05/16 11:11:15 INFO session.SessionState: Created local directory: /hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_02_000001/tmp/yarn/236acf3d-5253-4156-844b-320b9755c465
17/05/16 11:11:15 INFO session.SessionState: Created HDFS directory: /tmp/hive/fairy/236acf3d-5253-4156-844b-320b9755c465/_tmp_space.db
17/05/16 11:11:15 INFO client.HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is /hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_02_000001/spark-warehouse
17/05/16 11:11:15 INFO session.SessionState: Created local directory: /hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_02_000001/tmp/210533cb-541a-46dc-921a-592d85c96631_resources
17/05/16 11:11:15 INFO session.SessionState: Created HDFS directory: /tmp/hive/fairy/210533cb-541a-46dc-921a-592d85c96631
17/05/16 11:11:15 INFO session.SessionState: Created local directory: /hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_02_000001/tmp/yarn/210533cb-541a-46dc-921a-592d85c96631
17/05/16 11:11:15 INFO session.SessionState: Created HDFS directory: /tmp/hive/fairy/210533cb-541a-46dc-921a-592d85c96631/_tmp_space.db
17/05/16 11:11:15 INFO client.HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is /hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_02_000001/spark-warehouse
17/05/16 11:11:16 INFO metastore.HiveMetaStore: 0: create_database: Database(name:default, description:default database, locationUri:hdfs://c408.hadoop.gda.lo:8020/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_02_000001/spark-warehouse, parameters:{})
17/05/16 11:11:16 INFO HiveMetaStore.audit: ugi=fairy	ip=unknown-ip-addr	cmd=create_database: Database(name:default, description:default database, locationUri:hdfs://c408.hadoop.gda.lo:8020/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_02_000001/spark-warehouse, parameters:{})	
17/05/16 11:11:16 ERROR metastore.RetryingHMSHandler: AlreadyExistsException(message:Database default already exists)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:891)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at com.sun.proxy.$Proxy30.create_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:644)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
	at com.sun.proxy.$Proxy31.createDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Hive.java:306)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply$mcV$sp(HiveClientImpl.scala:309)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:309)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:309)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:280)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:227)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:226)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:269)
	at org.apache.spark.sql.hive.client.HiveClientImpl.createDatabase(HiveClientImpl.scala:308)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply$mcV$sp(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:72)
	at org.apache.spark.sql.hive.HiveExternalCatalog.createDatabase(HiveExternalCatalog.scala:98)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:147)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.<init>(SessionCatalog.scala:89)
	at org.apache.spark.sql.hive.HiveSessionCatalog.<init>(HiveSessionCatalog.scala:51)
	at org.apache.spark.sql.hive.HiveSessionState.catalog$lzycompute(HiveSessionState.scala:49)
	at org.apache.spark.sql.hive.HiveSessionState.catalog(HiveSessionState.scala:48)
	at org.apache.spark.sql.hive.HiveSessionState$$anon$1.<init>(HiveSessionState.scala:63)
	at org.apache.spark.sql.hive.HiveSessionState.analyzer$lzycompute(HiveSessionState.scala:63)
	at org.apache.spark.sql.hive.HiveSessionState.analyzer(HiveSessionState.scala:62)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:542)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:302)
	at vng.ge.stats.report.base.DataReader.loadFile(DataReader.scala:179)
	at vng.ge.stats.report.loader.GameDataLoader.run(GameDataLoader.scala:32)
	at vng.ge.stats.report.base.DataPool.report(DataPool.scala:44)
	at vng.ge.stats.report.job.Runner$$anonfun$run$2$$anonfun$apply$1.apply$mcV$sp(Runner.scala:76)
	at scala.util.control.Breaks.breakable(Breaks.scala:38)
	at vng.ge.stats.report.job.Runner$$anonfun$run$2.apply(Runner.scala:56)
	at vng.ge.stats.report.job.Runner$$anonfun$run$2.apply(Runner.scala:44)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at vng.ge.stats.report.job.Runner$.run(Runner.scala:44)
	at vng.ge.stats.report.job.Runner$.main(Runner.scala:19)
	at vng.ge.stats.report.job.Runner.main(Runner.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:627)

17/05/16 11:11:17 INFO spark.ContextCleaner: Cleaned accumulator 0
17/05/16 11:11:17 INFO spark.ContextCleaner: Cleaned accumulator 1
17/05/16 11:11:17 INFO codegen.CodeGenerator: Code generated in 369.185883 ms
17/05/16 11:11:17 INFO codegen.CodeGenerator: Code generated in 38.575098 ms
17/05/16 11:11:17 INFO codegen.CodeGenerator: Code generated in 66.69471 ms
17/05/16 11:11:17 INFO spark.SparkContext: Starting job: first at CcuReport.scala:39
17/05/16 11:11:17 INFO scheduler.DAGScheduler: Registering RDD 14 (first at CcuReport.scala:39)
17/05/16 11:11:17 INFO scheduler.DAGScheduler: Registering RDD 17 (first at CcuReport.scala:39)
17/05/16 11:11:17 INFO scheduler.DAGScheduler: Got job 0 (first at CcuReport.scala:39) with 1 output partitions
17/05/16 11:11:17 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (first at CcuReport.scala:39)
17/05/16 11:11:17 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
17/05/16 11:11:17 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 1)
17/05/16 11:11:17 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[17] at first at CcuReport.scala:39), which has no missing parents
17/05/16 11:11:17 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 30.3 KB, free 1458.6 MB)
17/05/16 11:11:17 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.2 KB, free 1458.6 MB)
17/05/16 11:11:17 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.60.43.16:33694 (size: 13.2 KB, free: 1458.6 MB)
17/05/16 11:11:17 INFO spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
17/05/16 11:11:17 INFO scheduler.DAGScheduler: Submitting 200 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[17] at first at CcuReport.scala:39)
17/05/16 11:11:17 INFO cluster.YarnClusterScheduler: Adding task set 1.0 with 200 tasks
17/05/16 11:11:17 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 0, c412.hadoop.gda.lo, partition 0, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:17 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 1, c419.hadoop.gda.lo, partition 1, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:18 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 1.0 (TID 2, c402.hadoop.gda.lo, partition 2, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:18 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 1.0 (TID 3, c411.hadoop.gda.lo, partition 3, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:18 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 0 on executor id: 1 hostname: c412.hadoop.gda.lo.
17/05/16 11:11:18 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 1 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:18 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 2 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:18 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 3 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:11:18 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on c419.hadoop.gda.lo:27876 (size: 13.2 KB, free: 1458.6 MB)
17/05/16 11:11:18 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on c402.hadoop.gda.lo:20069 (size: 13.2 KB, free: 1458.6 MB)
17/05/16 11:11:18 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on c412.hadoop.gda.lo:29206 (size: 13.2 KB, free: 1458.6 MB)
17/05/16 11:11:18 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on c411.hadoop.gda.lo:19196 (size: 13.2 KB, free: 1458.6 MB)
17/05/16 11:11:18 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.60.43.19:22817
17/05/16 11:11:18 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.60.43.2:47145
17/05/16 11:11:18 INFO spark.MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 83 bytes
17/05/16 11:11:18 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.60.43.12:54583
17/05/16 11:11:18 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.60.43.11:48413
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 1.0 (TID 4, c419.hadoop.gda.lo, partition 4, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 4 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 1) in 1156 ms on c419.hadoop.gda.lo (1/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 1.0 (TID 5, c419.hadoop.gda.lo, partition 5, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 5 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 1.0 (TID 4) in 40 ms on c419.hadoop.gda.lo (2/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 1.0 (TID 6, c419.hadoop.gda.lo, partition 6, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 6 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 1.0 (TID 5) in 33 ms on c419.hadoop.gda.lo (3/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 1.0 (TID 7, c419.hadoop.gda.lo, partition 7, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 7 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 1.0 (TID 6) in 27 ms on c419.hadoop.gda.lo (4/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 1.0 (TID 8, c419.hadoop.gda.lo, partition 8, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 8 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 1.0 (TID 7) in 26 ms on c419.hadoop.gda.lo (5/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 1.0 (TID 9, c419.hadoop.gda.lo, partition 9, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 9 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 1.0 (TID 8) in 22 ms on c419.hadoop.gda.lo (6/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 1.0 (TID 10, c402.hadoop.gda.lo, partition 10, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 10 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 1.0 (TID 2) in 1287 ms on c402.hadoop.gda.lo (7/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 1.0 (TID 11, c412.hadoop.gda.lo, partition 11, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 11 on executor id: 1 hostname: c412.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 0) in 1319 ms on c412.hadoop.gda.lo (8/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 1.0 (TID 12, c419.hadoop.gda.lo, partition 12, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 12 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 1.0 (TID 9) in 19 ms on c419.hadoop.gda.lo (9/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 1.0 (TID 13, c419.hadoop.gda.lo, partition 13, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 13 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 1.0 (TID 12) in 19 ms on c419.hadoop.gda.lo (10/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 1.0 (TID 14, c402.hadoop.gda.lo, partition 14, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 14 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 1.0 (TID 10) in 37 ms on c402.hadoop.gda.lo (11/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 1.0 (TID 15, c419.hadoop.gda.lo, partition 15, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 15 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 1.0 (TID 13) in 19 ms on c419.hadoop.gda.lo (12/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 16.0 in stage 1.0 (TID 16, c412.hadoop.gda.lo, partition 16, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 16 on executor id: 1 hostname: c412.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 1.0 (TID 11) in 48 ms on c412.hadoop.gda.lo (13/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 17.0 in stage 1.0 (TID 17, c402.hadoop.gda.lo, partition 17, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 17 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 1.0 (TID 14) in 31 ms on c402.hadoop.gda.lo (14/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 18.0 in stage 1.0 (TID 18, c419.hadoop.gda.lo, partition 18, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 18 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 1.0 (TID 15) in 19 ms on c419.hadoop.gda.lo (15/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 19.0 in stage 1.0 (TID 19, c412.hadoop.gda.lo, partition 19, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 19 on executor id: 1 hostname: c412.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 16.0 in stage 1.0 (TID 16) in 28 ms on c412.hadoop.gda.lo (16/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 20.0 in stage 1.0 (TID 20, c419.hadoop.gda.lo, partition 20, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 20 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 18.0 in stage 1.0 (TID 18) in 18 ms on c419.hadoop.gda.lo (17/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 21.0 in stage 1.0 (TID 21, c402.hadoop.gda.lo, partition 21, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 21 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 17.0 in stage 1.0 (TID 17) in 28 ms on c402.hadoop.gda.lo (18/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 22.0 in stage 1.0 (TID 22, c419.hadoop.gda.lo, partition 22, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 22 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 20.0 in stage 1.0 (TID 20) in 16 ms on c419.hadoop.gda.lo (19/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 23.0 in stage 1.0 (TID 23, c412.hadoop.gda.lo, partition 23, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 23 on executor id: 1 hostname: c412.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 19.0 in stage 1.0 (TID 19) in 27 ms on c412.hadoop.gda.lo (20/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 24.0 in stage 1.0 (TID 24, c419.hadoop.gda.lo, partition 24, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 24 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 22.0 in stage 1.0 (TID 22) in 14 ms on c419.hadoop.gda.lo (21/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 25.0 in stage 1.0 (TID 25, c402.hadoop.gda.lo, partition 25, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 25 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 21.0 in stage 1.0 (TID 21) in 25 ms on c402.hadoop.gda.lo (22/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 26.0 in stage 1.0 (TID 26, c412.hadoop.gda.lo, partition 26, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 26 on executor id: 1 hostname: c412.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 23.0 in stage 1.0 (TID 23) in 21 ms on c412.hadoop.gda.lo (23/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 27.0 in stage 1.0 (TID 27, c419.hadoop.gda.lo, partition 27, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 27 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 24.0 in stage 1.0 (TID 24) in 15 ms on c419.hadoop.gda.lo (24/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 28.0 in stage 1.0 (TID 28, c402.hadoop.gda.lo, partition 28, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 28 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 25.0 in stage 1.0 (TID 25) in 22 ms on c402.hadoop.gda.lo (25/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 29.0 in stage 1.0 (TID 29, c419.hadoop.gda.lo, partition 29, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 29 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 27.0 in stage 1.0 (TID 27) in 16 ms on c419.hadoop.gda.lo (26/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 30.0 in stage 1.0 (TID 30, c412.hadoop.gda.lo, partition 30, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 30 on executor id: 1 hostname: c412.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 26.0 in stage 1.0 (TID 26) in 28 ms on c412.hadoop.gda.lo (27/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 31.0 in stage 1.0 (TID 31, c402.hadoop.gda.lo, partition 31, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 31 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 28.0 in stage 1.0 (TID 28) in 23 ms on c402.hadoop.gda.lo (28/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 32.0 in stage 1.0 (TID 32, c419.hadoop.gda.lo, partition 32, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 32 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 29.0 in stage 1.0 (TID 29) in 23 ms on c419.hadoop.gda.lo (29/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 33.0 in stage 1.0 (TID 33, c412.hadoop.gda.lo, partition 33, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 33 on executor id: 1 hostname: c412.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 30.0 in stage 1.0 (TID 30) in 25 ms on c412.hadoop.gda.lo (30/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 34.0 in stage 1.0 (TID 34, c402.hadoop.gda.lo, partition 34, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 34 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 31.0 in stage 1.0 (TID 31) in 19 ms on c402.hadoop.gda.lo (31/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 35.0 in stage 1.0 (TID 35, c419.hadoop.gda.lo, partition 35, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 35 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 32.0 in stage 1.0 (TID 32) in 15 ms on c419.hadoop.gda.lo (32/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 36.0 in stage 1.0 (TID 36, c402.hadoop.gda.lo, partition 36, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 36 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 34.0 in stage 1.0 (TID 34) in 17 ms on c402.hadoop.gda.lo (33/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 37.0 in stage 1.0 (TID 37, c419.hadoop.gda.lo, partition 37, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 37 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 35.0 in stage 1.0 (TID 35) in 18 ms on c419.hadoop.gda.lo (34/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 38.0 in stage 1.0 (TID 38, c412.hadoop.gda.lo, partition 38, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 38 on executor id: 1 hostname: c412.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 33.0 in stage 1.0 (TID 33) in 24 ms on c412.hadoop.gda.lo (35/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 39.0 in stage 1.0 (TID 39, c419.hadoop.gda.lo, partition 39, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 39 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 37.0 in stage 1.0 (TID 37) in 16 ms on c419.hadoop.gda.lo (36/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 40.0 in stage 1.0 (TID 40, c402.hadoop.gda.lo, partition 40, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 40 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 36.0 in stage 1.0 (TID 36) in 20 ms on c402.hadoop.gda.lo (37/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 41.0 in stage 1.0 (TID 41, c412.hadoop.gda.lo, partition 41, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 41 on executor id: 1 hostname: c412.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 38.0 in stage 1.0 (TID 38) in 20 ms on c412.hadoop.gda.lo (38/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 42.0 in stage 1.0 (TID 42, c419.hadoop.gda.lo, partition 42, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 42 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 39.0 in stage 1.0 (TID 39) in 15 ms on c419.hadoop.gda.lo (39/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 43.0 in stage 1.0 (TID 43, c402.hadoop.gda.lo, partition 43, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 43 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 40.0 in stage 1.0 (TID 40) in 17 ms on c402.hadoop.gda.lo (40/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 44.0 in stage 1.0 (TID 44, c411.hadoop.gda.lo, partition 44, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 44 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 1.0 (TID 3) in 1524 ms on c411.hadoop.gda.lo (41/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 45.0 in stage 1.0 (TID 45, c419.hadoop.gda.lo, partition 45, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 45 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 42.0 in stage 1.0 (TID 42) in 14 ms on c419.hadoop.gda.lo (42/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 46.0 in stage 1.0 (TID 46, c402.hadoop.gda.lo, partition 46, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 46 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 43.0 in stage 1.0 (TID 43) in 18 ms on c402.hadoop.gda.lo (43/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 47.0 in stage 1.0 (TID 47, c419.hadoop.gda.lo, partition 47, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 47 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 45.0 in stage 1.0 (TID 45) in 16 ms on c419.hadoop.gda.lo (44/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 48.0 in stage 1.0 (TID 48, c402.hadoop.gda.lo, partition 48, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 48 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 46.0 in stage 1.0 (TID 46) in 22 ms on c402.hadoop.gda.lo (45/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 49.0 in stage 1.0 (TID 49, c419.hadoop.gda.lo, partition 49, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 49 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 47.0 in stage 1.0 (TID 47) in 18 ms on c419.hadoop.gda.lo (46/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 50.0 in stage 1.0 (TID 50, c411.hadoop.gda.lo, partition 50, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 50 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 44.0 in stage 1.0 (TID 44) in 45 ms on c411.hadoop.gda.lo (47/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 51.0 in stage 1.0 (TID 51, c402.hadoop.gda.lo, partition 51, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 51 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 48.0 in stage 1.0 (TID 48) in 19 ms on c402.hadoop.gda.lo (48/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 52.0 in stage 1.0 (TID 52, c419.hadoop.gda.lo, partition 52, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 52 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 49.0 in stage 1.0 (TID 49) in 19 ms on c419.hadoop.gda.lo (49/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 53.0 in stage 1.0 (TID 53, c402.hadoop.gda.lo, partition 53, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 53 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 51.0 in stage 1.0 (TID 51) in 19 ms on c402.hadoop.gda.lo (50/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 54.0 in stage 1.0 (TID 54, c419.hadoop.gda.lo, partition 54, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 54 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 52.0 in stage 1.0 (TID 52) in 18 ms on c419.hadoop.gda.lo (51/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 55.0 in stage 1.0 (TID 55, c411.hadoop.gda.lo, partition 55, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 55 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 50.0 in stage 1.0 (TID 50) in 27 ms on c411.hadoop.gda.lo (52/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 56.0 in stage 1.0 (TID 56, c402.hadoop.gda.lo, partition 56, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 56 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 53.0 in stage 1.0 (TID 53) in 21 ms on c402.hadoop.gda.lo (53/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 57.0 in stage 1.0 (TID 57, c419.hadoop.gda.lo, partition 57, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 57 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 54.0 in stage 1.0 (TID 54) in 19 ms on c419.hadoop.gda.lo (54/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 58.0 in stage 1.0 (TID 58, c411.hadoop.gda.lo, partition 58, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 58 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 55.0 in stage 1.0 (TID 55) in 24 ms on c411.hadoop.gda.lo (55/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 59.0 in stage 1.0 (TID 59, c402.hadoop.gda.lo, partition 59, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 59 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 56.0 in stage 1.0 (TID 56) in 15 ms on c402.hadoop.gda.lo (56/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 60.0 in stage 1.0 (TID 60, c419.hadoop.gda.lo, partition 60, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 60 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 57.0 in stage 1.0 (TID 57) in 16 ms on c419.hadoop.gda.lo (57/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 61.0 in stage 1.0 (TID 61, c411.hadoop.gda.lo, partition 61, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 61 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 58.0 in stage 1.0 (TID 58) in 21 ms on c411.hadoop.gda.lo (58/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 62.0 in stage 1.0 (TID 62, c402.hadoop.gda.lo, partition 62, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 62 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 59.0 in stage 1.0 (TID 59) in 17 ms on c402.hadoop.gda.lo (59/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 63.0 in stage 1.0 (TID 63, c419.hadoop.gda.lo, partition 63, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 63 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 60.0 in stage 1.0 (TID 60) in 22 ms on c419.hadoop.gda.lo (60/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 64.0 in stage 1.0 (TID 64, c411.hadoop.gda.lo, partition 64, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 64 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 61.0 in stage 1.0 (TID 61) in 21 ms on c411.hadoop.gda.lo (61/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 65.0 in stage 1.0 (TID 65, c402.hadoop.gda.lo, partition 65, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 65 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 62.0 in stage 1.0 (TID 62) in 19 ms on c402.hadoop.gda.lo (62/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 66.0 in stage 1.0 (TID 66, c419.hadoop.gda.lo, partition 66, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 66 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 63.0 in stage 1.0 (TID 63) in 19 ms on c419.hadoop.gda.lo (63/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 67.0 in stage 1.0 (TID 67, c402.hadoop.gda.lo, partition 67, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 67 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 65.0 in stage 1.0 (TID 65) in 19 ms on c402.hadoop.gda.lo (64/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 68.0 in stage 1.0 (TID 68, c411.hadoop.gda.lo, partition 68, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 68 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 64.0 in stage 1.0 (TID 64) in 21 ms on c411.hadoop.gda.lo (65/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 69.0 in stage 1.0 (TID 69, c419.hadoop.gda.lo, partition 69, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 69 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 66.0 in stage 1.0 (TID 66) in 18 ms on c419.hadoop.gda.lo (66/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 70.0 in stage 1.0 (TID 70, c402.hadoop.gda.lo, partition 70, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 70 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 67.0 in stage 1.0 (TID 67) in 19 ms on c402.hadoop.gda.lo (67/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 71.0 in stage 1.0 (TID 71, c411.hadoop.gda.lo, partition 71, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 71 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 68.0 in stage 1.0 (TID 68) in 20 ms on c411.hadoop.gda.lo (68/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 72.0 in stage 1.0 (TID 72, c412.hadoop.gda.lo, partition 72, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 72 on executor id: 1 hostname: c412.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 41.0 in stage 1.0 (TID 41) in 196 ms on c412.hadoop.gda.lo (69/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 73.0 in stage 1.0 (TID 73, c419.hadoop.gda.lo, partition 73, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 73 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 69.0 in stage 1.0 (TID 69) in 19 ms on c419.hadoop.gda.lo (70/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 74.0 in stage 1.0 (TID 74, c402.hadoop.gda.lo, partition 74, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 74 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 70.0 in stage 1.0 (TID 70) in 19 ms on c402.hadoop.gda.lo (71/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 75.0 in stage 1.0 (TID 75, c411.hadoop.gda.lo, partition 75, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 75 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 71.0 in stage 1.0 (TID 71) in 20 ms on c411.hadoop.gda.lo (72/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 76.0 in stage 1.0 (TID 76, c419.hadoop.gda.lo, partition 76, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 76 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 73.0 in stage 1.0 (TID 73) in 16 ms on c419.hadoop.gda.lo (73/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 77.0 in stage 1.0 (TID 77, c412.hadoop.gda.lo, partition 77, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 77 on executor id: 1 hostname: c412.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 72.0 in stage 1.0 (TID 72) in 25 ms on c412.hadoop.gda.lo (74/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 78.0 in stage 1.0 (TID 78, c402.hadoop.gda.lo, partition 78, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 78 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 74.0 in stage 1.0 (TID 74) in 18 ms on c402.hadoop.gda.lo (75/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 79.0 in stage 1.0 (TID 79, c419.hadoop.gda.lo, partition 79, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 79 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 76.0 in stage 1.0 (TID 76) in 16 ms on c419.hadoop.gda.lo (76/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 80.0 in stage 1.0 (TID 80, c412.hadoop.gda.lo, partition 80, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 80 on executor id: 1 hostname: c412.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 77.0 in stage 1.0 (TID 77) in 25 ms on c412.hadoop.gda.lo (77/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 81.0 in stage 1.0 (TID 81, c411.hadoop.gda.lo, partition 81, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 81 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 75.0 in stage 1.0 (TID 75) in 31 ms on c411.hadoop.gda.lo (78/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 82.0 in stage 1.0 (TID 82, c402.hadoop.gda.lo, partition 82, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 82 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 78.0 in stage 1.0 (TID 78) in 16 ms on c402.hadoop.gda.lo (79/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 83.0 in stage 1.0 (TID 83, c419.hadoop.gda.lo, partition 83, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 83 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 79.0 in stage 1.0 (TID 79) in 16 ms on c419.hadoop.gda.lo (80/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 84.0 in stage 1.0 (TID 84, c402.hadoop.gda.lo, partition 84, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 84 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 82.0 in stage 1.0 (TID 82) in 15 ms on c402.hadoop.gda.lo (81/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 85.0 in stage 1.0 (TID 85, c419.hadoop.gda.lo, partition 85, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 85 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 83.0 in stage 1.0 (TID 83) in 18 ms on c419.hadoop.gda.lo (82/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 86.0 in stage 1.0 (TID 86, c411.hadoop.gda.lo, partition 86, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 86 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 81.0 in stage 1.0 (TID 81) in 20 ms on c411.hadoop.gda.lo (83/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 87.0 in stage 1.0 (TID 87, c412.hadoop.gda.lo, partition 87, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 87 on executor id: 1 hostname: c412.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 80.0 in stage 1.0 (TID 80) in 27 ms on c412.hadoop.gda.lo (84/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 88.0 in stage 1.0 (TID 88, c402.hadoop.gda.lo, partition 88, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 88 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 84.0 in stage 1.0 (TID 84) in 17 ms on c402.hadoop.gda.lo (85/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 89.0 in stage 1.0 (TID 89, c419.hadoop.gda.lo, partition 89, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 89 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 85.0 in stage 1.0 (TID 85) in 16 ms on c419.hadoop.gda.lo (86/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 90.0 in stage 1.0 (TID 90, c411.hadoop.gda.lo, partition 90, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 90 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 86.0 in stage 1.0 (TID 86) in 22 ms on c411.hadoop.gda.lo (87/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 91.0 in stage 1.0 (TID 91, c412.hadoop.gda.lo, partition 91, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 91 on executor id: 1 hostname: c412.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 87.0 in stage 1.0 (TID 87) in 18 ms on c412.hadoop.gda.lo (88/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 92.0 in stage 1.0 (TID 92, c402.hadoop.gda.lo, partition 92, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 92 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 88.0 in stage 1.0 (TID 88) in 14 ms on c402.hadoop.gda.lo (89/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 93.0 in stage 1.0 (TID 93, c419.hadoop.gda.lo, partition 93, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 93 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 89.0 in stage 1.0 (TID 89) in 16 ms on c419.hadoop.gda.lo (90/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 94.0 in stage 1.0 (TID 94, c402.hadoop.gda.lo, partition 94, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 94 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 92.0 in stage 1.0 (TID 92) in 14 ms on c402.hadoop.gda.lo (91/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 95.0 in stage 1.0 (TID 95, c412.hadoop.gda.lo, partition 95, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 95 on executor id: 1 hostname: c412.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 91.0 in stage 1.0 (TID 91) in 19 ms on c412.hadoop.gda.lo (92/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 96.0 in stage 1.0 (TID 96, c411.hadoop.gda.lo, partition 96, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 96 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 90.0 in stage 1.0 (TID 90) in 25 ms on c411.hadoop.gda.lo (93/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 97.0 in stage 1.0 (TID 97, c419.hadoop.gda.lo, partition 97, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 97 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 93.0 in stage 1.0 (TID 93) in 23 ms on c419.hadoop.gda.lo (94/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 98.0 in stage 1.0 (TID 98, c402.hadoop.gda.lo, partition 98, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 98 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 94.0 in stage 1.0 (TID 94) in 16 ms on c402.hadoop.gda.lo (95/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 99.0 in stage 1.0 (TID 99, c412.hadoop.gda.lo, partition 99, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 99 on executor id: 1 hostname: c412.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 95.0 in stage 1.0 (TID 95) in 24 ms on c412.hadoop.gda.lo (96/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 100.0 in stage 1.0 (TID 100, c419.hadoop.gda.lo, partition 100, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 100 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 97.0 in stage 1.0 (TID 97) in 13 ms on c419.hadoop.gda.lo (97/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 101.0 in stage 1.0 (TID 101, c402.hadoop.gda.lo, partition 101, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 101 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 98.0 in stage 1.0 (TID 98) in 16 ms on c402.hadoop.gda.lo (98/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 102.0 in stage 1.0 (TID 102, c411.hadoop.gda.lo, partition 102, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 102 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 96.0 in stage 1.0 (TID 96) in 26 ms on c411.hadoop.gda.lo (99/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 103.0 in stage 1.0 (TID 103, c419.hadoop.gda.lo, partition 103, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 103 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 100.0 in stage 1.0 (TID 100) in 14 ms on c419.hadoop.gda.lo (100/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 104.0 in stage 1.0 (TID 104, c412.hadoop.gda.lo, partition 104, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 104 on executor id: 1 hostname: c412.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 99.0 in stage 1.0 (TID 99) in 16 ms on c412.hadoop.gda.lo (101/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 105.0 in stage 1.0 (TID 105, c402.hadoop.gda.lo, partition 105, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 105 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 101.0 in stage 1.0 (TID 101) in 16 ms on c402.hadoop.gda.lo (102/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 106.0 in stage 1.0 (TID 106, c419.hadoop.gda.lo, partition 106, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 106 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 103.0 in stage 1.0 (TID 103) in 16 ms on c419.hadoop.gda.lo (103/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 107.0 in stage 1.0 (TID 107, c412.hadoop.gda.lo, partition 107, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 107 on executor id: 1 hostname: c412.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 104.0 in stage 1.0 (TID 104) in 16 ms on c412.hadoop.gda.lo (104/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 108.0 in stage 1.0 (TID 108, c402.hadoop.gda.lo, partition 108, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 108 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 105.0 in stage 1.0 (TID 105) in 17 ms on c402.hadoop.gda.lo (105/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 109.0 in stage 1.0 (TID 109, c419.hadoop.gda.lo, partition 109, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 109 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 106.0 in stage 1.0 (TID 106) in 15 ms on c419.hadoop.gda.lo (106/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 110.0 in stage 1.0 (TID 110, c411.hadoop.gda.lo, partition 110, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 110 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 102.0 in stage 1.0 (TID 102) in 40 ms on c411.hadoop.gda.lo (107/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 111.0 in stage 1.0 (TID 111, c412.hadoop.gda.lo, partition 111, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 111 on executor id: 1 hostname: c412.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 107.0 in stage 1.0 (TID 107) in 16 ms on c412.hadoop.gda.lo (108/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 112.0 in stage 1.0 (TID 112, c402.hadoop.gda.lo, partition 112, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 112 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 108.0 in stage 1.0 (TID 108) in 14 ms on c402.hadoop.gda.lo (109/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 113.0 in stage 1.0 (TID 113, c419.hadoop.gda.lo, partition 113, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 113 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 109.0 in stage 1.0 (TID 109) in 17 ms on c419.hadoop.gda.lo (110/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 114.0 in stage 1.0 (TID 114, c412.hadoop.gda.lo, partition 114, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 114 on executor id: 1 hostname: c412.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 111.0 in stage 1.0 (TID 111) in 16 ms on c412.hadoop.gda.lo (111/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 115.0 in stage 1.0 (TID 115, c402.hadoop.gda.lo, partition 115, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 115 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 112.0 in stage 1.0 (TID 112) in 14 ms on c402.hadoop.gda.lo (112/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 116.0 in stage 1.0 (TID 116, c419.hadoop.gda.lo, partition 116, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 116 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 113.0 in stage 1.0 (TID 113) in 16 ms on c419.hadoop.gda.lo (113/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 117.0 in stage 1.0 (TID 117, c402.hadoop.gda.lo, partition 117, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 117 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 115.0 in stage 1.0 (TID 115) in 14 ms on c402.hadoop.gda.lo (114/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 118.0 in stage 1.0 (TID 118, c412.hadoop.gda.lo, partition 118, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 118 on executor id: 1 hostname: c412.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 114.0 in stage 1.0 (TID 114) in 20 ms on c412.hadoop.gda.lo (115/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 119.0 in stage 1.0 (TID 119, c411.hadoop.gda.lo, partition 119, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 119 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 110.0 in stage 1.0 (TID 110) in 37 ms on c411.hadoop.gda.lo (116/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 120.0 in stage 1.0 (TID 120, c419.hadoop.gda.lo, partition 120, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 120 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 116.0 in stage 1.0 (TID 116) in 14 ms on c419.hadoop.gda.lo (117/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 121.0 in stage 1.0 (TID 121, c402.hadoop.gda.lo, partition 121, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 121 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 117.0 in stage 1.0 (TID 117) in 12 ms on c402.hadoop.gda.lo (118/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 122.0 in stage 1.0 (TID 122, c412.hadoop.gda.lo, partition 122, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 122 on executor id: 1 hostname: c412.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 118.0 in stage 1.0 (TID 118) in 21 ms on c412.hadoop.gda.lo (119/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 123.0 in stage 1.0 (TID 123, c402.hadoop.gda.lo, partition 123, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 123 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 121.0 in stage 1.0 (TID 121) in 14 ms on c402.hadoop.gda.lo (120/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 124.0 in stage 1.0 (TID 124, c411.hadoop.gda.lo, partition 124, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 124 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 119.0 in stage 1.0 (TID 119) in 24 ms on c411.hadoop.gda.lo (121/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 125.0 in stage 1.0 (TID 125, c419.hadoop.gda.lo, partition 125, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 125 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 120.0 in stage 1.0 (TID 120) in 18 ms on c419.hadoop.gda.lo (122/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 126.0 in stage 1.0 (TID 126, c402.hadoop.gda.lo, partition 126, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 126 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 123.0 in stage 1.0 (TID 123) in 13 ms on c402.hadoop.gda.lo (123/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 127.0 in stage 1.0 (TID 127, c412.hadoop.gda.lo, partition 127, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 127 on executor id: 1 hostname: c412.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 122.0 in stage 1.0 (TID 122) in 20 ms on c412.hadoop.gda.lo (124/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 128.0 in stage 1.0 (TID 128, c419.hadoop.gda.lo, partition 128, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 128 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 125.0 in stage 1.0 (TID 125) in 16 ms on c419.hadoop.gda.lo (125/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 129.0 in stage 1.0 (TID 129, c411.hadoop.gda.lo, partition 129, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 129 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 124.0 in stage 1.0 (TID 124) in 23 ms on c411.hadoop.gda.lo (126/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 130.0 in stage 1.0 (TID 130, c402.hadoop.gda.lo, partition 130, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 130 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 126.0 in stage 1.0 (TID 126) in 13 ms on c402.hadoop.gda.lo (127/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 131.0 in stage 1.0 (TID 131, c419.hadoop.gda.lo, partition 131, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 131 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 128.0 in stage 1.0 (TID 128) in 12 ms on c419.hadoop.gda.lo (128/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 132.0 in stage 1.0 (TID 132, c412.hadoop.gda.lo, partition 132, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 132 on executor id: 1 hostname: c412.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 127.0 in stage 1.0 (TID 127) in 20 ms on c412.hadoop.gda.lo (129/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 133.0 in stage 1.0 (TID 133, c402.hadoop.gda.lo, partition 133, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 133 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 130.0 in stage 1.0 (TID 130) in 12 ms on c402.hadoop.gda.lo (130/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 134.0 in stage 1.0 (TID 134, c419.hadoop.gda.lo, partition 134, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 134 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 131.0 in stage 1.0 (TID 131) in 13 ms on c419.hadoop.gda.lo (131/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 135.0 in stage 1.0 (TID 135, c411.hadoop.gda.lo, partition 135, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 135 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 129.0 in stage 1.0 (TID 129) in 22 ms on c411.hadoop.gda.lo (132/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 136.0 in stage 1.0 (TID 136, c402.hadoop.gda.lo, partition 136, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 136 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 133.0 in stage 1.0 (TID 133) in 11 ms on c402.hadoop.gda.lo (133/200)
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Starting task 137.0 in stage 1.0 (TID 137, c412.hadoop.gda.lo, partition 137, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 137 on executor id: 1 hostname: c412.hadoop.gda.lo.
17/05/16 11:11:19 INFO scheduler.TaskSetManager: Finished task 132.0 in stage 1.0 (TID 132) in 19 ms on c412.hadoop.gda.lo (134/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 138.0 in stage 1.0 (TID 138, c419.hadoop.gda.lo, partition 138, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 138 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 134.0 in stage 1.0 (TID 134) in 19 ms on c419.hadoop.gda.lo (135/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 139.0 in stage 1.0 (TID 139, c411.hadoop.gda.lo, partition 139, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 139 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 135.0 in stage 1.0 (TID 135) in 21 ms on c411.hadoop.gda.lo (136/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 140.0 in stage 1.0 (TID 140, c402.hadoop.gda.lo, partition 140, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 140 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 136.0 in stage 1.0 (TID 136) in 22 ms on c402.hadoop.gda.lo (137/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 141.0 in stage 1.0 (TID 141, c412.hadoop.gda.lo, partition 141, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 141 on executor id: 1 hostname: c412.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 137.0 in stage 1.0 (TID 137) in 18 ms on c412.hadoop.gda.lo (138/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 142.0 in stage 1.0 (TID 142, c419.hadoop.gda.lo, partition 142, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 142 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 138.0 in stage 1.0 (TID 138) in 22 ms on c419.hadoop.gda.lo (139/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 143.0 in stage 1.0 (TID 143, c411.hadoop.gda.lo, partition 143, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 143 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 139.0 in stage 1.0 (TID 139) in 22 ms on c411.hadoop.gda.lo (140/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 144.0 in stage 1.0 (TID 144, c412.hadoop.gda.lo, partition 144, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 144 on executor id: 1 hostname: c412.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 141.0 in stage 1.0 (TID 141) in 18 ms on c412.hadoop.gda.lo (141/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 145.0 in stage 1.0 (TID 145, c402.hadoop.gda.lo, partition 145, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 145 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 140.0 in stage 1.0 (TID 140) in 23 ms on c402.hadoop.gda.lo (142/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 146.0 in stage 1.0 (TID 146, c412.hadoop.gda.lo, partition 146, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 146 on executor id: 1 hostname: c412.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 144.0 in stage 1.0 (TID 144) in 13 ms on c412.hadoop.gda.lo (143/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 147.0 in stage 1.0 (TID 147, c402.hadoop.gda.lo, partition 147, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 147 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 145.0 in stage 1.0 (TID 145) in 13 ms on c402.hadoop.gda.lo (144/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 148.0 in stage 1.0 (TID 148, c411.hadoop.gda.lo, partition 148, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 148 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 143.0 in stage 1.0 (TID 143) in 19 ms on c411.hadoop.gda.lo (145/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 149.0 in stage 1.0 (TID 149, c419.hadoop.gda.lo, partition 149, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 149 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 142.0 in stage 1.0 (TID 142) in 27 ms on c419.hadoop.gda.lo (146/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 150.0 in stage 1.0 (TID 150, c412.hadoop.gda.lo, partition 150, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 150 on executor id: 1 hostname: c412.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 146.0 in stage 1.0 (TID 146) in 13 ms on c412.hadoop.gda.lo (147/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 151.0 in stage 1.0 (TID 151, c419.hadoop.gda.lo, partition 151, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 151 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 149.0 in stage 1.0 (TID 149) in 16 ms on c419.hadoop.gda.lo (148/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 152.0 in stage 1.0 (TID 152, c412.hadoop.gda.lo, partition 152, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 152 on executor id: 1 hostname: c412.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 150.0 in stage 1.0 (TID 150) in 13 ms on c412.hadoop.gda.lo (149/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 153.0 in stage 1.0 (TID 153, c411.hadoop.gda.lo, partition 153, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 153 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 148.0 in stage 1.0 (TID 148) in 21 ms on c411.hadoop.gda.lo (150/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 154.0 in stage 1.0 (TID 154, c402.hadoop.gda.lo, partition 154, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 154 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 147.0 in stage 1.0 (TID 147) in 23 ms on c402.hadoop.gda.lo (151/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 155.0 in stage 1.0 (TID 155, c419.hadoop.gda.lo, partition 155, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 155 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 151.0 in stage 1.0 (TID 151) in 15 ms on c419.hadoop.gda.lo (152/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 156.0 in stage 1.0 (TID 156, c412.hadoop.gda.lo, partition 156, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 156 on executor id: 1 hostname: c412.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 152.0 in stage 1.0 (TID 152) in 12 ms on c412.hadoop.gda.lo (153/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 157.0 in stage 1.0 (TID 157, c402.hadoop.gda.lo, partition 157, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 157 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 158.0 in stage 1.0 (TID 158, c419.hadoop.gda.lo, partition 158, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 158 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 154.0 in stage 1.0 (TID 154) in 25 ms on c402.hadoop.gda.lo (154/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 155.0 in stage 1.0 (TID 155) in 15 ms on c419.hadoop.gda.lo (155/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 159.0 in stage 1.0 (TID 159, c412.hadoop.gda.lo, partition 159, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 156.0 in stage 1.0 (TID 156) in 14 ms on c412.hadoop.gda.lo (156/200)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 159 on executor id: 1 hostname: c412.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 160.0 in stage 1.0 (TID 160, c411.hadoop.gda.lo, partition 160, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 160 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 153.0 in stage 1.0 (TID 153) in 27 ms on c411.hadoop.gda.lo (157/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 161.0 in stage 1.0 (TID 161, c419.hadoop.gda.lo, partition 161, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 161 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 158.0 in stage 1.0 (TID 158) in 14 ms on c419.hadoop.gda.lo (158/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 162.0 in stage 1.0 (TID 162, c412.hadoop.gda.lo, partition 162, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 162 on executor id: 1 hostname: c412.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 159.0 in stage 1.0 (TID 159) in 14 ms on c412.hadoop.gda.lo (159/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 163.0 in stage 1.0 (TID 163, c402.hadoop.gda.lo, partition 163, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 163 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 157.0 in stage 1.0 (TID 157) in 17 ms on c402.hadoop.gda.lo (160/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 164.0 in stage 1.0 (TID 164, c411.hadoop.gda.lo, partition 164, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 164 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 160.0 in stage 1.0 (TID 160) in 17 ms on c411.hadoop.gda.lo (161/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 165.0 in stage 1.0 (TID 165, c419.hadoop.gda.lo, partition 165, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 165 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 161.0 in stage 1.0 (TID 161) in 13 ms on c419.hadoop.gda.lo (162/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 166.0 in stage 1.0 (TID 166, c412.hadoop.gda.lo, partition 166, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 166 on executor id: 1 hostname: c412.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 162.0 in stage 1.0 (TID 162) in 13 ms on c412.hadoop.gda.lo (163/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 167.0 in stage 1.0 (TID 167, c402.hadoop.gda.lo, partition 167, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 167 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 163.0 in stage 1.0 (TID 163) in 19 ms on c402.hadoop.gda.lo (164/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 168.0 in stage 1.0 (TID 168, c419.hadoop.gda.lo, partition 168, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 168 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 165.0 in stage 1.0 (TID 165) in 10 ms on c419.hadoop.gda.lo (165/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 169.0 in stage 1.0 (TID 169, c412.hadoop.gda.lo, partition 169, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 169 on executor id: 1 hostname: c412.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 166.0 in stage 1.0 (TID 166) in 11 ms on c412.hadoop.gda.lo (166/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 170.0 in stage 1.0 (TID 170, c411.hadoop.gda.lo, partition 170, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 170 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 164.0 in stage 1.0 (TID 164) in 24 ms on c411.hadoop.gda.lo (167/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 171.0 in stage 1.0 (TID 171, c419.hadoop.gda.lo, partition 171, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 171 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 168.0 in stage 1.0 (TID 168) in 11 ms on c419.hadoop.gda.lo (168/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 172.0 in stage 1.0 (TID 172, c402.hadoop.gda.lo, partition 172, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 172 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 167.0 in stage 1.0 (TID 167) in 16 ms on c402.hadoop.gda.lo (169/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 173.0 in stage 1.0 (TID 173, c412.hadoop.gda.lo, partition 173, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 173 on executor id: 1 hostname: c412.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 169.0 in stage 1.0 (TID 169) in 13 ms on c412.hadoop.gda.lo (170/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 174.0 in stage 1.0 (TID 174, c419.hadoop.gda.lo, partition 174, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 174 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 171.0 in stage 1.0 (TID 171) in 10 ms on c419.hadoop.gda.lo (171/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 175.0 in stage 1.0 (TID 175, c411.hadoop.gda.lo, partition 175, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 175 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 170.0 in stage 1.0 (TID 170) in 17 ms on c411.hadoop.gda.lo (172/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 176.0 in stage 1.0 (TID 176, c402.hadoop.gda.lo, partition 176, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 176 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 172.0 in stage 1.0 (TID 172) in 15 ms on c402.hadoop.gda.lo (173/200)
17/05/16 11:11:20 INFO yarn.YarnAllocator: Driver requested a total number of 5 executor(s).
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 177.0 in stage 1.0 (TID 177, c412.hadoop.gda.lo, partition 177, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 177 on executor id: 1 hostname: c412.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 173.0 in stage 1.0 (TID 173) in 16 ms on c412.hadoop.gda.lo (174/200)
17/05/16 11:11:20 INFO yarn.YarnAllocator: Will request 1 executor containers, each with 1 cores and 3456 MB memory including 384 MB overhead
17/05/16 11:11:20 INFO spark.ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 5)
17/05/16 11:11:20 INFO yarn.YarnAllocator: Canceled 0 container requests (locality no longer needed)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 178.0 in stage 1.0 (TID 178, c419.hadoop.gda.lo, partition 178, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 178 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 174.0 in stage 1.0 (TID 174) in 12 ms on c419.hadoop.gda.lo (175/200)
17/05/16 11:11:20 INFO yarn.YarnAllocator: Submitted container request (host: Any, capability: <memory:3456, vCores:1>)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 179.0 in stage 1.0 (TID 179, c411.hadoop.gda.lo, partition 179, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 179 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 175.0 in stage 1.0 (TID 175) in 20 ms on c411.hadoop.gda.lo (176/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 180.0 in stage 1.0 (TID 180, c419.hadoop.gda.lo, partition 180, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 180 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 178.0 in stage 1.0 (TID 178) in 11 ms on c419.hadoop.gda.lo (177/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 181.0 in stage 1.0 (TID 181, c412.hadoop.gda.lo, partition 181, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 181 on executor id: 1 hostname: c412.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 177.0 in stage 1.0 (TID 177) in 17 ms on c412.hadoop.gda.lo (178/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 182.0 in stage 1.0 (TID 182, c402.hadoop.gda.lo, partition 182, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 182 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 176.0 in stage 1.0 (TID 176) in 20 ms on c402.hadoop.gda.lo (179/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 183.0 in stage 1.0 (TID 183, c419.hadoop.gda.lo, partition 183, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 183 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 180.0 in stage 1.0 (TID 180) in 11 ms on c419.hadoop.gda.lo (180/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 184.0 in stage 1.0 (TID 184, c412.hadoop.gda.lo, partition 184, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 184 on executor id: 1 hostname: c412.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 181.0 in stage 1.0 (TID 181) in 13 ms on c412.hadoop.gda.lo (181/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 185.0 in stage 1.0 (TID 185, c411.hadoop.gda.lo, partition 185, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 185 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 179.0 in stage 1.0 (TID 179) in 22 ms on c411.hadoop.gda.lo (182/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 186.0 in stage 1.0 (TID 186, c402.hadoop.gda.lo, partition 186, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 186 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 182.0 in stage 1.0 (TID 182) in 16 ms on c402.hadoop.gda.lo (183/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 187.0 in stage 1.0 (TID 187, c419.hadoop.gda.lo, partition 187, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 187 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 183.0 in stage 1.0 (TID 183) in 15 ms on c419.hadoop.gda.lo (184/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 188.0 in stage 1.0 (TID 188, c411.hadoop.gda.lo, partition 188, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 188 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 185.0 in stage 1.0 (TID 185) in 14 ms on c411.hadoop.gda.lo (185/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 189.0 in stage 1.0 (TID 189, c412.hadoop.gda.lo, partition 189, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 189 on executor id: 1 hostname: c412.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 184.0 in stage 1.0 (TID 184) in 15 ms on c412.hadoop.gda.lo (186/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 190.0 in stage 1.0 (TID 190, c402.hadoop.gda.lo, partition 190, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 190 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 186.0 in stage 1.0 (TID 186) in 16 ms on c402.hadoop.gda.lo (187/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 191.0 in stage 1.0 (TID 191, c419.hadoop.gda.lo, partition 191, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 191 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 187.0 in stage 1.0 (TID 187) in 11 ms on c419.hadoop.gda.lo (188/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 192.0 in stage 1.0 (TID 192, c402.hadoop.gda.lo, partition 192, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 190.0 in stage 1.0 (TID 190) in 11 ms on c402.hadoop.gda.lo (189/200)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 192 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 193.0 in stage 1.0 (TID 193, c411.hadoop.gda.lo, partition 193, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 193 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 188.0 in stage 1.0 (TID 188) in 16 ms on c411.hadoop.gda.lo (190/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 194.0 in stage 1.0 (TID 194, c419.hadoop.gda.lo, partition 194, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 194 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 191.0 in stage 1.0 (TID 191) in 12 ms on c419.hadoop.gda.lo (191/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 195.0 in stage 1.0 (TID 195, c419.hadoop.gda.lo, partition 195, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 195 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 194.0 in stage 1.0 (TID 194) in 13 ms on c419.hadoop.gda.lo (192/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 196.0 in stage 1.0 (TID 196, c402.hadoop.gda.lo, partition 196, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 196 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 192.0 in stage 1.0 (TID 192) in 15 ms on c402.hadoop.gda.lo (193/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 197.0 in stage 1.0 (TID 197, c411.hadoop.gda.lo, partition 197, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 197 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 193.0 in stage 1.0 (TID 193) in 17 ms on c411.hadoop.gda.lo (194/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 198.0 in stage 1.0 (TID 198, c419.hadoop.gda.lo, partition 198, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 198 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 195.0 in stage 1.0 (TID 195) in 11 ms on c419.hadoop.gda.lo (195/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 199.0 in stage 1.0 (TID 199, c402.hadoop.gda.lo, partition 199, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 199 on executor id: 2 hostname: c402.hadoop.gda.lo.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 196.0 in stage 1.0 (TID 196) in 14 ms on c402.hadoop.gda.lo (196/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 198.0 in stage 1.0 (TID 198) in 17 ms on c419.hadoop.gda.lo (197/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 199.0 in stage 1.0 (TID 199) in 13 ms on c402.hadoop.gda.lo (198/200)
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 197.0 in stage 1.0 (TID 197) in 25 ms on c411.hadoop.gda.lo (199/200)
17/05/16 11:11:20 INFO yarn.YarnAllocator: Driver requested a total number of 3 executor(s).
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 189.0 in stage 1.0 (TID 189) in 77 ms on c412.hadoop.gda.lo (200/200)
17/05/16 11:11:20 INFO scheduler.DAGScheduler: ShuffleMapStage 1 (first at CcuReport.scala:39) finished in 2.312 s
17/05/16 11:11:20 INFO cluster.YarnClusterScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
17/05/16 11:11:20 INFO scheduler.DAGScheduler: looking for newly runnable stages
17/05/16 11:11:20 INFO scheduler.DAGScheduler: running: Set()
17/05/16 11:11:20 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 2)
17/05/16 11:11:20 INFO scheduler.DAGScheduler: failed: Set()
17/05/16 11:11:20 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[20] at first at CcuReport.scala:39), which has no missing parents
17/05/16 11:11:20 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.8 KB, free 1458.5 MB)
17/05/16 11:11:20 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.4 KB, free 1458.5 MB)
17/05/16 11:11:20 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.60.43.16:33694 (size: 4.4 KB, free: 1458.6 MB)
17/05/16 11:11:20 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1012
17/05/16 11:11:20 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[20] at first at CcuReport.scala:39)
17/05/16 11:11:20 INFO cluster.YarnClusterScheduler: Adding task set 2.0 with 1 tasks
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 200, c419.hadoop.gda.lo, partition 0, NODE_LOCAL, 5232 bytes)
17/05/16 11:11:20 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 200 on executor id: 4 hostname: c419.hadoop.gda.lo.
17/05/16 11:11:20 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on c419.hadoop.gda.lo:27876 (size: 4.4 KB, free: 1458.6 MB)
17/05/16 11:11:20 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.60.43.19:22817
17/05/16 11:11:20 INFO spark.MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 337 bytes
17/05/16 11:11:20 INFO yarn.YarnAllocator: Driver requested a total number of 1 executor(s).
17/05/16 11:11:20 INFO yarn.YarnAllocator: Canceling requests for 1 executor container(s) to have a new desired total 1 executors.
17/05/16 11:11:20 INFO yarn.YarnAllocator: Received 1 containers from YARN, launching executors on 0 of them.
17/05/16 11:11:20 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 200) in 435 ms on c419.hadoop.gda.lo (1/1)
17/05/16 11:11:20 INFO scheduler.DAGScheduler: ResultStage 2 (first at CcuReport.scala:39) finished in 0.435 s
17/05/16 11:11:20 INFO cluster.YarnClusterScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
17/05/16 11:11:20 INFO scheduler.DAGScheduler: Job 0 finished: first at CcuReport.scala:39, took 2.958093 s
17/05/16 11:11:20 INFO yarn.YarnAllocator: Driver requested a total number of 0 executor(s).
17/05/16 11:11:20 INFO yarn.YarnAllocator: Canceling requests for 0 executor container(s) to have a new desired total 0 executors.
17/05/16 11:11:20 WARN yarn.YarnAllocator: Expected to find pending requests, but found none.
17/05/16 11:11:20 INFO codegen.CodeGenerator: Code generated in 10.069619 ms
17/05/16 11:11:20 ERROR yarn.ApplicationMaster: User class threw exception: java.lang.NullPointerException: Value at index 0 is null
java.lang.NullPointerException: Value at index 0 is null
	at org.apache.spark.sql.Row$class.getAnyValAs(Row.scala:466)
	at org.apache.spark.sql.Row$class.getDouble(Row.scala:242)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getDouble(rows.scala:192)
	at vng.ge.stats.report.report.game.CcuReport.write(CcuReport.scala:39)
	at vng.ge.stats.report.base.TReport.run(TReport.scala:38)
	at vng.ge.stats.report.loader.GameDataLoader.run(GameDataLoader.scala:57)
	at vng.ge.stats.report.base.DataPool.report(DataPool.scala:44)
	at vng.ge.stats.report.job.Runner$$anonfun$run$2$$anonfun$apply$1.apply$mcV$sp(Runner.scala:76)
	at scala.util.control.Breaks.breakable(Breaks.scala:38)
	at vng.ge.stats.report.job.Runner$$anonfun$run$2.apply(Runner.scala:56)
	at vng.ge.stats.report.job.Runner$$anonfun$run$2.apply(Runner.scala:44)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at vng.ge.stats.report.job.Runner$.run(Runner.scala:44)
	at vng.ge.stats.report.job.Runner$.main(Runner.scala:19)
	at vng.ge.stats.report.job.Runner.main(Runner.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:627)
17/05/16 11:11:20 INFO yarn.ApplicationMaster: Final app status: FAILED, exitCode: 15, (reason: User class threw exception: java.lang.NullPointerException: Value at index 0 is null)
17/05/16 11:11:20 INFO spark.SparkContext: Invoking stop() from shutdown hook
17/05/16 11:11:20 INFO server.ServerConnector: Stopped ServerConnector@38325bf0{HTTP/1.1}{0.0.0.0:0}
17/05/16 11:11:20 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@3e95e2dc{/stages/stage/kill,null,UNAVAILABLE}
17/05/16 11:11:20 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@7a63d9d5{/api,null,UNAVAILABLE}
17/05/16 11:11:20 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@4454a5d2{/,null,UNAVAILABLE}
17/05/16 11:11:20 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@2f9a5169{/static,null,UNAVAILABLE}
17/05/16 11:11:20 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@64c40ed3{/executors/threadDump/json,null,UNAVAILABLE}
17/05/16 11:11:20 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@d666de0{/executors/threadDump,null,UNAVAILABLE}
17/05/16 11:11:20 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@4285240c{/executors/json,null,UNAVAILABLE}
17/05/16 11:11:20 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@5ca2e6e4{/executors,null,UNAVAILABLE}
17/05/16 11:11:20 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@79021936{/environment/json,null,UNAVAILABLE}
17/05/16 11:11:20 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@7c866551{/environment,null,UNAVAILABLE}
17/05/16 11:11:20 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@52b9801c{/storage/rdd/json,null,UNAVAILABLE}
17/05/16 11:11:20 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@4b6dcd07{/storage/rdd,null,UNAVAILABLE}
17/05/16 11:11:20 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@b51da05{/storage/json,null,UNAVAILABLE}
17/05/16 11:11:20 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@18e6dee4{/storage,null,UNAVAILABLE}
17/05/16 11:11:20 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@2a46616f{/stages/pool/json,null,UNAVAILABLE}
17/05/16 11:11:20 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@272cea12{/stages/pool,null,UNAVAILABLE}
17/05/16 11:11:20 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@45bad3de{/stages/stage/json,null,UNAVAILABLE}
17/05/16 11:11:20 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@3911607b{/stages/stage,null,UNAVAILABLE}
17/05/16 11:11:20 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@aa0e8f0{/stages/json,null,UNAVAILABLE}
17/05/16 11:11:20 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@2b5935be{/stages,null,UNAVAILABLE}
17/05/16 11:11:20 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@3ec843e4{/jobs/job/json,null,UNAVAILABLE}
17/05/16 11:11:20 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@76d8bb44{/jobs/job,null,UNAVAILABLE}
17/05/16 11:11:20 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@6c19b835{/jobs/json,null,UNAVAILABLE}
17/05/16 11:11:20 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@4b2bc5f6{/jobs,null,UNAVAILABLE}
17/05/16 11:11:20 INFO ui.SparkUI: Stopped Spark web UI at http://10.60.43.16:39509
17/05/16 11:11:21 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(3,WrappedArray())
17/05/16 11:11:22 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(1,WrappedArray())
17/05/16 11:11:23 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(4,WrappedArray())
17/05/16 11:11:25 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(2,WrappedArray())
17/05/16 11:11:31 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(3,WrappedArray())
17/05/16 11:11:31 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(1,WrappedArray())
17/05/16 11:11:33 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(4,WrappedArray())
17/05/16 11:11:35 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(2,WrappedArray())
17/05/16 11:11:41 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(3,WrappedArray())
17/05/16 11:11:41 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(1,WrappedArray())
17/05/16 11:11:43 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(4,WrappedArray())
17/05/16 11:11:45 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(2,WrappedArray())
17/05/16 11:11:51 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(3,WrappedArray())
17/05/16 11:11:51 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(1,WrappedArray())
17/05/16 11:11:53 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(4,WrappedArray())
17/05/16 11:11:55 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(2,WrappedArray())
17/05/16 11:12:01 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(3,WrappedArray())
17/05/16 11:12:01 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(1,WrappedArray())
17/05/16 11:12:03 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(4,WrappedArray())
17/05/16 11:12:05 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(2,WrappedArray())
17/05/16 11:12:11 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(3,WrappedArray())
17/05/16 11:12:11 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(1,WrappedArray())
17/05/16 11:12:13 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(4,WrappedArray())
17/05/16 11:12:15 ERROR scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(2,WrappedArray())
17/05/16 11:12:17 WARN hdfs.DFSClient: Slow ReadProcessor read fields took 50426ms (threshold=30000ms); ack: seqno: 31 reply: SUCCESS reply: SUCCESS reply: SUCCESS downstreamAckTimeNanos: 50424801514 flag: 0 flag: 0 flag: 0, targets: [DatanodeInfoWithStorage[10.60.43.16:50010,DS-efe8fd34-a004-4410-b2eb-ebd86611eb75,DISK], DatanodeInfoWithStorage[10.60.43.12:50010,DS-bdbe29f7-dc2f-4ed5-9c91-6d26a3ff805b,DISK], DatanodeInfoWithStorage[10.60.43.18:50010,DS-6d290886-79f5-48b7-92a5-cd62d01dd3d5,DISK]]
17/05/16 11:12:17 WARN hdfs.DFSClient: Slow waitForAckedSeqno took 50476ms (threshold=30000ms)
17/05/16 11:12:17 INFO cluster.YarnClusterSchedulerBackend: Shutting down all executors
17/05/16 11:12:17 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
17/05/16 11:12:17 INFO cluster.SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
17/05/16 11:12:17 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/05/16 11:12:17 INFO memory.MemoryStore: MemoryStore cleared
17/05/16 11:12:17 INFO storage.BlockManager: BlockManager stopped
17/05/16 11:12:17 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
17/05/16 11:12:17 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/05/16 11:12:17 INFO spark.SparkContext: Successfully stopped SparkContext
17/05/16 11:12:17 INFO yarn.ApplicationMaster: Unregistering ApplicationMaster with FAILED (diag message: User class threw exception: java.lang.NullPointerException: Value at index 0 is null)
17/05/16 11:12:17 INFO impl.AMRMClientImpl: Waiting for application to be successfully unregistered.
17/05/16 11:12:18 INFO yarn.ApplicationMaster: Deleting staging directory hdfs://c408.hadoop.gda.lo:8020/user/fairy/.sparkStaging/application_1494395298335_303847
17/05/16 11:12:18 INFO util.ShutdownHookManager: Shutdown hook called
17/05/16 11:12:18 INFO util.ShutdownHookManager: Deleting directory /hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/spark-476cda9a-32be-40b3-9d8d-c5bd981d6d23
End of LogType:stderr

LogType:stdout
Log Upload Time:Tue May 16 11:12:21 +0700 2017
LogLength:2995
Log Contents:
STATS INFO: 2017-05-16 11:11:08 \_ Parameters: Map(logDir -> /ge/warehouse, group_id -> game, source -> sdk, game_code -> cack, calc_id -> id, job_name -> cack, report_number -> 1-2-3-4-5-6-7-8-9, log_date -> 2017-05-06, run_timing -> a1,a3,a7,a14,a30,ac7,ac30)
STATS INFO: 2017-05-16 11:11:08 ---------------------------------------------------------------------------------------------
STATS INFO: 2017-05-16 11:11:08 \_ Timing: a1
STATS INFO: 2017-05-16 11:11:08 \_ ===============
STATS INFO: 2017-05-16 11:11:08 |	\_ Reading /ge/fairy/warehouse/cack/ub/sdk_data/ccu_2/2017-05-06
STATS WARN: 2017-05-16 11:11:08 |		\_ Loading file failed: Path does not exist: hdfs://c408.hadoop.gda.lo:8020/ge/fairy/warehouse/cack/ub/sdk_data/ccu_2/2017-05-06;
STATS WARN: 2017-05-16 11:11:08 |		\_ /ge/fairy/warehouse/cack/ub/sdk_data/ccu_2/2017-05-06 is not found => using default schema!
STATS INFO: 2017-05-16 11:11:16 |	\_ Reading /ge/fairy/warehouse/cack/ub/sdk_data/activity_2
STATS WARN: 2017-05-16 11:11:16 |		\_ Loading file failed: Path does not exist: hdfs://c408.hadoop.gda.lo:8020/ge/fairy/warehouse/cack/ub/sdk_data/activity_2/{2017-05-06};
STATS WARN: 2017-05-16 11:11:16 |		\_ /ge/fairy/warehouse/cack/ub/sdk_data/activity_2/{2017-05-06} is not found => using default schema!
STATS INFO: 2017-05-16 11:11:16 |	\_ Reading /ge/fairy/warehouse/cack/ub/sdk_data/accregister_2
STATS WARN: 2017-05-16 11:11:16 |		\_ Loading file failed: Path does not exist: hdfs://c408.hadoop.gda.lo:8020/ge/fairy/warehouse/cack/ub/sdk_data/accregister_2/{2017-05-06};
STATS WARN: 2017-05-16 11:11:16 |		\_ /ge/fairy/warehouse/cack/ub/sdk_data/accregister_2/{2017-05-06} is not found => using default schema!
STATS INFO: 2017-05-16 11:11:16 |	\_ Reading /ge/fairy/warehouse/cack/ub/sdk_data/payment_2
STATS WARN: 2017-05-16 11:11:16 |		\_ Loading file failed: Path does not exist: hdfs://c408.hadoop.gda.lo:8020/ge/fairy/warehouse/cack/ub/sdk_data/payment_2/{2017-05-06};
STATS WARN: 2017-05-16 11:11:16 |		\_ /ge/fairy/warehouse/cack/ub/sdk_data/payment_2/{2017-05-06} is not found => using default schema!
STATS INFO: 2017-05-16 11:11:16 |	\_ Reading /ge/fairy/warehouse/cack/ub/sdk_data/first_charge_2
STATS WARN: 2017-05-16 11:11:16 |		\_ Loading file failed: Path does not exist: hdfs://c408.hadoop.gda.lo:8020/ge/fairy/warehouse/cack/ub/sdk_data/first_charge_2/{2017-05-06};
STATS WARN: 2017-05-16 11:11:16 |		\_ /ge/fairy/warehouse/cack/ub/sdk_data/first_charge_2/{2017-05-06} is not found => using default schema!
STATS INFO: 2017-05-16 11:11:16 |	\_ Reading /ge/fairy/warehouse/cack/ub/sdk_data/total_login_acc_2/2017-05-06
STATS WARN: 2017-05-16 11:11:16 |		\_ Loading file failed: Path does not exist: hdfs://c408.hadoop.gda.lo:8020/ge/fairy/warehouse/cack/ub/sdk_data/total_login_acc_2/2017-05-06;
STATS WARN: 2017-05-16 11:11:16 |		\_ /ge/fairy/warehouse/cack/ub/sdk_data/total_login_acc_2/2017-05-06 is not found => using default schema!
STATS INFO: 2017-05-16 11:11:16 Skip game retention report!
End of LogType:stdout



Container: container_e81_1494395298335_303847_02_000005 on c419.hadoop.gda.lo_45454
=====================================================================================
LogType:directory.info
Log Upload Time:Tue May 16 11:12:19 +0700 2017
LogLength:29405
Log Contents:
ls -l:
total 100
lrwxrwxrwx 1 yarn hadoop    69 May 16 11:11 __app__.jar -> /hadoop/yarn/local/usercache/fairy/filecache/37977/stats-etlr-1.0.jar
-rw-r--r-- 1 yarn hadoop    87 May 16 11:11 container_tokens
-rwx------ 1 yarn hadoop   676 May 16 11:11 default_container_executor_session.sh
-rwx------ 1 yarn hadoop   730 May 16 11:11 default_container_executor.sh
-rwx------ 1 yarn hadoop 75763 May 16 11:11 launch_container.sh
lrwxrwxrwx 1 yarn hadoop    69 May 16 11:11 __spark_conf__ -> /hadoop/yarn/local/usercache/fairy/filecache/37978/__spark_conf__.zip
drwxr-xr-x 2 yarn hadoop  8192 May 16 11:11 __spark_libs__
drwx--x--- 2 yarn hadoop    10 May 16 11:11 tmp
find -L . -maxdepth 5 -ls:
8590305302    4 drwx--x---   4 yarn     hadoop       4096 May 16 11:11 .
17179963911    0 drwx--x---   2 yarn     hadoop         10 May 16 11:11 ./tmp
8590305305    4 -rw-r--r--   1 yarn     hadoop         87 May 16 11:11 ./container_tokens
8590305306    4 -rw-r--r--   1 yarn     hadoop         12 May 16 11:11 ./.container_tokens.crc
8590305308   76 -rwx------   1 yarn     hadoop      75763 May 16 11:11 ./launch_container.sh
8590305309    4 -rw-r--r--   1 yarn     hadoop        600 May 16 11:11 ./.launch_container.sh.crc
8590305311    4 -rwx------   1 yarn     hadoop        676 May 16 11:11 ./default_container_executor_session.sh
8590306020    4 -rw-r--r--   1 yarn     hadoop         16 May 16 11:11 ./.default_container_executor_session.sh.crc
8590306037    4 -rwx------   1 yarn     hadoop        730 May 16 11:11 ./default_container_executor.sh
8590306039    4 -rw-r--r--   1 yarn     hadoop         16 May 16 11:11 ./.default_container_executor.sh.crc
21474897174   12 drwxr-xr-x   2 yarn     hadoop       8192 May 16 11:11 ./__spark_libs__
12893318172  356 -r-xr-xr-x   1 yarn     hadoop     363908 May 10 12:58 ./__spark_libs__/hadoop-yarn-server-common-2.7.1.jar
12893307103 5380 -r-xr-xr-x   1 yarn     hadoop    5505200 May 10 12:58 ./__spark_libs__/hive-metastore-1.2.1.spark2.jar
975474  528 -rwxr-xr-x   1 yarn     hadoop     539912 Apr  5 13:43 ./__spark_libs__/jetty-6.1.26.jar
12893307508  116 -r-xr-xr-x   1 yarn     hadoop     114913 May 10 12:58 ./__spark_libs__/py4j-0.10.3.jar
12893307098   20 -r-xr-xr-x   1 yarn     hadoop      17385 May 10 12:58 ./__spark_libs__/hadoop-annotations-2.7.1.jar
17180303777   68 -rwxr-xr-x   1 yarn     hadoop      69409 Apr  5 13:43 ./__spark_libs__/activation-1.1.1.jar
21476352444 1168 -rwxr-xr-x   1 yarn     hadoop    1194003 Mar 23 14:46 ./__spark_libs__/arpack_combined_all-0.1.jar
4295008619  100 -r-xr-xr-x   1 yarn     hadoop     100636 May 10 12:58 ./__spark_libs__/jsp-api-2.1.jar
12893320895 3152 -r-xr-xr-x   1 yarn     hadoop    3224708 May 10 12:58 ./__spark_libs__/derby-10.12.1.1.jar
4295032540 5712 -r-xr-xr-x   1 yarn     hadoop    5847591 May 10 12:58 ./__spark_libs__/spark-mllib_2.11-2.0.1.jar
975494 1848 -rwxr-xr-x   1 yarn     hadoop    1890075 Apr  5 13:43 ./__spark_libs__/datanucleus-core-3.2.10.jar
8590873348   92 -rwxr-xr-x   1 yarn     hadoop      93407 Apr  5 13:43 ./__spark_libs__/pyrolite-4.9.jar
17180292366  932 -rwxr-xr-x   1 yarn     hadoop     951701 Apr  5 13:43 ./__spark_libs__/jersey-server-2.22.2.jar
12893733117 1972 -rwxr-xr-x   1 yarn     hadoop    2015514 Mar 23 14:46 ./__spark_libs__/hadoop-yarn-api-2.7.1.jar
4295009278   28 -rwxr-xr-x   1 yarn     hadoop      27084 Mar 23 14:46 ./__spark_libs__/jackson-xc-1.9.13.jar
975432  160 -rwxr-xr-x   1 yarn     hadoop     160519 Apr  5 13:43 ./__spark_libs__/commons-dbcp-1.4.jar
21476054872    4 -rwxr-xr-x   1 yarn     hadoop       2545 Apr  5 13:43 ./__spark_libs__/hadoop-client-2.7.1.jar
4295398245    8 -rwxr-xr-x   1 yarn     hadoop       4467 Apr  5 13:43 ./__spark_libs__/aopalliance-1.0.jar
21474965802 1204 -r-xr-xr-x   1 yarn     hadoop    1230201 May 10 12:58 ./__spark_libs__/netty-3.8.0.Final.jar
21474966533  352 -r-xr-xr-x   1 yarn     hadoop     358390 May 10 12:58 ./__spark_libs__/kryo-shaded-3.0.3.jar
8590873398  256 -rwxr-xr-x   1 yarn     hadoop     258370 Apr  5 13:43 ./__spark_libs__/calcite-avatica-1.2.0-incubating.jar
17180278057 5612 -rwxr-xr-x   1 yarn     hadoop    5744974 Apr  5 13:43 ./__spark_libs__/scala-library-2.11.8.jar
8590867878  284 -rwxr-xr-x   1 yarn     hadoop     290506 Apr  5 13:43 ./__spark_libs__/univocity-parsers-2.1.1.jar
8590873369   96 -rwxr-xr-x   1 yarn     hadoop      95806 Apr  5 13:43 ./__spark_libs__/javax.servlet-api-3.1.0.jar
21474966420  208 -r-xr-xr-x   1 yarn     hadoop     212453 May 10 12:58 ./__spark_libs__/commons-net-2.2.jar
8590873361   56 -rwxr-xr-x   1 yarn     hadoop      55511 Apr  5 13:43 ./__spark_libs__/spark-network-shuffle_2.11-2.0.1.jar
8590867918   64 -rwxr-xr-x   1 yarn     hadoop      65012 Apr  5 13:43 ./__spark_libs__/guice-servlet-3.0.jar
4295008645  952 -r-xr-xr-x   1 yarn     hadoop     971310 May 10 12:58 ./__spark_libs__/jersey-guava-2.22.2.jar
1236320 6716 -rwxr-xr-x   1 yarn     hadoop    6873892 Apr  5 13:43 ./__spark_libs__/spark-catalyst_2.11-2.0.1.jar
17180278056   40 -rwxr-xr-x   1 yarn     hadoop      40509 Apr  5 13:43 ./__spark_libs__/slf4j-api-1.7.16.jar
4295009274 1024 -rwxr-xr-x   1 yarn     hadoop    1048110 Mar 23 14:46 ./__spark_libs__/parquet-jackson-1.7.0.jar
21475052446  192 -rwxr-xr-x   1 yarn     hadoop     192993 Apr  5 13:43 ./__spark_libs__/avro-ipc-1.7.7.jar
12893307172  280 -r-xr-xr-x   1 yarn     hadoop     285447 May 10 12:58 ./__spark_libs__/parquet-encoding-1.7.0.jar
4295009268   20 -rwxr-xr-x   1 yarn     hadoop      20235 Mar 23 14:46 ./__spark_libs__/osgi-resource-locator-1.0.1.jar
975501   16 -rwxr-xr-x   1 yarn     hadoop      15827 Apr  5 13:43 ./__spark_libs__/metrics-json-3.1.2.jar
12893733101  512 -rwxr-xr-x   1 yarn     hadoop     521157 Mar 23 14:46 ./__spark_libs__/mail-1.4.7.jar
8590879041 1024 -rwxr-xr-x   1 yarn     hadoop    1044511 Apr  5 13:43 ./__spark_libs__/spark-hive_2.11-2.0.1.jar
8590879071   28 -rwxr-xr-x   1 yarn     hadoop      26514 Apr  5 13:43 ./__spark_libs__/stax-api-1.0.1.jar
8590873387   20 -rwxr-xr-x   1 yarn     hadoop      18098 Apr  5 13:43 ./__spark_libs__/jersey-container-servlet-2.22.2.jar
8590879097 7108 -rwxr-xr-x   1 yarn     hadoop    7276083 Apr  5 13:43 ./__spark_libs__/spire_2.11-0.7.4.jar
8590879982    4 -rwxr-xr-x   1 yarn     hadoop       2497 Apr  5 13:43 ./__spark_libs__/javax.inject-1.jar
975392  176 -rwxr-xr-x   1 yarn     hadoop     177131 Apr  5 13:43 ./__spark_libs__/jetty-util-6.1.26.jar
975403   68 -rwxr-xr-x   1 yarn     hadoop      66270 Apr  5 13:43 ./__spark_libs__/jersey-container-servlet-core-2.22.2.jar
8590879045 2732 -rwxr-xr-x   1 yarn     hadoop    2796935 Apr  5 13:43 ./__spark_libs__/parquet-hadoop-bundle-1.6.0.jar
4295032507  220 -r-xr-xr-x   1 yarn     hadoop     223573 May 10 12:58 ./__spark_libs__/chill_2.11-0.8.0.jar
12893321177   36 -r-xr-xr-x   1 yarn     hadoop      33015 May 10 12:58 ./__spark_libs__/jsr305-1.3.9.jar
922445 1444 -rwxr-xr-x   1 yarn     hadoop    1475955 Apr  5 13:43 ./__spark_libs__/htrace-core-3.1.0-incubating.jar
8590873347   32 -rwxr-xr-x   1 yarn     hadoop      32145 Apr  5 13:43 ./__spark_libs__/hadoop-yarn-server-web-proxy-2.7.1.jar
4295402336 1024 -rwxr-xr-x   1 yarn     hadoop    1045744 Apr  5 13:43 ./__spark_libs__/leveldbjni-all-1.8.jar
21476352449 2140 -rwxr-xr-x   1 yarn     hadoop    2189117 Mar 23 14:46 ./__spark_libs__/guava-14.0.1.jar
1181978 1592 -rwxr-xr-x   1 yarn     hadoop    1627065 Apr  5 13:43 ./__spark_libs__/jersey-bundle-1.19.1.jar
12893315410   72 -r-xr-xr-x   1 yarn     hadoop      70688 May 10 12:58 ./__spark_libs__/hadoop-auth-2.7.1.jar
21474966429 1768 -r-xr-xr-x   1 yarn     hadoop    1809447 May 10 12:58 ./__spark_libs__/datanucleus-rdbms-3.2.9.jar
4295008616   16 -r-xr-xr-x   1 yarn     hadoop      14766 May 10 12:58 ./__spark_libs__/aopalliance-repackaged-2.4.0-b34.jar
8590879085  508 -rwxr-xr-x   1 yarn     hadoop     516127 Apr  5 13:43 ./__spark_libs__/hadoop-mapreduce-client-app-2.7.1.jar
1005941 2044 -rwxr-xr-x   1 yarn     hadoop    2090370 Apr  5 13:43 ./__spark_libs__/spark-streaming_2.11-2.0.1.jar
21476352436  212 -rwxr-xr-x   1 yarn     hadoop     213911 Mar 23 14:46 ./__spark_libs__/jline-2.12.1.jar
8590879056  228 -rwxr-xr-x   1 yarn     hadoop     232248 Apr  5 13:43 ./__spark_libs__/jackson-core-asl-1.9.13.jar
4295010998   40 -rwxr-xr-x   1 yarn     hadoop      38134 Mar 23 14:46 ./__spark_libs__/hadoop-mapreduce-client-jobclient-2.7.1.jar
1236312  120 -rwxr-xr-x   1 yarn     hadoop     118973 Apr  5 13:43 ./__spark_libs__/hk2-utils-2.4.0-b34.jar
975514  236 -rwxr-xr-x   1 yarn     hadoop     241367 Apr  5 13:43 ./__spark_libs__/commons-compress-1.4.1.jar
4295033663 11452 -r-xr-xr-x   1 yarn     hadoop   11723537 May 10 12:58 ./__spark_libs__/spark-core_2.11-2.0.1.jar
975420  188 -rwxr-xr-x   1 yarn     hadoop     188671 Apr  5 13:43 ./__spark_libs__/commons-beanutils-1.7.0.jar
4295033670 6144 -r-xr-xr-x   1 yarn     hadoop    6290315 May 10 12:58 ./__spark_libs__/spark-sql_2.11-2.0.1.jar
4295008625  416 -r-xr-xr-x   1 yarn     hadoop     423753 May 10 12:58 ./__spark_libs__/scala-parser-combinators_2.11-1.0.4.jar
4295088451  696 -r-xr-xr-x   1 yarn     hadoop     710492 May 10 12:58 ./__spark_libs__/guice-3.0.jar
4295009271   64 -rwxr-xr-x   1 yarn     hadoop      62050 Mar 23 14:46 ./__spark_libs__/commons-logging-1.1.3.jar
21474966404  232 -r-xr-xr-x   1 yarn     hadoop     236880 May 10 12:58 ./__spark_libs__/lz4-1.3.0.jar
21474966424  136 -r-xr-xr-x   1 yarn     hadoop     138464 May 10 12:58 ./__spark_libs__/hive-beeline-1.2.1.spark2.jar
21475284206  232 -rwxr-xr-x   1 yarn     hadoop     236660 Apr  5 13:43 ./__spark_libs__/ST4-4.0.4.jar
975507   20 -rwxr-xr-x   1 yarn     hadoop      19827 Apr  5 13:43 ./__spark_libs__/opencsv-2.3.jar
21476352433  668 -rwxr-xr-x   1 yarn     hadoop     680106 Mar 23 14:46 ./__spark_libs__/spark-graphx_2.11-2.0.1.jar
17179932893   48 -rwxr-xr-x   1 yarn     hadoop      46983 Apr  5 13:43 ./__spark_libs__/jackson-annotations-2.6.5.jar
12893741273  440 -rwxr-xr-x   1 yarn     hadoop     448794 Mar 23 14:46 ./__spark_libs__/apache-log4j-extras-1.2.17.jar
12893320209    8 -r-xr-xr-x   1 yarn     hadoop       5711 May 10 12:58 ./__spark_libs__/minlog-1.3.0.jar
8590865776   48 -rwxr-xr-x   1 yarn     hadoop      45944 Apr  5 13:43 ./__spark_libs__/json-20090211.jar
21474965822   44 -r-xr-xr-x   1 yarn     hadoop      41070 May 10 12:58 ./__spark_libs__/spark-unsafe_2.11-2.0.1.jar
17180271170  144 -rwxr-xr-x   1 yarn     hadoop     143602 Apr  5 13:43 ./__spark_libs__/commons-digester-1.8.jar
975498  532 -rwxr-xr-x   1 yarn     hadoop     540852 Apr  5 13:43 ./__spark_libs__/mysql-connector-java-5.0.8-bin.jar
1165807  244 -rwxr-xr-x   1 yarn     hadoop     248171 Apr  5 13:43 ./__spark_libs__/curator-recipes-2.6.0.jar
8590865806 15128 -rwxr-xr-x   1 yarn     hadoop   15487351 Apr  5 13:43 ./__spark_libs__/scala-compiler-2.11.8.jar
12893321262   12 -r-xr-xr-x   1 yarn     hadoop      10023 May 10 12:58 ./__spark_libs__/java-xmlbuilder-1.0.jar
17180303642   44 -rwxr-xr-x   1 yarn     hadoop      41123 Apr  5 13:43 ./__spark_libs__/commons-cli-1.2.jar
17180271144   44 -rwxr-xr-x   1 yarn     hadoop      44925 Apr  5 13:43 ./__spark_libs__/apacheds-i18n-2.0.0-M15.jar
4295008649 1616 -r-xr-xr-x   1 yarn     hadoop    1654097 May 10 12:58 ./__spark_libs__/hadoop-yarn-common-2.7.1.jar
17180303594   16 -rwxr-xr-x   1 yarn     hadoop      15071 Apr  5 13:43 ./__spark_libs__/jta-1.1.jar
975444  256 -rwxr-xr-x   1 yarn     hadoop     258876 Apr  5 13:43 ./__spark_libs__/jackson-core-2.6.5.jar
4295009575   84 -rwxr-xr-x   1 yarn     hadoop      82421 Mar 23 14:46 ./__spark_libs__/json4s-ast_2.11-3.2.11.jar
8590865796  204 -rwxr-xr-x   1 yarn     hadoop     206035 Apr  5 13:43 ./__spark_libs__/commons-beanutils-core-1.8.0.jar
17180235849  164 -rwxr-xr-x   1 yarn     hadoop     164422 Apr  5 13:43 ./__spark_libs__/core-1.1.2.jar
4295086135 2304 -r-xr-xr-x   1 yarn     hadoop    2355465 May 10 12:58 ./__spark_libs__/spark-network-common_2.11-2.0.1.jar
12893307090 1248 -r-xr-xr-x   1 yarn     hadoop    1277883 May 10 12:58 ./__spark_libs__/mesos-0.21.1-shaded-protobuf.jar
975489  720 -rwxr-xr-x   1 yarn     hadoop     736658 Apr  5 13:43 ./__spark_libs__/httpclient-4.5.2.jar
8590873405  172 -rwxr-xr-x   1 yarn     hadoop     174351 Apr  5 13:43 ./__spark_libs__/stream-2.7.0.jar
17180278050  640 -rwxr-xr-x   1 yarn     hadoop     654216 Apr  5 13:43 ./__spark_libs__/pmml-model-1.2.15.jar
17180303614 4468 -rwxr-xr-x   1 yarn     hadoop    4573750 Apr  5 13:43 ./__spark_libs__/scala-reflect-2.11.8.jar
17179960175 1996 -rwxr-xr-x   1 yarn     hadoop    2041628 Apr  5 13:43 ./__spark_libs__/jets3t-0.9.3.jar
17180278110  208 -rwxr-xr-x   1 yarn     hadoop     209622 Apr  5 13:43 ./__spark_libs__/parquet-hadoop-1.7.0.jar
1181988   80 -rwxr-xr-x   1 yarn     hadoop      79845 Apr  5 13:43 ./__spark_libs__/compress-lzf-1.0.3.jar
12893307502   24 -r-xr-xr-x   1 yarn     hadoop      23346 May 10 12:58 ./__spark_libs__/stax-api-1.0-2.jar
17179968602  188 -rwxr-xr-x   1 yarn     hadoop     190432 Apr  5 13:43 ./__spark_libs__/gson-2.2.4.jar
21475067699 11232 -rwxr-xr-x   1 yarn     hadoop   11498852 Apr  5 13:43 ./__spark_libs__/hive-exec-1.2.1.spark2.jar
922319   40 -rwxr-xr-x   1 yarn     hadoop      40817 Apr  5 13:43 ./__spark_libs__/hive-cli-1.2.1.spark2.jar
8590873366   20 -rwxr-xr-x   1 yarn     hadoop      16993 Apr  5 13:43 ./__spark_libs__/JavaEWAH-0.3.2.jar
12893320869 1204 -r-xr-xr-x   1 yarn     hadoop    1229125 May 10 12:58 ./__spark_libs__/xercesImpl-2.9.1.jar
12893733104   96 -rwxr-xr-x   1 yarn     hadoop      94672 Mar 23 14:46 ./__spark_libs__/xz-1.0.jar
17180292364  176 -rwxr-xr-x   1 yarn     hadoop     178947 Apr  5 13:43 ./__spark_libs__/hk2-api-2.4.0-b34.jar
4295008382  524 -r-xr-xr-x   1 yarn     hadoop     533455 May 10 12:58 ./__spark_libs__/protobuf-java-2.5.0.jar
12893314730   20 -rwxr-xr-x   1 yarn     hadoop      17008 Apr  5 13:43 ./__spark_libs__/base64-2.3.8.jar
4295008638  684 -r-xr-xr-x   1 yarn     hadoop     698375 May 10 12:58 ./__spark_libs__/jersey-common-2.22.2.jar
8590879087  200 -rwxr-xr-x   1 yarn     hadoop     201928 Apr  5 13:43 ./__spark_libs__/RoaringBitmap-0.5.11.jar
922322   44 -rwxr-xr-x   1 yarn     hadoop      41755 Apr  5 13:43 ./__spark_libs__/objenesis-2.1.jar
17179967394   52 -rwxr-xr-x   1 yarn     hadoop      50619 Apr  5 13:43 ./__spark_libs__/chill-java-0.8.0.jar
8590865811  144 -rwxr-xr-x   1 yarn     hadoop     144660 Apr  5 13:43 ./__spark_libs__/xbean-asm5-shaded-4.4.jar
4295008656  100 -r-xr-xr-x   1 yarn     hadoop     100680 May 10 12:58 ./__spark_libs__/hive-jdbc-1.2.1.spark2.jar
17180292336  600 -rwxr-xr-x   1 yarn     hadoop     613299 Apr  5 13:43 ./__spark_libs__/janino-2.7.8.jar
21474965815  148 -r-xr-xr-x   1 yarn     hadoop     148627 May 10 12:58 ./__spark_libs__/stringtemplate-3.2.1.jar
1084236   40 -rwxr-xr-x   1 yarn     hadoop      39280 Apr  5 13:43 ./__spark_libs__/metrics-jvm-3.1.2.jar
21474965808  332 -r-xr-xr-x   1 yarn     hadoop     339666 May 10 12:58 ./__spark_libs__/datanucleus-api-jdo-3.2.6.jar
975467   68 -rwxr-xr-x   1 yarn     hadoop      65653 Apr  5 13:43 ./__spark_libs__/spark-launcher_2.11-2.0.1.jar
21474966377  748 -r-xr-xr-x   1 yarn     hadoop     764569 May 10 12:58 ./__spark_libs__/jtransforms-2.4.0.jar
975406 3440 -rwxr-xr-x   1 yarn     hadoop    3519262 Apr  5 13:43 ./__spark_libs__/calcite-core-1.2.0-incubating.jar
17180307845  296 -rwxr-xr-x   1 yarn     hadoop     302248 Apr  5 13:43 ./__spark_libs__/antlr4-runtime-4.5.3.jar
8590867929   40 -rwxr-xr-x   1 yarn     hadoop      40341 Apr  5 13:43 ./__spark_libs__/json4s-jackson_2.11-3.2.11.jar
21476352419  184 -rwxr-xr-x   1 yarn     hadoop     185245 Mar 23 14:46 ./__spark_libs__/curator-framework-2.6.0.jar
975397  224 -rwxr-xr-x   1 yarn     hadoop     227712 Apr  5 13:43 ./__spark_libs__/libthrift-0.9.2.jar
12893307100  280 -r-xr-xr-x   1 yarn     hadoop     284220 May 10 12:58 ./__spark_libs__/commons-lang-2.6.jar
1084252    8 -rwxr-xr-x   1 yarn     hadoop       5310 Apr  5 13:43 ./__spark_libs__/pmml-schema-1.2.15.jar
21476352425  896 -rwxr-xr-x   1 yarn     hadoop     917052 Mar 23 14:46 ./__spark_libs__/parquet-column-1.7.0.jar
21476352430   20 -rwxr-xr-x   1 yarn     hadoop      18336 Mar 23 14:46 ./__spark_libs__/jackson-jaxrs-1.9.13.jar
21474966530   16 -r-xr-xr-x   1 yarn     hadoop      15010 May 10 12:58 ./__spark_libs__/xmlenc-0.52.jar
975472  636 -rwxr-xr-x   1 yarn     hadoop     648678 Apr  5 13:43 ./__spark_libs__/scala-xml_2.11-1.0.2.jar
975480  136 -rwxr-xr-x   1 yarn     hadoop     135552 Apr  5 13:43 ./__spark_libs__/breeze-macros_2.11-0.11.2.jar
21474879086   32 -r-xr-xr-x   1 yarn     hadoop      29540 May 10 12:58 ./__spark_libs__/spark-sketch_2.11-2.0.1.jar
8590865799   64 -rwxr-xr-x   1 yarn     hadoop      63777 Apr  5 13:43 ./__spark_libs__/validation-api-1.1.0.Final.jar
4295008413  184 -r-xr-xr-x   1 yarn     hadoop     185140 May 10 12:58 ./__spark_libs__/commons-io-2.4.jar
17180188088  104 -r-xr-xr-x   1 yarn     hadoop     105134 May 10 12:58 ./__spark_libs__/jaxb-api-2.2.2.jar
21474966383  116 -r-xr-xr-x   1 yarn     hadoop     115534 May 10 12:58 ./__spark_libs__/javax.ws.rs-api-2.0.1.jar
17180307481  468 -rwxr-xr-x   1 yarn     hadoop     477970 Apr  5 13:43 ./__spark_libs__/lift-json_2.11-2.6.3.jar
12893733110   44 -rwxr-xr-x   1 yarn     hadoop      45015 Mar 23 14:46 ./__spark_libs__/hadoop-mapreduce-client-shuffle-2.7.1.jar
21474966386  292 -r-xr-xr-x   1 yarn     hadoop     298829 May 10 12:58 ./__spark_libs__/commons-configuration-1.6.jar
4295087762  616 -r-xr-xr-x   1 yarn     hadoop     627814 May 10 12:58 ./__spark_libs__/joda-time-2.9.3.jar
4295008613   68 -r-xr-xr-x   1 yarn     hadoop      68866 May 10 12:58 ./__spark_libs__/curator-client-2.6.0.jar
12893324022   64 -r-xr-xr-x   1 yarn     hadoop      65261 May 10 12:58 ./__spark_libs__/oro-2.0.8.jar
12893324898 1032 -r-xr-xr-x   1 yarn     hadoop    1056168 May 10 12:58 ./__spark_libs__/snappy-java-1.1.2.6.jar
17180278102   32 -rwxr-xr-x   1 yarn     hadoop      29555 Apr  5 13:43 ./__spark_libs__/paranamer-2.3.jar
21475051007    8 -rwxr-xr-x   1 yarn     hadoop       5950 Apr  5 13:43 ./__spark_libs__/javax.inject-2.4.0-b34.jar
975437  788 -rwxr-xr-x   1 yarn     hadoop     802818 Apr  5 13:43 ./__spark_libs__/scalap-2.11.8.jar
12893315413   24 -r-xr-xr-x   1 yarn     hadoop      21575 May 10 12:58 ./__spark_libs__/parquet-common-1.7.0.jar
17180271194   92 -rwxr-xr-x   1 yarn     hadoop      93210 Apr  5 13:43 ./__spark_libs__/super-csv-2.2.0.jar
17180277222  176 -rwxr-xr-x   1 yarn     hadoop     177832 Apr  5 13:43 ./__spark_libs__/spark-mllib-local_2.11-2.0.1.jar
17179968605  180 -rwxr-xr-x   1 yarn     hadoop     180736 Apr  5 13:43 ./__spark_libs__/avro-mapred-1.7.7-hadoop2.jar
12893275599   72 -r-xr-xr-x   1 yarn     hadoop      72733 May 10 12:58 ./__spark_libs__/jersey-media-jaxb-2.22.2.jar
12893318189  676 -r-xr-xr-x   1 yarn     hadoop     691479 May 10 12:58 ./__spark_libs__/apacheds-kerberos-codec-2.0.0-M15.jar
4295033642    8 -r-xr-xr-x   1 yarn     hadoop       4596 May 10 12:58 ./__spark_libs__/jul-to-slf4j-1.7.16.jar
1236311  164 -rwxr-xr-x   1 yarn     hadoop     167421 Apr  5 13:43 ./__spark_libs__/jersey-client-2.22.2.jar
4295008610   88 -r-xr-xr-x   1 yarn     hadoop      86811 May 10 12:58 ./__spark_libs__/spire-macros_2.11-0.7.4.jar
1181992   24 -rwxr-xr-x   1 yarn     hadoop      21243 Apr  5 13:43 ./__spark_libs__/parquet-generator-1.7.0.jar
8590865794   96 -rwxr-xr-x   1 yarn     hadoop      96221 Apr  5 13:43 ./__spark_libs__/commons-pool-1.5.4.jar
1236328 1772 -rwxr-xr-x   1 yarn     hadoop    1814309 Apr  5 13:43 ./__spark_libs__/spark-hive-thriftserver_2.11-2.0.1.jar
1181961  308 -rwxr-xr-x   1 yarn     hadoop     313686 Apr  5 13:43 ./__spark_libs__/libfb303-0.9.2.jar
12893307175   20 -r-xr-xr-x   1 yarn     hadoop      16560 May 10 12:58 ./__spark_libs__/api-asn1-api-1.0.0-M20.jar
17180307172  400 -rwxr-xr-x   1 yarn     hadoop     409467 Apr  5 13:43 ./__spark_libs__/mx4j-3.0.2.jar
21474966380   48 -r-xr-xr-x   1 yarn     hadoop      48720 May 10 12:58 ./__spark_libs__/snappy-0.2.jar
8590879092  112 -rwxr-xr-x   1 yarn     hadoop     110600 Apr  5 13:43 ./__spark_libs__/bonecp-0.8.0.RELEASE.jar
8590873373  576 -rwxr-xr-x   1 yarn     hadoop     589462 Apr  5 13:43 ./__spark_libs__/json4s-core_2.11-3.2.11.jar
12893307080  436 -r-xr-xr-x   1 yarn     hadoop     445288 May 10 12:58 ./__spark_libs__/antlr-2.7.7.jar
12893310258  380 -r-xr-xr-x   1 yarn     hadoop     387188 May 10 12:58 ./__spark_libs__/parquet-format-2.3.0-incubating.jar
12893318192  300 -r-xr-xr-x   1 yarn     hadoop     305001 May 10 12:58 ./__spark_libs__/commons-httpclient-3.1.jar
12893741270  504 -rwxr-xr-x   1 yarn     hadoop     515604 Mar 23 14:46 ./__spark_libs__/jackson-module-scala_2.11-2.6.5.jar
17180271152   16 -rwxr-xr-x   1 yarn     hadoop      15305 Apr  5 13:43 ./__spark_libs__/spark-tags_2.11-2.0.1.jar
4295087802   32 -r-xr-xr-x   1 yarn     hadoop      30595 May 10 12:58 ./__spark_libs__/commons-compiler-2.7.6.jar
17180188095  404 -r-xr-xr-x   1 yarn     hadoop     412739 May 10 12:58 ./__spark_libs__/commons-lang3-3.3.2.jar
21474966400  576 -r-xr-xr-x   1 yarn     hadoop     588337 May 10 12:58 ./__spark_libs__/commons-collections-3.2.2.jar
4295009583 3352 -rwxr-xr-x   1 yarn     hadoop    3431544 Mar 23 14:46 ./__spark_libs__/hadoop-common-2.7.1.jar
21474966537  700 -r-xr-xr-x   1 yarn     hadoop     714194 May 10 12:58 ./__spark_libs__/javassist-3.18.1-GA.jar
21474966542  320 -r-xr-xr-x   1 yarn     hadoop     326724 May 10 12:58 ./__spark_libs__/httpcore-4.4.4.jar
8590865817   44 -rwxr-xr-x   1 yarn     hadoop      41263 Apr  5 13:43 ./__spark_libs__/jackson-module-paranamer-2.6.5.jar
17180292351   12 -rwxr-xr-x   1 yarn     hadoop       9939 Apr  5 13:43 ./__spark_libs__/slf4j-log4j12-1.7.16.jar
8590867919  112 -rwxr-xr-x   1 yarn     hadoop     112558 Apr  5 13:43 ./__spark_libs__/metrics-core-3.1.2.jar
8590879060  764 -rwxr-xr-x   1 yarn     hadoop     780664 Apr  5 13:43 ./__spark_libs__/jackson-mapper-asl-1.9.13.jar
4295089788   20 -r-xr-xr-x   1 yarn     hadoop      18482 May 10 12:58 ./__spark_libs__/eigenbase-properties-1.1.5.jar
8590879050  140 -rwxr-xr-x   1 yarn     hadoop     142631 Apr  5 13:43 ./__spark_libs__/hadoop-yarn-client-2.7.1.jar
17179959606 8068 -rwxr-xr-x   1 yarn     hadoop    8260573 Apr  5 13:43 ./__spark_libs__/hadoop-hdfs-2.7.1.jar
12893733094   24 -rwxr-xr-x   1 yarn     hadoop      20852 Mar 23 14:46 ./__spark_libs__/metrics-graphite-3.1.2.jar
8590873344  280 -rwxr-xr-x   1 yarn     hadoop     284184 Apr  5 13:43 ./__spark_libs__/commons-codec-1.10.jar
1165818  388 -rwxr-xr-x   1 yarn     hadoop     395195 Apr  5 13:43 ./__spark_libs__/javolution-5.5.1.jar
1084188  656 -rwxr-xr-x   1 yarn     hadoop     669589 Apr  5 13:43 ./__spark_libs__/spark-yarn_2.11-2.0.1.jar
12893320881   80 -r-xr-xr-x   1 yarn     hadoop      79912 May 10 12:58 ./__spark_libs__/api-util-1.0.0-M20.jar
17180303612 2008 -rwxr-xr-x   1 yarn     hadoop    2054931 Apr  5 13:43 ./__spark_libs__/netty-all-4.0.29.Final.jar
8590867883  776 -rwxr-xr-x   1 yarn     hadoop     792964 Apr  5 13:43 ./__spark_libs__/zookeeper-3.4.6.jar
21474966389   12 -r-xr-xr-x   1 yarn     hadoop      12131 May 10 12:58 ./__spark_libs__/jpam-1.1.jar
4295008653  480 -r-xr-xr-x   1 yarn     hadoop     489884 May 10 12:58 ./__spark_libs__/log4j-1.2.17.jar
21474880061  164 -r-xr-xr-x   1 yarn     hadoop     164368 May 10 12:58 ./__spark_libs__/antlr-runtime-3.4.jar
17179960134 1512 -rwxr-xr-x   1 yarn     hadoop    1544875 Apr  5 13:43 ./__spark_libs__/hadoop-mapreduce-client-core-2.7.1.jar
21474965799 2780 -r-xr-xr-x   1 yarn     hadoop    2842667 May 10 12:58 ./__spark_libs__/bcprov-jdk15on-1.51.jar
975468   20 -rwxr-xr-x   1 yarn     hadoop      16430 Apr  5 13:43 ./__spark_libs__/jcl-over-slf4j-1.7.16.jar
17179968273 1988 -rwxr-xr-x   1 yarn     hadoop    2035066 Apr  5 13:43 ./__spark_libs__/commons-math3-3.4.1.jar
8590873357  436 -rwxr-xr-x   1 yarn     hadoop     442406 Apr  5 13:43 ./__spark_libs__/calcite-linq4j-1.2.0-incubating.jar
21474966409  200 -r-xr-xr-x   1 yarn     hadoop     201124 May 10 12:58 ./__spark_libs__/jdo-api-3.0.1.jar
8590867915   64 -rwxr-xr-x   1 yarn     hadoop      63316 Apr  5 13:43 ./__spark_libs__/spark-repl_2.11-2.0.1.jar
8590873400 1144 -rwxr-xr-x   1 yarn     hadoop    1171380 Apr  5 13:43 ./__spark_libs__/jackson-databind-2.6.5.jar
21474966412   28 -r-xr-xr-x   1 yarn     hadoop      26366 May 10 12:58 ./__spark_libs__/javax.annotation-api-1.2.jar
8590873402  180 -rwxr-xr-x   1 yarn     hadoop     181271 Apr  5 13:43 ./__spark_libs__/hk2-locator-2.4.0-b34.jar
17180307472  736 -rwxr-xr-x   1 yarn     hadoop     753012 Apr  5 13:43 ./__spark_libs__/hadoop-mapreduce-client-common-2.7.1.jar
1181958  420 -rwxr-xr-x   1 yarn     hadoop     427780 Apr  5 13:43 ./__spark_libs__/jodd-core-3.5.2.jar
1084229 1256 -rwxr-xr-x   1 yarn     hadoop    1282424 Apr  5 13:43 ./__spark_libs__/ivy-2.4.0.jar
17180272337  428 -rwxr-xr-x   1 yarn     hadoop     436303 Apr  5 13:43 ./__spark_libs__/avro-1.7.7.jar
17180271181 13136 -rwxr-xr-x   1 yarn     hadoop   13448966 Apr  5 13:43 ./__spark_libs__/breeze_2.11-0.11.2.jar
21474889177    4 drwx------   2 yarn     hadoop       4096 May 16 11:11 ./__spark_conf__
21474890244    8 -r-x------   1 yarn     hadoop       6836 May 16 11:11 ./__spark_conf__/mapred-site.xml
21474890253    8 -r-x------   1 yarn     hadoop       5467 May 16 11:11 ./__spark_conf__/hadoop-env.sh
21474890254   12 -r-x------   1 yarn     hadoop       8699 May 16 11:11 ./__spark_conf__/log4j.properties
21474890258    4 -r-x------   1 yarn     hadoop       2302 May 16 11:11 ./__spark_conf__/hadoop-metrics2.properties
21474890263   20 -r-x------   1 yarn     hadoop      16530 May 16 11:11 ./__spark_conf__/yarn-site.xml
21474890271    4 -r-x------   1 yarn     hadoop       3979 May 16 11:11 ./__spark_conf__/hadoop-env.cmd
21474890276    0 -r-x------   1 yarn     hadoop          0 May 16 11:11 ./__spark_conf__/yarn.exclude
21474890286    8 -r-x------   1 yarn     hadoop       4197 May 16 11:11 ./__spark_conf__/core-site.xml
21474890295    4 -r-x------   1 yarn     hadoop       1631 May 16 11:11 ./__spark_conf__/kms-log4j.properties
21474890301    4 -r-x------   1 yarn     hadoop       2250 May 16 11:11 ./__spark_conf__/yarn-env.cmd
21474890336    4 -r-x------   1 yarn     hadoop        884 May 16 11:11 ./__spark_conf__/ssl-client.xml
21474890337    8 -r-x------   1 yarn     hadoop       5404 May 16 11:11 ./__spark_conf__/capacity-scheduler.xml
21474890341    4 -r-x------   1 yarn     hadoop       3518 May 16 11:11 ./__spark_conf__/kms-acls.xml
21474890348    4 -r-x------   1 yarn     hadoop        758 May 16 11:11 ./__spark_conf__/mapred-site.xml.template
21474890762    4 -r-x------   1 yarn     hadoop       2358 May 16 11:11 ./__spark_conf__/topology_script.py
21474890764    4 -r-x------   1 yarn     hadoop       1335 May 16 11:11 ./__spark_conf__/configuration.xsl
21474890765    8 -r-x------   1 yarn     hadoop       5006 May 16 11:11 ./__spark_conf__/yarn-env.sh
21474890767    8 -r-x------   1 yarn     hadoop       6572 May 16 11:11 ./__spark_conf__/hdfs-site.xml
21474890776    4 -r-x------   1 yarn     hadoop       1019 May 16 11:11 ./__spark_conf__/container-executor.cfg
21474890815    4 -r-x------   1 yarn     hadoop       1020 May 16 11:11 ./__spark_conf__/commons-logging.properties
21474890816    4 -r-x------   1 yarn     hadoop       2490 May 16 11:11 ./__spark_conf__/hadoop-metrics.properties
21474890833    8 -r-x------   1 yarn     hadoop       4221 May 16 11:11 ./__spark_conf__/task-log4j.properties
21474890835    4 -r-x------   1 yarn     hadoop        690 May 16 11:11 ./__spark_conf__/mapred-env.sh
21474890838    4 -r-x------   1 yarn     hadoop       1602 May 16 11:11 ./__spark_conf__/health_check
21474890840    4 -r-x------   1 yarn     hadoop       2316 May 16 11:11 ./__spark_conf__/ssl-client.xml.example
21474890848    4 -r-x------   1 yarn     hadoop       1527 May 16 11:11 ./__spark_conf__/kms-env.sh
21474890863    4 -r-x------   1 yarn     hadoop       1308 May 16 11:11 ./__spark_conf__/hadoop-policy.xml
21474891629    4 -r-x------   1 yarn     hadoop        229 May 16 11:11 ./__spark_conf__/slaves
21474891630    4 -r-x------   1 yarn     hadoop        724 May 16 11:11 ./__spark_conf__/topology_mappings.data
21474891631    4 -r-x------   1 yarn     hadoop        951 May 16 11:11 ./__spark_conf__/mapred-env.cmd
21474891633    4 -r-x------   1 yarn     hadoop       1000 May 16 11:11 ./__spark_conf__/ssl-server.xml
21474891634    4 -r-x------   1 yarn     hadoop       2268 May 16 11:11 ./__spark_conf__/ssl-server.xml.example
21474891636    8 -r-x------   1 yarn     hadoop       4113 May 16 11:11 ./__spark_conf__/mapred-queues.xml.template
21474897153    8 -r-x------   1 yarn     hadoop       5511 May 16 11:11 ./__spark_conf__/kms-site.xml
21474897155    4 -r-x------   1 yarn     hadoop        945 May 16 11:11 ./__spark_conf__/taskcontroller.cfg
21474897166   36 -r-x------   1 yarn     hadoop      34061 May 16 11:11 ./__spark_conf__/__spark_conf__.properties
8590305273 1152 -r-x------   1 yarn     hadoop    1178454 May 16 11:11 ./__app__.jar
broken symlinks(find -L . -maxdepth 5 -type l -ls):
End of LogType:directory.info

LogType:launch_container.sh
Log Upload Time:Tue May 16 11:12:19 +0700 2017
LogLength:75763
Log Contents:
#!/bin/bash

export SPARK_YARN_STAGING_DIR="hdfs://c408.hadoop.gda.lo:8020/user/fairy/.sparkStaging/application_1494395298335_303847"
export LOCAL_DIRS="/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847"
export HADOOP_CONF_DIR="/usr/hdp/current/hadoop-client/conf"
export NM_HTTP_PORT="8042"
export JAVA_HOME="/usr/jdk64/jdk1.8.0_92/"
export LOG_DIRS="/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000005"
export NM_AUX_SERVICE_mapreduce_shuffle="AAA0+gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=
"
export NM_PORT="45454"
export USER="fairy"
export HADOOP_YARN_HOME="/usr/hdp/current/hadoop-yarn-nodemanager"
export CLASSPATH="$PWD:$PWD/__spark_conf__:$PWD/__spark_libs__/*:$HADOOP_CONF_DIR:/usr/hdp/current/hadoop-client/*:/usr/hdp/current/hadoop-client/lib/*:/usr/hdp/current/hadoop-hdfs-client/*:/usr/hdp/current/hadoop-hdfs-client/lib/*:/usr/hdp/current/hadoop-yarn-client/*:/usr/hdp/current/hadoop-yarn-client/lib/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.4.2.0-258/hadoop/lib/hadoop-lzo-0.6.0.2.4.2.0-258.jar:/etc/hadoop/conf/secure"
export SPARK_LOG_URL_STDOUT="http://c419.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_02_000005/fairy/stdout?start=-4096"
export NM_HOST="c419.hadoop.gda.lo"
export SPARK_YARN_MODE="true"
export HADOOP_TOKEN_FILE_LOCATION="/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_02_000005/container_tokens"
export NM_AUX_SERVICE_spark_shuffle=""
export SPARK_USER="fairy"
export SPARK_LOG_URL_STDERR="http://c419.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_02_000005/fairy/stderr?start=-4096"
export LOGNAME="fairy"
export JVM_PID="$$"
export PWD="/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_02_000005"
export HOME="/home/"
export CONTAINER_ID="container_e81_1494395298335_303847_02_000005"
export MALLOC_ARENA_MAX="4"
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64131/hadoop-yarn-server-common-2.7.1.jar" "__spark_libs__/hadoop-yarn-server-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64110/hive-metastore-1.2.1.spark2.jar" "__spark_libs__/hive-metastore-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55201/jetty-6.1.26.jar" "__spark_libs__/jetty-6.1.26.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64122/py4j-0.10.3.jar" "__spark_libs__/py4j-0.10.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64104/hadoop-annotations-2.7.1.jar" "__spark_libs__/hadoop-annotations-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55212/activation-1.1.1.jar" "__spark_libs__/activation-1.1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/49678/arpack_combined_all-0.1.jar" "__spark_libs__/arpack_combined_all-0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64109/jsp-api-2.1.jar" "__spark_libs__/jsp-api-2.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64149/derby-10.12.1.1.jar" "__spark_libs__/derby-10.12.1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64136/spark-mllib_2.11-2.0.1.jar" "__spark_libs__/spark-mllib_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55180/datanucleus-core-3.2.10.jar" "__spark_libs__/datanucleus-core-3.2.10.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55178/pyrolite-4.9.jar" "__spark_libs__/pyrolite-4.9.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55164/jersey-server-2.22.2.jar" "__spark_libs__/jersey-server-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/49674/hadoop-yarn-api-2.7.1.jar" "__spark_libs__/hadoop-yarn-api-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/49670/jackson-xc-1.9.13.jar" "__spark_libs__/jackson-xc-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55168/commons-dbcp-1.4.jar" "__spark_libs__/commons-dbcp-1.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55134/hadoop-client-2.7.1.jar" "__spark_libs__/hadoop-client-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55199/aopalliance-1.0.jar" "__spark_libs__/aopalliance-1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64102/netty-3.8.0.Final.jar" "__spark_libs__/netty-3.8.0.Final.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64156/kryo-shaded-3.0.3.jar" "__spark_libs__/kryo-shaded-3.0.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55208/calcite-avatica-1.2.0-incubating.jar" "__spark_libs__/calcite-avatica-1.2.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55206/scala-library-2.11.8.jar" "__spark_libs__/scala-library-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55169/univocity-parsers-2.1.1.jar" "__spark_libs__/univocity-parsers-2.1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55136/javax.servlet-api-3.1.0.jar" "__spark_libs__/javax.servlet-api-3.1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64144/commons-net-2.2.jar" "__spark_libs__/commons-net-2.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55130/spark-network-shuffle_2.11-2.0.1.jar" "__spark_libs__/spark-network-shuffle_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55127/guice-servlet-3.0.jar" "__spark_libs__/guice-servlet-3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64118/jersey-guava-2.22.2.jar" "__spark_libs__/jersey-guava-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55210/spark-catalyst_2.11-2.0.1.jar" "__spark_libs__/spark-catalyst_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55182/slf4j-api-1.7.16.jar" "__spark_libs__/slf4j-api-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/49667/parquet-jackson-1.7.0.jar" "__spark_libs__/parquet-jackson-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55203/avro-ipc-1.7.7.jar" "__spark_libs__/avro-ipc-1.7.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64113/parquet-encoding-1.7.0.jar" "__spark_libs__/parquet-encoding-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/49661/osgi-resource-locator-1.0.1.jar" "__spark_libs__/osgi-resource-locator-1.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55129/metrics-json-3.1.2.jar" "__spark_libs__/metrics-json-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/49665/mail-1.4.7.jar" "__spark_libs__/mail-1.4.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55220/spark-hive_2.11-2.0.1.jar" "__spark_libs__/spark-hive_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55223/stax-api-1.0.1.jar" "__spark_libs__/stax-api-1.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55175/jersey-container-servlet-2.22.2.jar" "__spark_libs__/jersey-container-servlet-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55229/spire_2.11-0.7.4.jar" "__spark_libs__/spire_2.11-0.7.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55193/javax.inject-1.jar" "__spark_libs__/javax.inject-1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55123/jetty-util-6.1.26.jar" "__spark_libs__/jetty-util-6.1.26.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55153/jersey-container-servlet-core-2.22.2.jar" "__spark_libs__/jersey-container-servlet-core-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55157/parquet-hadoop-bundle-1.6.0.jar" "__spark_libs__/parquet-hadoop-bundle-1.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64133/chill_2.11-0.8.0.jar" "__spark_libs__/chill_2.11-0.8.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64152/jsr305-1.3.9.jar" "__spark_libs__/jsr305-1.3.9.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55138/htrace-core-3.1.0-incubating.jar" "__spark_libs__/htrace-core-3.1.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55187/hadoop-yarn-server-web-proxy-2.7.1.jar" "__spark_libs__/hadoop-yarn-server-web-proxy-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55197/leveldbjni-all-1.8.jar" "__spark_libs__/leveldbjni-all-1.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/49681/guava-14.0.1.jar" "__spark_libs__/guava-14.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55192/jersey-bundle-1.19.1.jar" "__spark_libs__/jersey-bundle-1.19.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64128/hadoop-auth-2.7.1.jar" "__spark_libs__/hadoop-auth-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64150/datanucleus-rdbms-3.2.9.jar" "__spark_libs__/datanucleus-rdbms-3.2.9.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64106/aopalliance-repackaged-2.4.0-b34.jar" "__spark_libs__/aopalliance-repackaged-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55232/hadoop-mapreduce-client-app-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-app-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55144/spark-streaming_2.11-2.0.1.jar" "__spark_libs__/spark-streaming_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/49675/jline-2.12.1.jar" "__spark_libs__/jline-2.12.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55172/jackson-core-asl-1.9.13.jar" "__spark_libs__/jackson-core-asl-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/49679/hadoop-mapreduce-client-jobclient-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-jobclient-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55219/hk2-utils-2.4.0-b34.jar" "__spark_libs__/hk2-utils-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55207/commons-compress-1.4.1.jar" "__spark_libs__/commons-compress-1.4.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64142/spark-core_2.11-2.0.1.jar" "__spark_libs__/spark-core_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55150/commons-beanutils-1.7.0.jar" "__spark_libs__/commons-beanutils-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64145/spark-sql_2.11-2.0.1.jar" "__spark_libs__/spark-sql_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64112/scala-parser-combinators_2.11-1.0.4.jar" "__spark_libs__/scala-parser-combinators_2.11-1.0.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64157/guice-3.0.jar" "__spark_libs__/guice-3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/49664/commons-logging-1.1.3.jar" "__spark_libs__/commons-logging-1.1.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64135/lz4-1.3.0.jar" "__spark_libs__/lz4-1.3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/usercache/fairy/filecache/37978/__spark_conf__.zip" "__spark_conf__"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64147/hive-beeline-1.2.1.spark2.jar" "__spark_libs__/hive-beeline-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55196/ST4-4.0.4.jar" "__spark_libs__/ST4-4.0.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55174/opencsv-2.3.jar" "__spark_libs__/opencsv-2.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/49672/spark-graphx_2.11-2.0.1.jar" "__spark_libs__/spark-graphx_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55131/jackson-annotations-2.6.5.jar" "__spark_libs__/jackson-annotations-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/49680/apache-log4j-extras-1.2.17.jar" "__spark_libs__/apache-log4j-extras-1.2.17.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64140/minlog-1.3.0.jar" "__spark_libs__/minlog-1.3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55133/json-20090211.jar" "__spark_libs__/json-20090211.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/usercache/fairy/filecache/37977/stats-etlr-1.0.jar" "__app__.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64111/spark-unsafe_2.11-2.0.1.jar" "__spark_libs__/spark-unsafe_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55173/commons-digester-1.8.jar" "__spark_libs__/commons-digester-1.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55159/mysql-connector-java-5.0.8-bin.jar" "__spark_libs__/mysql-connector-java-5.0.8-bin.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55165/curator-recipes-2.6.0.jar" "__spark_libs__/curator-recipes-2.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55145/scala-compiler-2.11.8.jar" "__spark_libs__/scala-compiler-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64155/java-xmlbuilder-1.0.jar" "__spark_libs__/java-xmlbuilder-1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55224/commons-cli-1.2.jar" "__spark_libs__/commons-cli-1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55128/apacheds-i18n-2.0.0-M15.jar" "__spark_libs__/apacheds-i18n-2.0.0-M15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64121/hadoop-yarn-common-2.7.1.jar" "__spark_libs__/hadoop-yarn-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55215/jta-1.1.jar" "__spark_libs__/jta-1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55162/jackson-core-2.6.5.jar" "__spark_libs__/jackson-core-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/49673/json4s-ast_2.11-3.2.11.jar" "__spark_libs__/json4s-ast_2.11-3.2.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55121/commons-beanutils-core-1.8.0.jar" "__spark_libs__/commons-beanutils-core-1.8.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55179/core-1.1.2.jar" "__spark_libs__/core-1.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64148/spark-network-common_2.11-2.0.1.jar" "__spark_libs__/spark-network-common_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64101/mesos-0.21.1-shaded-protobuf.jar" "__spark_libs__/mesos-0.21.1-shaded-protobuf.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55204/httpclient-4.5.2.jar" "__spark_libs__/httpclient-4.5.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55226/stream-2.7.0.jar" "__spark_libs__/stream-2.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55149/pmml-model-1.2.15.jar" "__spark_libs__/pmml-model-1.2.15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55209/scala-reflect-2.11.8.jar" "__spark_libs__/scala-reflect-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55167/jets3t-0.9.3.jar" "__spark_libs__/jets3t-0.9.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55194/parquet-hadoop-1.7.0.jar" "__spark_libs__/parquet-hadoop-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55195/compress-lzf-1.0.3.jar" "__spark_libs__/compress-lzf-1.0.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64119/stax-api-1.0-2.jar" "__spark_libs__/stax-api-1.0-2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55161/gson-2.2.4.jar" "__spark_libs__/gson-2.2.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55198/hive-exec-1.2.1.spark2.jar" "__spark_libs__/hive-exec-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55132/hive-cli-1.2.1.spark2.jar" "__spark_libs__/hive-cli-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55160/JavaEWAH-0.3.2.jar" "__spark_libs__/JavaEWAH-0.3.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64143/xercesImpl-2.9.1.jar" "__spark_libs__/xercesImpl-2.9.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55170/hk2-api-2.4.0-b34.jar" "__spark_libs__/hk2-api-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64094/protobuf-java-2.5.0.jar" "__spark_libs__/protobuf-java-2.5.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55200/base64-2.3.8.jar" "__spark_libs__/base64-2.3.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64115/jersey-common-2.22.2.jar" "__spark_libs__/jersey-common-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55217/RoaringBitmap-0.5.11.jar" "__spark_libs__/RoaringBitmap-0.5.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55141/objenesis-2.1.jar" "__spark_libs__/objenesis-2.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55191/chill-java-0.8.0.jar" "__spark_libs__/chill-java-0.8.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55154/xbean-asm5-shaded-4.4.jar" "__spark_libs__/xbean-asm5-shaded-4.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64127/hive-jdbc-1.2.1.spark2.jar" "__spark_libs__/hive-jdbc-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55176/janino-2.7.8.jar" "__spark_libs__/janino-2.7.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64108/stringtemplate-3.2.1.jar" "__spark_libs__/stringtemplate-3.2.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55156/metrics-jvm-3.1.2.jar" "__spark_libs__/metrics-jvm-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64105/datanucleus-api-jdo-3.2.6.jar" "__spark_libs__/datanucleus-api-jdo-3.2.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55186/spark-launcher_2.11-2.0.1.jar" "__spark_libs__/spark-launcher_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64114/jtransforms-2.4.0.jar" "__spark_libs__/jtransforms-2.4.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55126/calcite-core-1.2.0-incubating.jar" "__spark_libs__/calcite-core-1.2.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55227/antlr4-runtime-4.5.3.jar" "__spark_libs__/antlr4-runtime-4.5.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55148/json4s-jackson_2.11-3.2.11.jar" "__spark_libs__/json4s-jackson_2.11-3.2.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/49663/curator-framework-2.6.0.jar" "__spark_libs__/curator-framework-2.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55120/libthrift-0.9.2.jar" "__spark_libs__/libthrift-0.9.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64107/commons-lang-2.6.jar" "__spark_libs__/commons-lang-2.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55234/pmml-schema-1.2.15.jar" "__spark_libs__/pmml-schema-1.2.15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/49666/parquet-column-1.7.0.jar" "__spark_libs__/parquet-column-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/49669/jackson-jaxrs-1.9.13.jar" "__spark_libs__/jackson-jaxrs-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64153/xmlenc-0.52.jar" "__spark_libs__/xmlenc-0.52.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55135/scala-xml_2.11-1.0.2.jar" "__spark_libs__/scala-xml_2.11-1.0.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55177/breeze-macros_2.11-0.11.2.jar" "__spark_libs__/breeze-macros_2.11-0.11.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64093/spark-sketch_2.11-2.0.1.jar" "__spark_libs__/spark-sketch_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/49668/xz-1.0.jar" "__spark_libs__/xz-1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55142/validation-api-1.1.0.Final.jar" "__spark_libs__/validation-api-1.1.0.Final.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64097/commons-io-2.4.jar" "__spark_libs__/commons-io-2.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64098/jaxb-api-2.2.2.jar" "__spark_libs__/jaxb-api-2.2.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64120/javax.ws.rs-api-2.0.1.jar" "__spark_libs__/javax.ws.rs-api-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55218/lift-json_2.11-2.6.3.jar" "__spark_libs__/lift-json_2.11-2.6.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/49671/hadoop-mapreduce-client-shuffle-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-shuffle-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64123/commons-configuration-1.6.jar" "__spark_libs__/commons-configuration-1.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64151/joda-time-2.9.3.jar" "__spark_libs__/joda-time-2.9.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64103/curator-client-2.6.0.jar" "__spark_libs__/curator-client-2.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64158/oro-2.0.8.jar" "__spark_libs__/oro-2.0.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64161/snappy-java-1.1.2.6.jar" "__spark_libs__/snappy-java-1.1.2.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55158/paranamer-2.3.jar" "__spark_libs__/paranamer-2.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55137/javax.inject-2.4.0-b34.jar" "__spark_libs__/javax.inject-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55171/scalap-2.11.8.jar" "__spark_libs__/scalap-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64130/parquet-common-1.7.0.jar" "__spark_libs__/parquet-common-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55125/super-csv-2.2.0.jar" "__spark_libs__/super-csv-2.2.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55152/spark-mllib-local_2.11-2.0.1.jar" "__spark_libs__/spark-mllib-local_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55122/avro-mapred-1.7.7-hadoop2.jar" "__spark_libs__/avro-mapred-1.7.7-hadoop2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64092/jersey-media-jaxb-2.22.2.jar" "__spark_libs__/jersey-media-jaxb-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64134/apacheds-kerberos-codec-2.0.0-M15.jar" "__spark_libs__/apacheds-kerberos-codec-2.0.0-M15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64139/jul-to-slf4j-1.7.16.jar" "__spark_libs__/jul-to-slf4j-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55231/jersey-client-2.22.2.jar" "__spark_libs__/jersey-client-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64100/spire-macros_2.11-0.7.4.jar" "__spark_libs__/spire-macros_2.11-0.7.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55225/parquet-generator-1.7.0.jar" "__spark_libs__/parquet-generator-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55139/commons-pool-1.5.4.jar" "__spark_libs__/commons-pool-1.5.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55222/spark-hive-thriftserver_2.11-2.0.1.jar" "__spark_libs__/spark-hive-thriftserver_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55228/libfb303-0.9.2.jar" "__spark_libs__/libfb303-0.9.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64116/api-asn1-api-1.0.0-M20.jar" "__spark_libs__/api-asn1-api-1.0.0-M20.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55221/mx4j-3.0.2.jar" "__spark_libs__/mx4j-3.0.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64117/snappy-0.2.jar" "__spark_libs__/snappy-0.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55190/bonecp-0.8.0.RELEASE.jar" "__spark_libs__/bonecp-0.8.0.RELEASE.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55151/json4s-core_2.11-3.2.11.jar" "__spark_libs__/json4s-core_2.11-3.2.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64095/antlr-2.7.7.jar" "__spark_libs__/antlr-2.7.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64125/parquet-format-2.3.0-incubating.jar" "__spark_libs__/parquet-format-2.3.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64137/commons-httpclient-3.1.jar" "__spark_libs__/commons-httpclient-3.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/49677/jackson-module-scala_2.11-2.6.5.jar" "__spark_libs__/jackson-module-scala_2.11-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55140/spark-tags_2.11-2.0.1.jar" "__spark_libs__/spark-tags_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64154/commons-compiler-2.7.6.jar" "__spark_libs__/commons-compiler-2.7.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64129/commons-lang3-3.3.2.jar" "__spark_libs__/commons-lang3-3.3.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64132/commons-collections-3.2.2.jar" "__spark_libs__/commons-collections-3.2.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/49676/hadoop-common-2.7.1.jar" "__spark_libs__/hadoop-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64159/javassist-3.18.1-GA.jar" "__spark_libs__/javassist-3.18.1-GA.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64162/httpcore-4.4.4.jar" "__spark_libs__/httpcore-4.4.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55166/jackson-module-paranamer-2.6.5.jar" "__spark_libs__/jackson-module-paranamer-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55188/slf4j-log4j12-1.7.16.jar" "__spark_libs__/slf4j-log4j12-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55118/metrics-core-3.1.2.jar" "__spark_libs__/metrics-core-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55184/jackson-mapper-asl-1.9.13.jar" "__spark_libs__/jackson-mapper-asl-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64160/eigenbase-properties-1.1.5.jar" "__spark_libs__/eigenbase-properties-1.1.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55214/hadoop-yarn-client-2.7.1.jar" "__spark_libs__/hadoop-yarn-client-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55143/hadoop-hdfs-2.7.1.jar" "__spark_libs__/hadoop-hdfs-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/49662/metrics-graphite-3.1.2.jar" "__spark_libs__/metrics-graphite-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55181/commons-codec-1.10.jar" "__spark_libs__/commons-codec-1.10.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55216/javolution-5.5.1.jar" "__spark_libs__/javolution-5.5.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55183/spark-yarn_2.11-2.0.1.jar" "__spark_libs__/spark-yarn_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64146/api-util-1.0.0-M20.jar" "__spark_libs__/api-util-1.0.0-M20.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55230/netty-all-4.0.29.Final.jar" "__spark_libs__/netty-all-4.0.29.Final.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55124/zookeeper-3.4.6.jar" "__spark_libs__/zookeeper-3.4.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64126/jpam-1.1.jar" "__spark_libs__/jpam-1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64124/log4j-1.2.17.jar" "__spark_libs__/log4j-1.2.17.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64096/antlr-runtime-3.4.jar" "__spark_libs__/antlr-runtime-3.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55119/hadoop-mapreduce-client-core-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-core-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64099/bcprov-jdk15on-1.51.jar" "__spark_libs__/bcprov-jdk15on-1.51.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55147/jcl-over-slf4j-1.7.16.jar" "__spark_libs__/jcl-over-slf4j-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55155/commons-math3-3.4.1.jar" "__spark_libs__/commons-math3-3.4.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55205/calcite-linq4j-1.2.0-incubating.jar" "__spark_libs__/calcite-linq4j-1.2.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64138/jdo-api-3.0.1.jar" "__spark_libs__/jdo-api-3.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55163/spark-repl_2.11-2.0.1.jar" "__spark_libs__/spark-repl_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55211/jackson-databind-2.6.5.jar" "__spark_libs__/jackson-databind-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/64141/javax.annotation-api-1.2.jar" "__spark_libs__/javax.annotation-api-1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55202/hk2-locator-2.4.0-b34.jar" "__spark_libs__/hk2-locator-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55233/hadoop-mapreduce-client-common-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55213/jodd-core-3.5.2.jar" "__spark_libs__/jodd-core-3.5.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55189/ivy-2.4.0.jar" "__spark_libs__/ivy-2.4.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55146/avro-1.7.7.jar" "__spark_libs__/avro-1.7.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55185/breeze_2.11-0.11.2.jar" "__spark_libs__/breeze_2.11-0.11.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
# Creating copy of launch script
cp "launch_container.sh" "/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000005/launch_container.sh"
chmod 640 "/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000005/launch_container.sh"
# Determining directory contents
echo "ls -l:" 1>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000005/directory.info"
ls -l 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000005/directory.info"
echo "find -L . -maxdepth 5 -ls:" 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000005/directory.info"
find -L . -maxdepth 5 -ls 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000005/directory.info"
echo "broken symlinks(find -L . -maxdepth 5 -type l -ls):" 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000005/directory.info"
find -L . -maxdepth 5 -type l -ls 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000005/directory.info"
exec /bin/bash -c "$JAVA_HOME/bin/java -server -Xmx3072m '-Dhdp.version=2.4.2.0-258' -Djava.io.tmpdir=$PWD/tmp '-Dspark.driver.port=39032' '-Dspark.history.ui.port=18080' '-Dspark.ui.port=0' -Dspark.yarn.app.container.log.dir=/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000005 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.60.43.16:39032 --executor-id 4 --hostname c419.hadoop.gda.lo --cores 1 --app-id application_1494395298335_303847 --user-class-path file:$PWD/__app__.jar 1> /hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000005/stdout 2> /hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_02_000005/stderr"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
End of LogType:launch_container.sh

LogType:stderr
Log Upload Time:Tue May 16 11:12:19 +0700 2017
LogLength:40298
Log Contents:
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/var/lib/hadoop/yarn/local/filecache/55188/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/hdp/2.4.2.0-258/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
17/05/16 11:11:04 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 12687@c419.hadoop.gda.lo
17/05/16 11:11:04 INFO util.SignalUtils: Registered signal handler for TERM
17/05/16 11:11:04 INFO util.SignalUtils: Registered signal handler for HUP
17/05/16 11:11:04 INFO util.SignalUtils: Registered signal handler for INT
17/05/16 11:11:04 INFO spark.SecurityManager: Changing view acls to: yarn,fairy
17/05/16 11:11:04 INFO spark.SecurityManager: Changing modify acls to: yarn,fairy
17/05/16 11:11:04 INFO spark.SecurityManager: Changing view acls groups to: 
17/05/16 11:11:04 INFO spark.SecurityManager: Changing modify acls groups to: 
17/05/16 11:11:04 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(yarn, fairy); groups with view permissions: Set(); users  with modify permissions: Set(yarn, fairy); groups with modify permissions: Set()
17/05/16 11:11:05 INFO client.TransportClientFactory: Successfully created connection to /10.60.43.16:39032 after 102 ms (0 ms spent in bootstraps)
17/05/16 11:11:05 INFO spark.SecurityManager: Changing view acls to: yarn,fairy
17/05/16 11:11:05 INFO spark.SecurityManager: Changing modify acls to: yarn,fairy
17/05/16 11:11:05 INFO spark.SecurityManager: Changing view acls groups to: 
17/05/16 11:11:05 INFO spark.SecurityManager: Changing modify acls groups to: 
17/05/16 11:11:05 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(yarn, fairy); groups with view permissions: Set(); users  with modify permissions: Set(yarn, fairy); groups with modify permissions: Set()
17/05/16 11:11:05 INFO client.TransportClientFactory: Successfully created connection to /10.60.43.16:39032 after 2 ms (0 ms spent in bootstraps)
17/05/16 11:11:05 INFO storage.DiskBlockManager: Created local directory at /var/lib/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/blockmgr-d1bc2e35-2121-48d5-96fe-f51cd0c746b7
17/05/16 11:11:05 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MB
17/05/16 11:11:05 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@10.60.43.16:39032
17/05/16 11:11:05 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver
17/05/16 11:11:05 INFO executor.Executor: Starting executor ID 4 on host c419.hadoop.gda.lo
17/05/16 11:11:06 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 27876.
17/05/16 11:11:06 INFO netty.NettyBlockTransferService: Server created on c419.hadoop.gda.lo:27876
17/05/16 11:11:06 INFO storage.BlockManager: external shuffle service port = 7337
17/05/16 11:11:06 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(4, c419.hadoop.gda.lo, 27876)
17/05/16 11:11:06 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(4, c419.hadoop.gda.lo, 27876)
17/05/16 11:11:06 INFO storage.BlockManager: Registering executor with local external shuffle service.
17/05/16 11:11:06 INFO client.TransportClientFactory: Successfully created connection to c419.hadoop.gda.lo/10.60.43.19:7337 after 1 ms (0 ms spent in bootstraps)
17/05/16 11:11:18 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1
17/05/16 11:11:18 INFO executor.Executor: Running task 1.0 in stage 1.0 (TID 1)
17/05/16 11:11:18 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 0
17/05/16 11:11:18 INFO client.TransportClientFactory: Successfully created connection to /10.60.43.16:33694 after 2 ms (0 ms spent in bootstraps)
17/05/16 11:11:18 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.2 KB, free 1458.6 MB)
17/05/16 11:11:18 INFO broadcast.TorrentBroadcast: Reading broadcast variable 0 took 167 ms
17/05/16 11:11:18 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 30.3 KB, free 1458.6 MB)
17/05/16 11:11:18 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
17/05/16 11:11:18 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.60.43.16:39032)
17/05/16 11:11:18 INFO spark.MapOutputTrackerWorker: Got the output locations
17/05/16 11:11:18 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:18 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
17/05/16 11:11:18 INFO codegen.CodeGenerator: Code generated in 291.364107 ms
17/05/16 11:11:19 INFO codegen.CodeGenerator: Code generated in 10.781619 ms
17/05/16 11:11:19 INFO codegen.CodeGenerator: Code generated in 9.792022 ms
17/05/16 11:11:19 INFO codegen.CodeGenerator: Code generated in 9.012733 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 1.0 in stage 1.0 (TID 1). 4219 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 4
17/05/16 11:11:19 INFO executor.Executor: Running task 4.0 in stage 1.0 (TID 4)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 4.0 in stage 1.0 (TID 4). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 5
17/05/16 11:11:19 INFO executor.Executor: Running task 5.0 in stage 1.0 (TID 5)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 5.0 in stage 1.0 (TID 5). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 6
17/05/16 11:11:19 INFO executor.Executor: Running task 6.0 in stage 1.0 (TID 6)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 6.0 in stage 1.0 (TID 6). 3414 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 7
17/05/16 11:11:19 INFO executor.Executor: Running task 7.0 in stage 1.0 (TID 7)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 7.0 in stage 1.0 (TID 7). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 8
17/05/16 11:11:19 INFO executor.Executor: Running task 8.0 in stage 1.0 (TID 8)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 8.0 in stage 1.0 (TID 8). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 9
17/05/16 11:11:19 INFO executor.Executor: Running task 9.0 in stage 1.0 (TID 9)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 9.0 in stage 1.0 (TID 9). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 12
17/05/16 11:11:19 INFO executor.Executor: Running task 12.0 in stage 1.0 (TID 12)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 12.0 in stage 1.0 (TID 12). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 13
17/05/16 11:11:19 INFO executor.Executor: Running task 13.0 in stage 1.0 (TID 13)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 13.0 in stage 1.0 (TID 13). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 15
17/05/16 11:11:19 INFO executor.Executor: Running task 15.0 in stage 1.0 (TID 15)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 15.0 in stage 1.0 (TID 15). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 18
17/05/16 11:11:19 INFO executor.Executor: Running task 18.0 in stage 1.0 (TID 18)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 18.0 in stage 1.0 (TID 18). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 20
17/05/16 11:11:19 INFO executor.Executor: Running task 20.0 in stage 1.0 (TID 20)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 20.0 in stage 1.0 (TID 20). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 22
17/05/16 11:11:19 INFO executor.Executor: Running task 22.0 in stage 1.0 (TID 22)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 22.0 in stage 1.0 (TID 22). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 24
17/05/16 11:11:19 INFO executor.Executor: Running task 24.0 in stage 1.0 (TID 24)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 24.0 in stage 1.0 (TID 24). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 27
17/05/16 11:11:19 INFO executor.Executor: Running task 27.0 in stage 1.0 (TID 27)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 27.0 in stage 1.0 (TID 27). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 29
17/05/16 11:11:19 INFO executor.Executor: Running task 29.0 in stage 1.0 (TID 29)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 29.0 in stage 1.0 (TID 29). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 32
17/05/16 11:11:19 INFO executor.Executor: Running task 32.0 in stage 1.0 (TID 32)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 32.0 in stage 1.0 (TID 32). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 35
17/05/16 11:11:19 INFO executor.Executor: Running task 35.0 in stage 1.0 (TID 35)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 35.0 in stage 1.0 (TID 35). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 37
17/05/16 11:11:19 INFO executor.Executor: Running task 37.0 in stage 1.0 (TID 37)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 37.0 in stage 1.0 (TID 37). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 39
17/05/16 11:11:19 INFO executor.Executor: Running task 39.0 in stage 1.0 (TID 39)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 39.0 in stage 1.0 (TID 39). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 42
17/05/16 11:11:19 INFO executor.Executor: Running task 42.0 in stage 1.0 (TID 42)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 42.0 in stage 1.0 (TID 42). 3414 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 45
17/05/16 11:11:19 INFO executor.Executor: Running task 45.0 in stage 1.0 (TID 45)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 45.0 in stage 1.0 (TID 45). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 47
17/05/16 11:11:19 INFO executor.Executor: Running task 47.0 in stage 1.0 (TID 47)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 47.0 in stage 1.0 (TID 47). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 49
17/05/16 11:11:19 INFO executor.Executor: Running task 49.0 in stage 1.0 (TID 49)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 49.0 in stage 1.0 (TID 49). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 52
17/05/16 11:11:19 INFO executor.Executor: Running task 52.0 in stage 1.0 (TID 52)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 52.0 in stage 1.0 (TID 52). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 54
17/05/16 11:11:19 INFO executor.Executor: Running task 54.0 in stage 1.0 (TID 54)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 54.0 in stage 1.0 (TID 54). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 57
17/05/16 11:11:19 INFO executor.Executor: Running task 57.0 in stage 1.0 (TID 57)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 57.0 in stage 1.0 (TID 57). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 60
17/05/16 11:11:19 INFO executor.Executor: Running task 60.0 in stage 1.0 (TID 60)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 60.0 in stage 1.0 (TID 60). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 63
17/05/16 11:11:19 INFO executor.Executor: Running task 63.0 in stage 1.0 (TID 63)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 63.0 in stage 1.0 (TID 63). 3414 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 66
17/05/16 11:11:19 INFO executor.Executor: Running task 66.0 in stage 1.0 (TID 66)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 66.0 in stage 1.0 (TID 66). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 69
17/05/16 11:11:19 INFO executor.Executor: Running task 69.0 in stage 1.0 (TID 69)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 69.0 in stage 1.0 (TID 69). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 73
17/05/16 11:11:19 INFO executor.Executor: Running task 73.0 in stage 1.0 (TID 73)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 73.0 in stage 1.0 (TID 73). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 76
17/05/16 11:11:19 INFO executor.Executor: Running task 76.0 in stage 1.0 (TID 76)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 76.0 in stage 1.0 (TID 76). 3414 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 79
17/05/16 11:11:19 INFO executor.Executor: Running task 79.0 in stage 1.0 (TID 79)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 79.0 in stage 1.0 (TID 79). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 83
17/05/16 11:11:19 INFO executor.Executor: Running task 83.0 in stage 1.0 (TID 83)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 83.0 in stage 1.0 (TID 83). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 85
17/05/16 11:11:19 INFO executor.Executor: Running task 85.0 in stage 1.0 (TID 85)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 85.0 in stage 1.0 (TID 85). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 89
17/05/16 11:11:19 INFO executor.Executor: Running task 89.0 in stage 1.0 (TID 89)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 89.0 in stage 1.0 (TID 89). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 93
17/05/16 11:11:19 INFO executor.Executor: Running task 93.0 in stage 1.0 (TID 93)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 93.0 in stage 1.0 (TID 93). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 97
17/05/16 11:11:19 INFO executor.Executor: Running task 97.0 in stage 1.0 (TID 97)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 97.0 in stage 1.0 (TID 97). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 100
17/05/16 11:11:19 INFO executor.Executor: Running task 100.0 in stage 1.0 (TID 100)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 100.0 in stage 1.0 (TID 100). 3414 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 103
17/05/16 11:11:19 INFO executor.Executor: Running task 103.0 in stage 1.0 (TID 103)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 103.0 in stage 1.0 (TID 103). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 106
17/05/16 11:11:19 INFO executor.Executor: Running task 106.0 in stage 1.0 (TID 106)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 106.0 in stage 1.0 (TID 106). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 109
17/05/16 11:11:19 INFO executor.Executor: Running task 109.0 in stage 1.0 (TID 109)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 109.0 in stage 1.0 (TID 109). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 113
17/05/16 11:11:19 INFO executor.Executor: Running task 113.0 in stage 1.0 (TID 113)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 113.0 in stage 1.0 (TID 113). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 116
17/05/16 11:11:19 INFO executor.Executor: Running task 116.0 in stage 1.0 (TID 116)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 116.0 in stage 1.0 (TID 116). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 120
17/05/16 11:11:19 INFO executor.Executor: Running task 120.0 in stage 1.0 (TID 120)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 120.0 in stage 1.0 (TID 120). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 125
17/05/16 11:11:19 INFO executor.Executor: Running task 125.0 in stage 1.0 (TID 125)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 125.0 in stage 1.0 (TID 125). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 128
17/05/16 11:11:19 INFO executor.Executor: Running task 128.0 in stage 1.0 (TID 128)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 128.0 in stage 1.0 (TID 128). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 131
17/05/16 11:11:19 INFO executor.Executor: Running task 131.0 in stage 1.0 (TID 131)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 131.0 in stage 1.0 (TID 131). 3327 bytes result sent to driver
17/05/16 11:11:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 134
17/05/16 11:11:19 INFO executor.Executor: Running task 134.0 in stage 1.0 (TID 134)
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:19 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:19 INFO executor.Executor: Finished task 134.0 in stage 1.0 (TID 134). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 138
17/05/16 11:11:20 INFO executor.Executor: Running task 138.0 in stage 1.0 (TID 138)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 138.0 in stage 1.0 (TID 138). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 142
17/05/16 11:11:20 INFO executor.Executor: Running task 142.0 in stage 1.0 (TID 142)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 142.0 in stage 1.0 (TID 142). 3414 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 149
17/05/16 11:11:20 INFO executor.Executor: Running task 149.0 in stage 1.0 (TID 149)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 149.0 in stage 1.0 (TID 149). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 151
17/05/16 11:11:20 INFO executor.Executor: Running task 151.0 in stage 1.0 (TID 151)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 151.0 in stage 1.0 (TID 151). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 155
17/05/16 11:11:20 INFO executor.Executor: Running task 155.0 in stage 1.0 (TID 155)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 155.0 in stage 1.0 (TID 155). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 158
17/05/16 11:11:20 INFO executor.Executor: Running task 158.0 in stage 1.0 (TID 158)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 158.0 in stage 1.0 (TID 158). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 161
17/05/16 11:11:20 INFO executor.Executor: Running task 161.0 in stage 1.0 (TID 161)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 161.0 in stage 1.0 (TID 161). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 165
17/05/16 11:11:20 INFO executor.Executor: Running task 165.0 in stage 1.0 (TID 165)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 165.0 in stage 1.0 (TID 165). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 168
17/05/16 11:11:20 INFO executor.Executor: Running task 168.0 in stage 1.0 (TID 168)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 168.0 in stage 1.0 (TID 168). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 171
17/05/16 11:11:20 INFO executor.Executor: Running task 171.0 in stage 1.0 (TID 171)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 171.0 in stage 1.0 (TID 171). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 174
17/05/16 11:11:20 INFO executor.Executor: Running task 174.0 in stage 1.0 (TID 174)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 174.0 in stage 1.0 (TID 174). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 178
17/05/16 11:11:20 INFO executor.Executor: Running task 178.0 in stage 1.0 (TID 178)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 178.0 in stage 1.0 (TID 178). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 180
17/05/16 11:11:20 INFO executor.Executor: Running task 180.0 in stage 1.0 (TID 180)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 180.0 in stage 1.0 (TID 180). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 183
17/05/16 11:11:20 INFO executor.Executor: Running task 183.0 in stage 1.0 (TID 183)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 183.0 in stage 1.0 (TID 183). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 187
17/05/16 11:11:20 INFO executor.Executor: Running task 187.0 in stage 1.0 (TID 187)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 187.0 in stage 1.0 (TID 187). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 191
17/05/16 11:11:20 INFO executor.Executor: Running task 191.0 in stage 1.0 (TID 191)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 191.0 in stage 1.0 (TID 191). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 194
17/05/16 11:11:20 INFO executor.Executor: Running task 194.0 in stage 1.0 (TID 194)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 194.0 in stage 1.0 (TID 194). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 195
17/05/16 11:11:20 INFO executor.Executor: Running task 195.0 in stage 1.0 (TID 195)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 195.0 in stage 1.0 (TID 195). 3414 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 198
17/05/16 11:11:20 INFO executor.Executor: Running task 198.0 in stage 1.0 (TID 198)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 198.0 in stage 1.0 (TID 198). 3327 bytes result sent to driver
17/05/16 11:11:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 200
17/05/16 11:11:20 INFO executor.Executor: Running task 0.0 in stage 2.0 (TID 200)
17/05/16 11:11:20 INFO spark.MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
17/05/16 11:11:20 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 1
17/05/16 11:11:20 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.4 KB, free 1458.6 MB)
17/05/16 11:11:20 INFO broadcast.TorrentBroadcast: Reading broadcast variable 1 took 12 ms
17/05/16 11:11:20 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.8 KB, free 1458.5 MB)
17/05/16 11:11:20 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them
17/05/16 11:11:20 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.60.43.16:39032)
17/05/16 11:11:20 INFO spark.MapOutputTrackerWorker: Got the output locations
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
17/05/16 11:11:20 INFO client.TransportClientFactory: Successfully created connection to c411.hadoop.gda.lo/10.60.43.11:7337 after 2 ms (0 ms spent in bootstraps)
17/05/16 11:11:20 INFO client.TransportClientFactory: Successfully created connection to c402.hadoop.gda.lo/10.60.43.2:7337 after 223 ms (0 ms spent in bootstraps)
17/05/16 11:11:20 INFO client.TransportClientFactory: Successfully created connection to c412.hadoop.gda.lo/10.60.43.12:7337 after 33 ms (0 ms spent in bootstraps)
17/05/16 11:11:20 INFO storage.ShuffleBlockFetcherIterator: Started 3 remote fetches in 281 ms
17/05/16 11:11:20 INFO codegen.CodeGenerator: Code generated in 53.687676 ms
17/05/16 11:11:20 INFO executor.Executor: Finished task 0.0 in stage 2.0 (TID 200). 2809 bytes result sent to driver
17/05/16 11:12:17 INFO executor.CoarseGrainedExecutorBackend: Driver commanded a shutdown
17/05/16 11:12:17 INFO memory.MemoryStore: MemoryStore cleared
17/05/16 11:12:17 INFO storage.BlockManager: BlockManager stopped
17/05/16 11:12:17 INFO util.ShutdownHookManager: Shutdown hook called
End of LogType:stderr

LogType:stdout
Log Upload Time:Tue May 16 11:12:19 +0700 2017
LogLength:0
Log Contents:
End of LogType:stdout



Container: container_e81_1494395298335_303847_01_000008 on c427.hadoop.gda.lo_45454
=====================================================================================
LogType:directory.info
Log Upload Time:Tue May 16 11:12:18 +0700 2017
LogLength:28883
Log Contents:
ls -l:
total 112
lrwxrwxrwx 1 yarn hadoop    69 May 16 11:10 __app__.jar -> /hadoop/yarn/local/usercache/fairy/filecache/11649/stats-etlr-1.0.jar
-rw-r--r-- 1 yarn hadoop    87 May 16 11:10 container_tokens
-rwx------ 1 yarn hadoop   676 May 16 11:10 default_container_executor_session.sh
-rwx------ 1 yarn hadoop   730 May 16 11:10 default_container_executor.sh
-rwx------ 1 yarn hadoop 75554 May 16 11:10 launch_container.sh
lrwxrwxrwx 1 yarn hadoop    69 May 16 11:10 __spark_conf__ -> /hadoop/yarn/local/usercache/fairy/filecache/11650/__spark_conf__.zip
drwxr-xr-x 2 yarn hadoop 12288 May 16 11:10 __spark_libs__
drwx--x--- 2 yarn hadoop  4096 May 16 11:10 tmp
find -L . -maxdepth 5 -ls:
14287076    4 drwx--x---   4 yarn     hadoop       4096 May 16 11:10 .
14287079    4 -rw-r--r--   1 yarn     hadoop         12 May 16 11:10 ./.container_tokens.crc
11142301    4 drwx------   2 yarn     hadoop       4096 May 16 11:10 ./__spark_conf__
11142407    4 -r-x------   1 yarn     hadoop        945 May 16 11:10 ./__spark_conf__/taskcontroller.cfg
11142317    4 -r-x------   1 yarn     hadoop        884 May 16 11:10 ./__spark_conf__/ssl-client.xml
11142349    8 -r-x------   1 yarn     hadoop       4221 May 16 11:10 ./__spark_conf__/task-log4j.properties
11142388    4 -r-x------   1 yarn     hadoop       2268 May 16 11:10 ./__spark_conf__/ssl-server.xml.example
11142311    0 -r-x------   1 yarn     hadoop          0 May 16 11:10 ./__spark_conf__/yarn.exclude
11142314    4 -r-x------   1 yarn     hadoop       2250 May 16 11:10 ./__spark_conf__/yarn-env.cmd
11142375    4 -r-x------   1 yarn     hadoop        229 May 16 11:10 ./__spark_conf__/slaves
11142369    4 -r-x------   1 yarn     hadoop       1527 May 16 11:10 ./__spark_conf__/kms-env.sh
11142347    4 -r-x------   1 yarn     hadoop       2490 May 16 11:10 ./__spark_conf__/hadoop-metrics.properties
11142366    4 -r-x------   1 yarn     hadoop        690 May 16 11:10 ./__spark_conf__/mapred-env.sh
11142386    4 -r-x------   1 yarn     hadoop       1000 May 16 11:10 ./__spark_conf__/ssl-server.xml
11142323    4 -r-x------   1 yarn     hadoop       3518 May 16 11:10 ./__spark_conf__/kms-acls.xml
11142307   12 -r-x------   1 yarn     hadoop       8699 May 16 11:10 ./__spark_conf__/log4j.properties
11142341    8 -r-x------   1 yarn     hadoop       6572 May 16 11:10 ./__spark_conf__/hdfs-site.xml
11142309   20 -r-x------   1 yarn     hadoop      16530 May 16 11:10 ./__spark_conf__/yarn-site.xml
11142344    4 -r-x------   1 yarn     hadoop       1019 May 16 11:10 ./__spark_conf__/container-executor.cfg
11142304    8 -r-x------   1 yarn     hadoop       5467 May 16 11:10 ./__spark_conf__/hadoop-env.sh
11142329    4 -r-x------   1 yarn     hadoop       2358 May 16 11:10 ./__spark_conf__/topology_script.py
11142380    4 -r-x------   1 yarn     hadoop        951 May 16 11:10 ./__spark_conf__/mapred-env.cmd
11142310    4 -r-x------   1 yarn     hadoop       3979 May 16 11:10 ./__spark_conf__/hadoop-env.cmd
11142367    4 -r-x------   1 yarn     hadoop       1602 May 16 11:10 ./__spark_conf__/health_check
11142313    4 -r-x------   1 yarn     hadoop       1631 May 16 11:10 ./__spark_conf__/kms-log4j.properties
11142302    8 -r-x------   1 yarn     hadoop       6836 May 16 11:10 ./__spark_conf__/mapred-site.xml
11142378    4 -r-x------   1 yarn     hadoop        724 May 16 11:10 ./__spark_conf__/topology_mappings.data
11142392    8 -r-x------   1 yarn     hadoop       4113 May 16 11:10 ./__spark_conf__/mapred-queues.xml.template
11142327    4 -r-x------   1 yarn     hadoop        758 May 16 11:10 ./__spark_conf__/mapred-site.xml.template
11142345    4 -r-x------   1 yarn     hadoop       1020 May 16 11:10 ./__spark_conf__/commons-logging.properties
11142406    8 -r-x------   1 yarn     hadoop       5511 May 16 11:10 ./__spark_conf__/kms-site.xml
11142368    4 -r-x------   1 yarn     hadoop       2316 May 16 11:10 ./__spark_conf__/ssl-client.xml.example
11142408   36 -r-x------   1 yarn     hadoop      34061 May 16 11:10 ./__spark_conf__/__spark_conf__.properties
11142372    4 -r-x------   1 yarn     hadoop       1308 May 16 11:10 ./__spark_conf__/hadoop-policy.xml
11142308    4 -r-x------   1 yarn     hadoop       2302 May 16 11:10 ./__spark_conf__/hadoop-metrics2.properties
11142339    8 -r-x------   1 yarn     hadoop       5006 May 16 11:10 ./__spark_conf__/yarn-env.sh
11142322    8 -r-x------   1 yarn     hadoop       5404 May 16 11:10 ./__spark_conf__/capacity-scheduler.xml
11142331    4 -r-x------   1 yarn     hadoop       1335 May 16 11:10 ./__spark_conf__/configuration.xsl
11142312    8 -r-x------   1 yarn     hadoop       4197 May 16 11:10 ./__spark_conf__/core-site.xml
14287083    4 -rw-r--r--   1 yarn     hadoop         16 May 16 11:10 ./.default_container_executor_session.sh.crc
14287080   76 -rwx------   1 yarn     hadoop      75554 May 16 11:10 ./launch_container.sh
14287085    4 -rw-r--r--   1 yarn     hadoop         16 May 16 11:10 ./.default_container_executor.sh.crc
14287078    4 -rw-r--r--   1 yarn     hadoop         87 May 16 11:10 ./container_tokens
11142202 1152 -r-x------   1 yarn     hadoop    1178454 May 16 11:10 ./__app__.jar
14287086   12 drwxr-xr-x   2 yarn     hadoop      12288 May 16 11:10 ./__spark_libs__
11928380 2044 -rwxr-xr-x   1 yarn     hadoop    2090370 Mar 27 13:37 ./__spark_libs__/spark-streaming_2.11-2.0.1.jar
11928483   92 -rwxr-xr-x   1 yarn     hadoop      93407 Mar 27 13:37 ./__spark_libs__/pyrolite-4.9.jar
11928357   96 -rwxr-xr-x   1 yarn     hadoop      95806 Mar 27 13:37 ./__spark_libs__/javax.servlet-api-3.1.0.jar
11929220  736 -rwxr-xr-x   1 yarn     hadoop     753012 Mar 27 13:37 ./__spark_libs__/hadoop-mapreduce-client-common-2.7.1.jar
11928757 1204 -rwxr-xr-x   1 yarn     hadoop    1229125 Mar 27 13:37 ./__spark_libs__/xercesImpl-2.9.1.jar
11928327 3440 -rwxr-xr-x   1 yarn     hadoop    3519262 Mar 27 13:37 ./__spark_libs__/calcite-core-1.2.0-incubating.jar
11928719  356 -rwxr-xr-x   1 yarn     hadoop     363908 Mar 27 13:37 ./__spark_libs__/hadoop-yarn-server-common-2.7.1.jar
11929137 3352 -rwxr-xr-x   1 yarn     hadoop    3431544 Mar 27 13:37 ./__spark_libs__/hadoop-common-2.7.1.jar
11928457  176 -rwxr-xr-x   1 yarn     hadoop     178947 Mar 27 13:37 ./__spark_libs__/hk2-api-2.4.0-b34.jar
11929124   44 -rwxr-xr-x   1 yarn     hadoop      45015 Mar 27 13:37 ./__spark_libs__/hadoop-mapreduce-client-shuffle-2.7.1.jar
11928626   88 -rwxr-xr-x   1 yarn     hadoop      86811 Mar 27 13:37 ./__spark_libs__/spire-macros_2.11-0.7.4.jar
11928732  232 -rwxr-xr-x   1 yarn     hadoop     236880 Mar 27 13:37 ./__spark_libs__/lz4-1.3.0.jar
11929169  388 -rwxr-xr-x   1 yarn     hadoop     395195 Mar 27 13:37 ./__spark_libs__/javolution-5.5.1.jar
11929158   68 -rwxr-xr-x   1 yarn     hadoop      69409 Mar 27 13:37 ./__spark_libs__/activation-1.1.1.jar
11928524   52 -rwxr-xr-x   1 yarn     hadoop      50619 Mar 27 13:37 ./__spark_libs__/chill-java-0.8.0.jar
11928698  480 -rwxr-xr-x   1 yarn     hadoop     489884 Mar 27 13:37 ./__spark_libs__/log4j-1.2.17.jar
11928361    8 -rwxr-xr-x   1 yarn     hadoop       5950 Mar 27 13:37 ./__spark_libs__/javax.inject-2.4.0-b34.jar
11928831  320 -rwxr-xr-x   1 yarn     hadoop     326724 Mar 27 13:37 ./__spark_libs__/httpcore-4.4.4.jar
11928851   24 -rwxr-xr-x   1 yarn     hadoop      20852 Mar 27 13:37 ./__spark_libs__/metrics-graphite-3.1.2.jar
11928434  188 -rwxr-xr-x   1 yarn     hadoop     190432 Mar 27 13:37 ./__spark_libs__/gson-2.2.4.jar
11928498  656 -rwxr-xr-x   1 yarn     hadoop     669589 Mar 27 13:37 ./__spark_libs__/spark-yarn_2.11-2.0.1.jar
11928377 8068 -rwxr-xr-x   1 yarn     hadoop    8260573 Mar 27 13:37 ./__spark_libs__/hadoop-hdfs-2.7.1.jar
11928572  192 -rwxr-xr-x   1 yarn     hadoop     192993 Mar 27 13:37 ./__spark_libs__/avro-ipc-1.7.7.jar
11929211 2008 -rwxr-xr-x   1 yarn     hadoop    2054931 Mar 27 13:37 ./__spark_libs__/netty-all-4.0.29.Final.jar
11928683   24 -rwxr-xr-x   1 yarn     hadoop      23346 Mar 27 13:37 ./__spark_libs__/stax-api-1.0-2.jar
11928813   20 -rwxr-xr-x   1 yarn     hadoop      18482 Mar 27 13:37 ./__spark_libs__/eigenbase-properties-1.1.5.jar
11928722  576 -rwxr-xr-x   1 yarn     hadoop     588337 Mar 27 13:37 ./__spark_libs__/commons-collections-3.2.2.jar
11928477  600 -rwxr-xr-x   1 yarn     hadoop     613299 Mar 27 13:37 ./__spark_libs__/janino-2.7.8.jar
11928460 1024 -rwxr-xr-x   1 yarn     hadoop    1045744 Mar 27 13:37 ./__spark_libs__/leveldbjni-all-1.8.jar
11928441  932 -rwxr-xr-x   1 yarn     hadoop     951701 Mar 27 13:37 ./__spark_libs__/jersey-server-2.22.2.jar
11928622 2780 -rwxr-xr-x   1 yarn     hadoop    2842667 Mar 27 13:37 ./__spark_libs__/bcprov-jdk15on-1.51.jar
11928768  136 -rwxr-xr-x   1 yarn     hadoop     138464 Mar 27 13:37 ./__spark_libs__/hive-beeline-1.2.1.spark2.jar
11928464  228 -rwxr-xr-x   1 yarn     hadoop     232248 Mar 27 13:37 ./__spark_libs__/jackson-core-asl-1.9.13.jar
11928480  136 -rwxr-xr-x   1 yarn     hadoop     135552 Mar 27 13:37 ./__spark_libs__/breeze-macros_2.11-0.11.2.jar
11928691  116 -rwxr-xr-x   1 yarn     hadoop     114913 Mar 27 13:37 ./__spark_libs__/py4j-0.10.3.jar
11928342   40 -rwxr-xr-x   1 yarn     hadoop      40817 Mar 27 13:37 ./__spark_libs__/hive-cli-1.2.1.spark2.jar
11928330   64 -rwxr-xr-x   1 yarn     hadoop      65012 Mar 27 13:37 ./__spark_libs__/guice-servlet-3.0.jar
11927553  164 -rwxr-xr-x   1 yarn     hadoop     164422 Mar 27 13:37 ./__spark_libs__/core-1.1.2.jar
11929150 2140 -rwxr-xr-x   1 yarn     hadoop    2189117 Mar 27 13:37 ./__spark_libs__/guava-14.0.1.jar
11929206 7108 -rwxr-xr-x   1 yarn     hadoop    7276083 Mar 27 13:37 ./__spark_libs__/spire_2.11-0.7.4.jar
11928679  952 -rwxr-xr-x   1 yarn     hadoop     971310 Mar 27 13:37 ./__spark_libs__/jersey-guava-2.22.2.jar
11928349    4 -rwxr-xr-x   1 yarn     hadoop       2545 Mar 27 13:37 ./__spark_libs__/hadoop-client-2.7.1.jar
11928443  284 -rwxr-xr-x   1 yarn     hadoop     290506 Mar 27 13:37 ./__spark_libs__/univocity-parsers-2.1.1.jar
11928402  576 -rwxr-xr-x   1 yarn     hadoop     589462 Mar 27 13:37 ./__spark_libs__/json4s-core_2.11-3.2.11.jar
11929143 1168 -rwxr-xr-x   1 yarn     hadoop    1194003 Mar 27 13:37 ./__spark_libs__/arpack_combined_all-0.1.jar
11928631 1204 -rwxr-xr-x   1 yarn     hadoop    1230201 Mar 27 13:37 ./__spark_libs__/netty-3.8.0.Final.jar
11928382  280 -rwxr-xr-x   1 yarn     hadoop     284220 Mar 27 13:37 ./__spark_libs__/commons-lang-2.6.jar
11928674   20 -rwxr-xr-x   1 yarn     hadoop      16560 Mar 27 13:37 ./__spark_libs__/api-asn1-api-1.0.0-M20.jar
11928350   48 -rwxr-xr-x   1 yarn     hadoop      45944 Mar 27 13:37 ./__spark_libs__/json-20090211.jar
11928788   36 -rwxr-xr-x   1 yarn     hadoop      33015 Mar 27 13:37 ./__spark_libs__/jsr305-1.3.9.jar
11928635   68 -rwxr-xr-x   1 yarn     hadoop      68866 Mar 27 13:37 ./__spark_libs__/curator-client-2.6.0.jar
11928620   20 -rwxr-xr-x   1 yarn     hadoop      17385 Mar 27 13:37 ./__spark_libs__/hadoop-annotations-2.7.1.jar
11928715   24 -rwxr-xr-x   1 yarn     hadoop      21575 Mar 27 13:37 ./__spark_libs__/parquet-common-1.7.0.jar
11928314  180 -rwxr-xr-x   1 yarn     hadoop     180736 Mar 27 13:37 ./__spark_libs__/avro-mapred-1.7.7-hadoop2.jar
11928654 5380 -rwxr-xr-x   1 yarn     hadoop    5505200 Mar 27 13:37 ./__spark_libs__/hive-metastore-1.2.1.spark2.jar
11928420 2732 -rwxr-xr-x   1 yarn     hadoop    2796935 Mar 27 13:37 ./__spark_libs__/parquet-hadoop-bundle-1.6.0.jar
11928365   96 -rwxr-xr-x   1 yarn     hadoop      96221 Mar 27 13:37 ./__spark_libs__/commons-pool-1.5.4.jar
11928501  764 -rwxr-xr-x   1 yarn     hadoop     780664 Mar 27 13:37 ./__spark_libs__/jackson-mapper-asl-1.9.13.jar
11928766   80 -rwxr-xr-x   1 yarn     hadoop      79912 Mar 27 13:37 ./__spark_libs__/api-util-1.0.0-M20.jar
11928320  776 -rwxr-xr-x   1 yarn     hadoop     792964 Mar 27 13:37 ./__spark_libs__/zookeeper-3.4.6.jar
11928396  640 -rwxr-xr-x   1 yarn     hadoop     654216 Mar 27 13:37 ./__spark_libs__/pmml-model-1.2.15.jar
11928605  524 -rwxr-xr-x   1 yarn     hadoop     533455 Mar 27 13:37 ./__spark_libs__/protobuf-java-2.5.0.jar
11929121  668 -rwxr-xr-x   1 yarn     hadoop     680106 Mar 27 13:37 ./__spark_libs__/spark-graphx_2.11-2.0.1.jar
11928339   56 -rwxr-xr-x   1 yarn     hadoop      55511 Mar 27 13:37 ./__spark_libs__/spark-network-shuffle_2.11-2.0.1.jar
11929172  200 -rwxr-xr-x   1 yarn     hadoop     201928 Mar 27 13:37 ./__spark_libs__/RoaringBitmap-0.5.11.jar
11928655  100 -rwxr-xr-x   1 yarn     hadoop     100636 Mar 27 13:37 ./__spark_libs__/jsp-api-2.1.jar
11929173  468 -rwxr-xr-x   1 yarn     hadoop     477970 Mar 27 13:37 ./__spark_libs__/lift-json_2.11-2.6.3.jar
11929123   28 -rwxr-xr-x   1 yarn     hadoop      27084 Mar 27 13:37 ./__spark_libs__/jackson-xc-1.9.13.jar
11929154 1144 -rwxr-xr-x   1 yarn     hadoop    1171380 Mar 27 13:37 ./__spark_libs__/jackson-databind-2.6.5.jar
11928522    4 -rwxr-xr-x   1 yarn     hadoop       2497 Mar 27 13:37 ./__spark_libs__/javax.inject-1.jar
11928685  116 -rwxr-xr-x   1 yarn     hadoop     115534 Mar 27 13:37 ./__spark_libs__/javax.ws.rs-api-2.0.1.jar
11928535  208 -rwxr-xr-x   1 yarn     hadoop     209622 Mar 27 13:37 ./__spark_libs__/parquet-hadoop-1.7.0.jar
11928471   20 -rwxr-xr-x   1 yarn     hadoop      19827 Mar 27 13:37 ./__spark_libs__/opencsv-2.3.jar
11929181 1024 -rwxr-xr-x   1 yarn     hadoop    1044511 Mar 27 13:37 ./__spark_libs__/spark-hive_2.11-2.0.1.jar
11928650  148 -rwxr-xr-x   1 yarn     hadoop     148627 Mar 27 13:37 ./__spark_libs__/stringtemplate-3.2.1.jar
11928318  176 -rwxr-xr-x   1 yarn     hadoop     177131 Mar 27 13:37 ./__spark_libs__/jetty-util-6.1.26.jar
11929223    8 -rwxr-xr-x   1 yarn     hadoop       5310 Mar 27 13:37 ./__spark_libs__/pmml-schema-1.2.15.jar
11929159  420 -rwxr-xr-x   1 yarn     hadoop     427780 Mar 27 13:37 ./__spark_libs__/jodd-core-3.5.2.jar
11928399  188 -rwxr-xr-x   1 yarn     hadoop     188671 Mar 27 13:37 ./__spark_libs__/commons-beanutils-1.7.0.jar
11928566  528 -rwxr-xr-x   1 yarn     hadoop     539912 Mar 27 13:37 ./__spark_libs__/jetty-6.1.26.jar
11928546 11232 -rwxr-xr-x   1 yarn     hadoop   11498852 Mar 27 13:37 ./__spark_libs__/hive-exec-1.2.1.spark2.jar
11929217  508 -rwxr-xr-x   1 yarn     hadoop     516127 Mar 27 13:37 ./__spark_libs__/hadoop-mapreduce-client-app-2.7.1.jar
11928821  700 -rwxr-xr-x   1 yarn     hadoop     714194 Mar 27 13:37 ./__spark_libs__/javassist-3.18.1-GA.jar
11928492  280 -rwxr-xr-x   1 yarn     hadoop     284184 Mar 27 13:37 ./__spark_libs__/commons-codec-1.10.jar
11928408   68 -rwxr-xr-x   1 yarn     hadoop      66270 Mar 27 13:37 ./__spark_libs__/jersey-container-servlet-core-2.22.2.jar
11928701  380 -rwxr-xr-x   1 yarn     hadoop     387188 Mar 27 13:37 ./__spark_libs__/parquet-format-2.3.0-incubating.jar
11928593 4468 -rwxr-xr-x   1 yarn     hadoop    4573750 Mar 27 13:37 ./__spark_libs__/scala-reflect-2.11.8.jar
11928423   32 -rwxr-xr-x   1 yarn     hadoop      29555 Mar 27 13:37 ./__spark_libs__/paranamer-2.3.jar
11928445  244 -rwxr-xr-x   1 yarn     hadoop     248171 Mar 27 13:37 ./__spark_libs__/curator-recipes-2.6.0.jar
11929127   84 -rwxr-xr-x   1 yarn     hadoop      82421 Mar 27 13:37 ./__spark_libs__/json4s-ast_2.11-3.2.11.jar
11928468  144 -rwxr-xr-x   1 yarn     hadoop     143602 Mar 27 13:37 ./__spark_libs__/commons-digester-1.8.jar
11928487 1848 -rwxr-xr-x   1 yarn     hadoop    1890075 Mar 27 13:37 ./__spark_libs__/datanucleus-core-3.2.10.jar
11929178  120 -rwxr-xr-x   1 yarn     hadoop     118973 Mar 27 13:37 ./__spark_libs__/hk2-utils-2.4.0-b34.jar
11928730 2304 -rwxr-xr-x   1 yarn     hadoop    2355465 Mar 27 13:37 ./__spark_libs__/spark-network-common_2.11-2.0.1.jar
11928417   40 -rwxr-xr-x   1 yarn     hadoop      39280 Mar 27 13:37 ./__spark_libs__/metrics-jvm-3.1.2.jar
11929193   44 -rwxr-xr-x   1 yarn     hadoop      41123 Mar 27 13:37 ./__spark_libs__/commons-cli-1.2.jar
11928662  416 -rwxr-xr-x   1 yarn     hadoop     423753 Mar 27 13:37 ./__spark_libs__/scala-parser-combinators_2.11-1.0.4.jar
11928311  204 -rwxr-xr-x   1 yarn     hadoop     206035 Mar 27 13:37 ./__spark_libs__/commons-beanutils-core-1.8.0.jar
11928474   20 -rwxr-xr-x   1 yarn     hadoop      18098 Mar 27 13:37 ./__spark_libs__/jersey-container-servlet-2.22.2.jar
11928712  404 -rwxr-xr-x   1 yarn     hadoop     412739 Mar 27 13:37 ./__spark_libs__/commons-lang3-3.3.2.jar
11928369   16 -rwxr-xr-x   1 yarn     hadoop      15305 Mar 27 13:37 ./__spark_libs__/spark-tags_2.11-2.0.1.jar
11929196   24 -rwxr-xr-x   1 yarn     hadoop      21243 Mar 27 13:37 ./__spark_libs__/parquet-generator-1.7.0.jar
11929167   16 -rwxr-xr-x   1 yarn     hadoop      15071 Mar 27 13:37 ./__spark_libs__/jta-1.1.jar
11929198  172 -rwxr-xr-x   1 yarn     hadoop     174351 Mar 27 13:37 ./__spark_libs__/stream-2.7.0.jar
11928433  256 -rwxr-xr-x   1 yarn     hadoop     258876 Mar 27 13:37 ./__spark_libs__/jackson-core-2.6.5.jar
11928786  616 -rwxr-xr-x   1 yarn     hadoop     627814 Mar 27 13:37 ./__spark_libs__/joda-time-2.9.3.jar
11928618  104 -rwxr-xr-x   1 yarn     hadoop     105134 Mar 27 13:37 ./__spark_libs__/jaxb-api-2.2.2.jar
11928372   44 -rwxr-xr-x   1 yarn     hadoop      41755 Mar 27 13:37 ./__spark_libs__/objenesis-2.1.jar
11928447   44 -rwxr-xr-x   1 yarn     hadoop      41263 Mar 27 13:37 ./__spark_libs__/jackson-module-paranamer-2.6.5.jar
11928415 1988 -rwxr-xr-x   1 yarn     hadoop    2035066 Mar 27 13:37 ./__spark_libs__/commons-math3-3.4.1.jar
11928791   16 -rwxr-xr-x   1 yarn     hadoop      15010 Mar 27 13:37 ./__spark_libs__/xmlenc-0.52.jar
11928801   32 -rwxr-xr-x   1 yarn     hadoop      30595 Mar 27 13:37 ./__spark_libs__/commons-compiler-2.7.6.jar
11929163  140 -rwxr-xr-x   1 yarn     hadoop     142631 Mar 27 13:37 ./__spark_libs__/hadoop-yarn-client-2.7.1.jar
11928599   72 -rwxr-xr-x   1 yarn     hadoop      72733 Mar 27 13:37 ./__spark_libs__/jersey-media-jaxb-2.22.2.jar
11928776   64 -rwxr-xr-x   1 yarn     hadoop      65261 Mar 27 13:37 ./__spark_libs__/oro-2.0.8.jar
11928637   20 -rwxr-xr-x   1 yarn     hadoop      20235 Mar 27 13:37 ./__spark_libs__/osgi-resource-locator-1.0.1.jar
11929214  164 -rwxr-xr-x   1 yarn     hadoop     167421 Mar 27 13:37 ./__spark_libs__/jersey-client-2.22.2.jar
11928453  160 -rwxr-xr-x   1 yarn     hadoop     160519 Mar 27 13:37 ./__spark_libs__/commons-dbcp-1.4.jar
11928578  184 -rwxr-xr-x   1 yarn     hadoop     185140 Mar 27 13:37 ./__spark_libs__/commons-io-2.4.jar
11928670  684 -rwxr-xr-x   1 yarn     hadoop     698375 Mar 27 13:37 ./__spark_libs__/jersey-common-2.22.2.jar
11928539  232 -rwxr-xr-x   1 yarn     hadoop     236660 Mar 27 13:37 ./__spark_libs__/ST4-4.0.4.jar
11928866  184 -rwxr-xr-x   1 yarn     hadoop     185245 Mar 27 13:37 ./__spark_libs__/curator-framework-2.6.0.jar
11929113   96 -rwxr-xr-x   1 yarn     hadoop      94672 Mar 27 13:37 ./__spark_libs__/xz-1.0.jar
11929185  400 -rwxr-xr-x   1 yarn     hadoop     409467 Mar 27 13:37 ./__spark_libs__/mx4j-3.0.2.jar
11929082 1024 -rwxr-xr-x   1 yarn     hadoop    1048110 Mar 27 13:37 ./__spark_libs__/parquet-jackson-1.7.0.jar
11928336   16 -rwxr-xr-x   1 yarn     hadoop      15827 Mar 27 13:37 ./__spark_libs__/metrics-json-3.1.2.jar
11928745    8 -rwxr-xr-x   1 yarn     hadoop       4596 Mar 27 13:37 ./__spark_libs__/jul-to-slf4j-1.7.16.jar
11928485  112 -rwxr-xr-x   1 yarn     hadoop     110600 Mar 27 13:37 ./__spark_libs__/bonecp-0.8.0.RELEASE.jar
11928569  180 -rwxr-xr-x   1 yarn     hadoop     181271 Mar 27 13:37 ./__spark_libs__/hk2-locator-2.4.0-b34.jar
11928740  200 -rwxr-xr-x   1 yarn     hadoop     201124 Mar 27 13:37 ./__spark_libs__/jdo-api-3.0.1.jar
11928516  436 -rwxr-xr-x   1 yarn     hadoop     442406 Mar 27 13:37 ./__spark_libs__/calcite-linq4j-1.2.0-incubating.jar
11928307  224 -rwxr-xr-x   1 yarn     hadoop     227712 Mar 27 13:37 ./__spark_libs__/libthrift-0.9.2.jar
11928724  220 -rwxr-xr-x   1 yarn     hadoop     223573 Mar 27 13:37 ./__spark_libs__/chill_2.11-0.8.0.jar
11928643   16 -rwxr-xr-x   1 yarn     hadoop      14766 Mar 27 13:37 ./__spark_libs__/aopalliance-repackaged-2.4.0-b34.jar
11928667  280 -rwxr-xr-x   1 yarn     hadoop     285447 Mar 27 13:37 ./__spark_libs__/parquet-encoding-1.7.0.jar
11928518 1256 -rwxr-xr-x   1 yarn     hadoop    1282424 Mar 27 13:37 ./__spark_libs__/ivy-2.4.0.jar
11928584  236 -rwxr-xr-x   1 yarn     hadoop     241367 Mar 27 13:37 ./__spark_libs__/commons-compress-1.4.1.jar
11929075  512 -rwxr-xr-x   1 yarn     hadoop     521157 Mar 27 13:37 ./__spark_libs__/mail-1.4.7.jar
11928737  300 -rwxr-xr-x   1 yarn     hadoop     305001 Mar 27 13:37 ./__spark_libs__/commons-httpclient-3.1.jar
11928695  292 -rwxr-xr-x   1 yarn     hadoop     298829 Mar 27 13:37 ./__spark_libs__/commons-configuration-1.6.jar
11928753 11452 -rwxr-xr-x   1 yarn     hadoop   11723537 Mar 27 13:37 ./__spark_libs__/spark-core_2.11-2.0.1.jar
11928411  144 -rwxr-xr-x   1 yarn     hadoop     144660 Mar 27 13:37 ./__spark_libs__/xbean-asm5-shaded-4.4.jar
11929130 1972 -rwxr-xr-x   1 yarn     hadoop    2015514 Mar 27 13:37 ./__spark_libs__/hadoop-yarn-api-2.7.1.jar
11928705  100 -rwxr-xr-x   1 yarn     hadoop     100680 Mar 27 13:37 ./__spark_libs__/hive-jdbc-1.2.1.spark2.jar
11928614  164 -rwxr-xr-x   1 yarn     hadoop     164368 Mar 27 13:37 ./__spark_libs__/antlr-runtime-3.4.jar
11928450 1996 -rwxr-xr-x   1 yarn     hadoop    2041628 Mar 27 13:37 ./__spark_libs__/jets3t-0.9.3.jar
11928324   92 -rwxr-xr-x   1 yarn     hadoop      93210 Mar 27 13:37 ./__spark_libs__/super-csv-2.2.0.jar
11929186 1772 -rwxr-xr-x   1 yarn     hadoop    1814309 Mar 27 13:37 ./__spark_libs__/spark-hive-thriftserver_2.11-2.0.1.jar
11928367   64 -rwxr-xr-x   1 yarn     hadoop      63777 Mar 27 13:37 ./__spark_libs__/validation-api-1.1.0.Final.jar
11928550   20 -rwxr-xr-x   1 yarn     hadoop      17008 Mar 27 13:37 ./__spark_libs__/base64-2.3.8.jar
11928629 1248 -rwxr-xr-x   1 yarn     hadoop    1277883 Mar 27 13:37 ./__spark_libs__/mesos-0.21.1-shaded-protobuf.jar
11928392   40 -rwxr-xr-x   1 yarn     hadoop      40341 Mar 27 13:37 ./__spark_libs__/json4s-jackson_2.11-3.2.11.jar
11928587  256 -rwxr-xr-x   1 yarn     hadoop     258370 Mar 27 13:37 ./__spark_libs__/calcite-avatica-1.2.0-incubating.jar
11928603   32 -rwxr-xr-x   1 yarn     hadoop      29540 Mar 27 13:37 ./__spark_libs__/spark-sketch_2.11-2.0.1.jar
11928527 1592 -rwxr-xr-x   1 yarn     hadoop    1627065 Mar 27 13:37 ./__spark_libs__/jersey-bundle-1.19.1.jar
11928405  176 -rwxr-xr-x   1 yarn     hadoop     177832 Mar 27 13:37 ./__spark_libs__/spark-mllib-local_2.11-2.0.1.jar
11928763 6144 -rwxr-xr-x   1 yarn     hadoop    6290315 Mar 27 13:37 ./__spark_libs__/spark-sql_2.11-2.0.1.jar
11928438   64 -rwxr-xr-x   1 yarn     hadoop      63316 Mar 27 13:37 ./__spark_libs__/spark-repl_2.11-2.0.1.jar
11928582 5612 -rwxr-xr-x   1 yarn     hadoop    5744974 Mar 27 13:37 ./__spark_libs__/scala-library-2.11.8.jar
11928808  696 -rwxr-xr-x   1 yarn     hadoop     710492 Mar 27 13:37 ./__spark_libs__/guice-3.0.jar
11929189   28 -rwxr-xr-x   1 yarn     hadoop      26514 Mar 27 13:37 ./__spark_libs__/stax-api-1.0.1.jar
11928551    8 -rwxr-xr-x   1 yarn     hadoop       4467 Mar 27 13:37 ./__spark_libs__/aopalliance-1.0.jar
11928591 6716 -rwxr-xr-x   1 yarn     hadoop    6873892 Mar 27 13:37 ./__spark_libs__/spark-catalyst_2.11-2.0.1.jar
11929024  896 -rwxr-xr-x   1 yarn     hadoop     917052 Mar 27 13:37 ./__spark_libs__/parquet-column-1.7.0.jar
11928390   20 -rwxr-xr-x   1 yarn     hadoop      16430 Mar 27 13:37 ./__spark_libs__/jcl-over-slf4j-1.7.16.jar
11928534   80 -rwxr-xr-x   1 yarn     hadoop      79845 Mar 27 13:37 ./__spark_libs__/compress-lzf-1.0.3.jar
11928608  436 -rwxr-xr-x   1 yarn     hadoop     445288 Mar 27 13:37 ./__spark_libs__/antlr-2.7.7.jar
11928751   28 -rwxr-xr-x   1 yarn     hadoop      26366 Mar 27 13:37 ./__spark_libs__/javax.annotation-api-1.2.jar
11928726  676 -rwxr-xr-x   1 yarn     hadoop     691479 Mar 27 13:37 ./__spark_libs__/apacheds-kerberos-codec-2.0.0-M15.jar
11928689 1616 -rwxr-xr-x   1 yarn     hadoop    1654097 Mar 27 13:37 ./__spark_libs__/hadoop-yarn-common-2.7.1.jar
11928666  748 -rwxr-xr-x   1 yarn     hadoop     764569 Mar 27 13:37 ./__spark_libs__/jtransforms-2.4.0.jar
11928490   12 -rwxr-xr-x   1 yarn     hadoop       9939 Mar 27 13:37 ./__spark_libs__/slf4j-log4j12-1.7.16.jar
11928747    8 -rwxr-xr-x   1 yarn     hadoop       5711 Mar 27 13:37 ./__spark_libs__/minlog-1.3.0.jar
11928677   48 -rwxr-xr-x   1 yarn     hadoop      48720 Mar 27 13:37 ./__spark_libs__/snappy-0.2.jar
11928353  636 -rwxr-xr-x   1 yarn     hadoop     648678 Mar 27 13:37 ./__spark_libs__/scala-xml_2.11-1.0.2.jar
11928781 1768 -rwxr-xr-x   1 yarn     hadoop    1809447 Mar 27 13:37 ./__spark_libs__/datanucleus-rdbms-3.2.9.jar
11928733 5712 -rwxr-xr-x   1 yarn     hadoop    5847591 Mar 27 13:37 ./__spark_libs__/spark-mllib_2.11-2.0.1.jar
11928426  532 -rwxr-xr-x   1 yarn     hadoop     540852 Mar 27 13:37 ./__spark_libs__/mysql-connector-java-5.0.8-bin.jar
11928495   40 -rwxr-xr-x   1 yarn     hadoop      40509 Mar 27 13:37 ./__spark_libs__/slf4j-api-1.7.16.jar
11929076   64 -rwxr-xr-x   1 yarn     hadoop      62050 Mar 27 13:37 ./__spark_libs__/commons-logging-1.1.3.jar
11929135  212 -rwxr-xr-x   1 yarn     hadoop     213911 Mar 27 13:37 ./__spark_libs__/jline-2.12.1.jar
11928462  788 -rwxr-xr-x   1 yarn     hadoop     802818 Mar 27 13:37 ./__spark_libs__/scalap-2.11.8.jar
11928504 13136 -rwxr-xr-x   1 yarn     hadoop   13448966 Mar 27 13:37 ./__spark_libs__/breeze_2.11-0.11.2.jar
11929139  504 -rwxr-xr-x   1 yarn     hadoop     515604 Mar 27 13:37 ./__spark_libs__/jackson-module-scala_2.11-2.6.5.jar
11928803   12 -rwxr-xr-x   1 yarn     hadoop      10023 Mar 27 13:37 ./__spark_libs__/java-xmlbuilder-1.0.jar
11929133  440 -rwxr-xr-x   1 yarn     hadoop     448794 Mar 27 13:37 ./__spark_libs__/apache-log4j-extras-1.2.17.jar
11928341   48 -rwxr-xr-x   1 yarn     hadoop      46983 Mar 27 13:37 ./__spark_libs__/jackson-annotations-2.6.5.jar
11928333   44 -rwxr-xr-x   1 yarn     hadoop      44925 Mar 27 13:37 ./__spark_libs__/apacheds-i18n-2.0.0-M15.jar
11928300  112 -rwxr-xr-x   1 yarn     hadoop     112558 Mar 27 13:37 ./__spark_libs__/metrics-core-3.1.2.jar
11928544  720 -rwxr-xr-x   1 yarn     hadoop     736658 Mar 27 13:37 ./__spark_libs__/httpclient-4.5.2.jar
11929114   20 -rwxr-xr-x   1 yarn     hadoop      18336 Mar 27 13:37 ./__spark_libs__/jackson-jaxrs-1.9.13.jar
11928770  352 -rwxr-xr-x   1 yarn     hadoop     358390 Mar 27 13:37 ./__spark_libs__/kryo-shaded-3.0.3.jar
11928387  428 -rwxr-xr-x   1 yarn     hadoop     436303 Mar 27 13:37 ./__spark_libs__/avro-1.7.7.jar
11928507   68 -rwxr-xr-x   1 yarn     hadoop      65653 Mar 27 13:37 ./__spark_libs__/spark-launcher_2.11-2.0.1.jar
11928706   12 -rwxr-xr-x   1 yarn     hadoop      12131 Mar 27 13:37 ./__spark_libs__/jpam-1.1.jar
11929207  308 -rwxr-xr-x   1 yarn     hadoop     313686 Mar 27 13:37 ./__spark_libs__/libfb303-0.9.2.jar
11928384 15128 -rwxr-xr-x   1 yarn     hadoop   15487351 Mar 27 13:37 ./__spark_libs__/scala-compiler-2.11.8.jar
11928819 1032 -rwxr-xr-x   1 yarn     hadoop    1056168 Mar 27 13:37 ./__spark_libs__/snappy-java-1.1.2.6.jar
11928660   44 -rwxr-xr-x   1 yarn     hadoop      41070 Mar 27 13:37 ./__spark_libs__/spark-unsafe_2.11-2.0.1.jar
11928710   72 -rwxr-xr-x   1 yarn     hadoop      70688 Mar 27 13:37 ./__spark_libs__/hadoop-auth-2.7.1.jar
11928778 3152 -rwxr-xr-x   1 yarn     hadoop    3224708 Mar 27 13:37 ./__spark_libs__/derby-10.12.1.1.jar
11928510   32 -rwxr-xr-x   1 yarn     hadoop      32145 Mar 27 13:37 ./__spark_libs__/hadoop-yarn-server-web-proxy-2.7.1.jar
11928641  332 -rwxr-xr-x   1 yarn     hadoop     339666 Mar 27 13:37 ./__spark_libs__/datanucleus-api-jdo-3.2.6.jar
11928362 1444 -rwxr-xr-x   1 yarn     hadoop    1475955 Mar 27 13:37 ./__spark_libs__/htrace-core-3.1.0-incubating.jar
11929145   40 -rwxr-xr-x   1 yarn     hadoop      38134 Mar 27 13:37 ./__spark_libs__/hadoop-mapreduce-client-jobclient-2.7.1.jar
11928304 1512 -rwxr-xr-x   1 yarn     hadoop    1544875 Mar 27 13:37 ./__spark_libs__/hadoop-mapreduce-client-core-2.7.1.jar
11929199  296 -rwxr-xr-x   1 yarn     hadoop     302248 Mar 27 13:37 ./__spark_libs__/antlr4-runtime-4.5.3.jar
11928429   20 -rwxr-xr-x   1 yarn     hadoop      16993 Mar 27 13:37 ./__spark_libs__/JavaEWAH-0.3.2.jar
11928760  208 -rwxr-xr-x   1 yarn     hadoop     212453 Mar 27 13:37 ./__spark_libs__/commons-net-2.2.jar
14287081    4 -rw-r--r--   1 yarn     hadoop        600 May 16 11:10 ./.launch_container.sh.crc
14287082    4 -rwx------   1 yarn     hadoop        676 May 16 11:10 ./default_container_executor_session.sh
14287084    4 -rwx------   1 yarn     hadoop        730 May 16 11:10 ./default_container_executor.sh
14287077    4 drwx--x---   2 yarn     hadoop       4096 May 16 11:10 ./tmp
broken symlinks(find -L . -maxdepth 5 -type l -ls):
End of LogType:directory.info

LogType:launch_container.sh
Log Upload Time:Tue May 16 11:12:18 +0700 2017
LogLength:75554
Log Contents:
#!/bin/bash

export SPARK_YARN_STAGING_DIR="hdfs://c408.hadoop.gda.lo:8020/user/fairy/.sparkStaging/application_1494395298335_303847"
export LOCAL_DIRS="/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847"
export HADOOP_CONF_DIR="/usr/hdp/current/hadoop-client/conf"
export NM_HTTP_PORT="8042"
export JAVA_HOME="/usr/jdk64/jdk1.8.0_92/"
export LOG_DIRS="/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000008"
export NM_AUX_SERVICE_mapreduce_shuffle="AAA0+gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=
"
export NM_PORT="45454"
export USER="fairy"
export HADOOP_YARN_HOME="/usr/hdp/current/hadoop-yarn-nodemanager"
export CLASSPATH="$PWD:$PWD/__spark_conf__:$PWD/__spark_libs__/*:$HADOOP_CONF_DIR:/usr/hdp/current/hadoop-client/*:/usr/hdp/current/hadoop-client/lib/*:/usr/hdp/current/hadoop-hdfs-client/*:/usr/hdp/current/hadoop-hdfs-client/lib/*:/usr/hdp/current/hadoop-yarn-client/*:/usr/hdp/current/hadoop-yarn-client/lib/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.4.2.0-258/hadoop/lib/hadoop-lzo-0.6.0.2.4.2.0-258.jar:/etc/hadoop/conf/secure"
export SPARK_LOG_URL_STDOUT="http://c427.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_01_000008/fairy/stdout?start=-4096"
export NM_HOST="c427.hadoop.gda.lo"
export SPARK_YARN_MODE="true"
export HADOOP_TOKEN_FILE_LOCATION="/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_01_000008/container_tokens"
export NM_AUX_SERVICE_spark_shuffle=""
export SPARK_USER="fairy"
export SPARK_LOG_URL_STDERR="http://c427.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_01_000008/fairy/stderr?start=-4096"
export LOGNAME="fairy"
export JVM_PID="$$"
export PWD="/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_01_000008"
export HOME="/home/"
export CONTAINER_ID="container_e81_1494395298335_303847_01_000008"
export MALLOC_ARENA_MAX="4"
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3310/paranamer-2.3.jar" "__spark_libs__/paranamer-2.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3355/avro-ipc-1.7.7.jar" "__spark_libs__/avro-ipc-1.7.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3296/spark-streaming_2.11-2.0.1.jar" "__spark_libs__/spark-streaming_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3445/spark-graphx_2.11-2.0.1.jar" "__spark_libs__/spark-graphx_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3449/hadoop-common-2.7.1.jar" "__spark_libs__/hadoop-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3284/hive-cli-1.2.1.spark2.jar" "__spark_libs__/hive-cli-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3299/jcl-over-slf4j-1.7.16.jar" "__spark_libs__/jcl-over-slf4j-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3384/parquet-encoding-1.7.0.jar" "__spark_libs__/parquet-encoding-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3306/xbean-asm5-shaded-4.4.jar" "__spark_libs__/xbean-asm5-shaded-4.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3468/commons-cli-1.2.jar" "__spark_libs__/commons-cli-1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3353/jetty-6.1.26.jar" "__spark_libs__/jetty-6.1.26.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3282/spark-network-shuffle_2.11-2.0.1.jar" "__spark_libs__/spark-network-shuffle_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3316/jersey-server-2.22.2.jar" "__spark_libs__/jersey-server-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3276/zookeeper-3.4.6.jar" "__spark_libs__/zookeeper-3.4.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3317/curator-recipes-2.6.0.jar" "__spark_libs__/curator-recipes-2.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3322/hk2-api-2.4.0-b34.jar" "__spark_libs__/hk2-api-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3450/jackson-module-scala_2.11-2.6.5.jar" "__spark_libs__/jackson-module-scala_2.11-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3415/commons-net-2.2.jar" "__spark_libs__/commons-net-2.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3413/spark-core_2.11-2.0.1.jar" "__spark_libs__/spark-core_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3349/leveldbjni-all-1.8.jar" "__spark_libs__/leveldbjni-all-1.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3435/metrics-graphite-3.1.2.jar" "__spark_libs__/metrics-graphite-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3375/hadoop-annotations-2.7.1.jar" "__spark_libs__/hadoop-annotations-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3309/parquet-hadoop-bundle-1.6.0.jar" "__spark_libs__/parquet-hadoop-bundle-1.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3438/mail-1.4.7.jar" "__spark_libs__/mail-1.4.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3389/jersey-guava-2.22.2.jar" "__spark_libs__/jersey-guava-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3442/jackson-jaxrs-1.9.13.jar" "__spark_libs__/jackson-jaxrs-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3383/scala-parser-combinators_2.11-1.0.4.jar" "__spark_libs__/scala-parser-combinators_2.11-1.0.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3463/hk2-utils-2.4.0-b34.jar" "__spark_libs__/hk2-utils-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3318/jackson-module-paranamer-2.6.5.jar" "__spark_libs__/jackson-module-paranamer-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3274/avro-mapred-1.7.7-hadoop2.jar" "__spark_libs__/avro-mapred-1.7.7-hadoop2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3428/guice-3.0.jar" "__spark_libs__/guice-3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3302/commons-beanutils-1.7.0.jar" "__spark_libs__/commons-beanutils-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3441/xz-1.0.jar" "__spark_libs__/xz-1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3408/commons-httpclient-3.1.jar" "__spark_libs__/commons-httpclient-3.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3272/libthrift-0.9.2.jar" "__spark_libs__/libthrift-0.9.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3290/htrace-core-3.1.0-incubating.jar" "__spark_libs__/htrace-core-3.1.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3292/spark-tags_2.11-2.0.1.jar" "__spark_libs__/spark-tags_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3376/datanucleus-api-jdo-3.2.6.jar" "__spark_libs__/datanucleus-api-jdo-3.2.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3478/pmml-schema-1.2.15.jar" "__spark_libs__/pmml-schema-1.2.15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3434/osgi-resource-locator-1.0.1.jar" "__spark_libs__/osgi-resource-locator-1.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3347/compress-lzf-1.0.3.jar" "__spark_libs__/compress-lzf-1.0.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3437/commons-logging-1.1.3.jar" "__spark_libs__/commons-logging-1.1.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3350/hive-exec-1.2.1.spark2.jar" "__spark_libs__/hive-exec-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3280/apacheds-i18n-2.0.0-M15.jar" "__spark_libs__/apacheds-i18n-2.0.0-M15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3403/commons-collections-3.2.2.jar" "__spark_libs__/commons-collections-3.2.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3351/aopalliance-1.0.jar" "__spark_libs__/aopalliance-1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3366/antlr-2.7.7.jar" "__spark_libs__/antlr-2.7.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3335/spark-yarn_2.11-2.0.1.jar" "__spark_libs__/spark-yarn_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3287/scala-xml_2.11-1.0.2.jar" "__spark_libs__/scala-xml_2.11-1.0.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3314/jackson-core-2.6.5.jar" "__spark_libs__/jackson-core-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3361/scala-reflect-2.11.8.jar" "__spark_libs__/scala-reflect-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3447/hadoop-yarn-api-2.7.1.jar" "__spark_libs__/hadoop-yarn-api-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3414/xercesImpl-2.9.1.jar" "__spark_libs__/xercesImpl-2.9.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3400/commons-lang3-3.3.2.jar" "__spark_libs__/commons-lang3-3.3.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3432/snappy-java-1.1.2.6.jar" "__spark_libs__/snappy-java-1.1.2.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3390/stax-api-1.0-2.jar" "__spark_libs__/stax-api-1.0-2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3452/hadoop-mapreduce-client-jobclient-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-jobclient-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3278/calcite-core-1.2.0-incubating.jar" "__spark_libs__/calcite-core-1.2.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3368/commons-io-2.4.jar" "__spark_libs__/commons-io-2.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3294/validation-api-1.1.0.Final.jar" "__spark_libs__/validation-api-1.1.0.Final.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3300/json4s-jackson_2.11-3.2.11.jar" "__spark_libs__/json4s-jackson_2.11-3.2.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3382/spark-unsafe_2.11-2.0.1.jar" "__spark_libs__/spark-unsafe_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3325/commons-digester-1.8.jar" "__spark_libs__/commons-digester-1.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3321/univocity-parsers-2.1.1.jar" "__spark_libs__/univocity-parsers-2.1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3373/netty-3.8.0.Final.jar" "__spark_libs__/netty-3.8.0.Final.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3409/jdo-api-3.0.1.jar" "__spark_libs__/jdo-api-3.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3464/spark-hive_2.11-2.0.1.jar" "__spark_libs__/spark-hive_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3320/commons-dbcp-1.4.jar" "__spark_libs__/commons-dbcp-1.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3338/spark-launcher_2.11-2.0.1.jar" "__spark_libs__/spark-launcher_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3356/httpclient-4.5.2.jar" "__spark_libs__/httpclient-4.5.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3270/metrics-core-3.1.2.jar" "__spark_libs__/metrics-core-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3308/metrics-jvm-3.1.2.jar" "__spark_libs__/metrics-jvm-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3455/jackson-databind-2.6.5.jar" "__spark_libs__/jackson-databind-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3472/libfb303-0.9.2.jar" "__spark_libs__/libfb303-0.9.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3433/httpcore-4.4.4.jar" "__spark_libs__/httpcore-4.4.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3330/pyrolite-4.9.jar" "__spark_libs__/pyrolite-4.9.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3475/jersey-client-2.22.2.jar" "__spark_libs__/jersey-client-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3365/protobuf-java-2.5.0.jar" "__spark_libs__/protobuf-java-2.5.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3405/apacheds-kerberos-codec-2.0.0-M15.jar" "__spark_libs__/apacheds-kerberos-codec-2.0.0-M15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3323/scalap-2.11.8.jar" "__spark_libs__/scalap-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3429/oro-2.0.8.jar" "__spark_libs__/oro-2.0.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3391/javax.ws.rs-api-2.0.1.jar" "__spark_libs__/javax.ws.rs-api-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3295/hadoop-hdfs-2.7.1.jar" "__spark_libs__/hadoop-hdfs-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/usercache/fairy/filecache/11650/__spark_conf__.zip" "__spark_conf__"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3370/bcprov-jdk15on-1.51.jar" "__spark_libs__/bcprov-jdk15on-1.51.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3327/jersey-container-servlet-2.22.2.jar" "__spark_libs__/jersey-container-servlet-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3466/spark-hive-thriftserver_2.11-2.0.1.jar" "__spark_libs__/spark-hive-thriftserver_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3470/stream-2.7.0.jar" "__spark_libs__/stream-2.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3431/eigenbase-properties-1.1.5.jar" "__spark_libs__/eigenbase-properties-1.1.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3369/jaxb-api-2.2.2.jar" "__spark_libs__/jaxb-api-2.2.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3371/spire-macros_2.11-0.7.4.jar" "__spark_libs__/spire-macros_2.11-0.7.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3291/commons-pool-1.5.4.jar" "__spark_libs__/commons-pool-1.5.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3392/hadoop-yarn-common-2.7.1.jar" "__spark_libs__/hadoop-yarn-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3454/guava-14.0.1.jar" "__spark_libs__/guava-14.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3304/spark-mllib-local_2.11-2.0.1.jar" "__spark_libs__/spark-mllib-local_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3426/java-xmlbuilder-1.0.jar" "__spark_libs__/java-xmlbuilder-1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3456/activation-1.1.1.jar" "__spark_libs__/activation-1.1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3476/hadoop-mapreduce-client-app-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-app-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3359/commons-compress-1.4.1.jar" "__spark_libs__/commons-compress-1.4.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3467/stax-api-1.0.1.jar" "__spark_libs__/stax-api-1.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3381/hive-metastore-1.2.1.spark2.jar" "__spark_libs__/hive-metastore-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3394/commons-configuration-1.6.jar" "__spark_libs__/commons-configuration-1.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3345/javax.inject-1.jar" "__spark_libs__/javax.inject-1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3305/jersey-container-servlet-core-2.22.2.jar" "__spark_libs__/jersey-container-servlet-core-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3402/hadoop-yarn-server-common-2.7.1.jar" "__spark_libs__/hadoop-yarn-server-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3364/spark-sketch_2.11-2.0.1.jar" "__spark_libs__/spark-sketch_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3357/calcite-linq4j-1.2.0-incubating.jar" "__spark_libs__/calcite-linq4j-1.2.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3333/commons-codec-1.10.jar" "__spark_libs__/commons-codec-1.10.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3406/lz4-1.3.0.jar" "__spark_libs__/lz4-1.3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3421/datanucleus-rdbms-3.2.9.jar" "__spark_libs__/datanucleus-rdbms-3.2.9.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3395/log4j-1.2.17.jar" "__spark_libs__/log4j-1.2.17.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3474/netty-all-4.0.29.Final.jar" "__spark_libs__/netty-all-4.0.29.Final.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3459/jta-1.1.jar" "__spark_libs__/jta-1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3397/jpam-1.1.jar" "__spark_libs__/jpam-1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3279/guice-servlet-3.0.jar" "__spark_libs__/guice-servlet-3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3443/jackson-xc-1.9.13.jar" "__spark_libs__/jackson-xc-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3386/jersey-common-2.22.2.jar" "__spark_libs__/jersey-common-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3301/pmml-model-1.2.15.jar" "__spark_libs__/pmml-model-1.2.15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3430/javassist-3.18.1-GA.jar" "__spark_libs__/javassist-3.18.1-GA.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3453/apache-log4j-extras-1.2.17.jar" "__spark_libs__/apache-log4j-extras-1.2.17.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3420/derby-10.12.1.1.jar" "__spark_libs__/derby-10.12.1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3424/xmlenc-0.52.jar" "__spark_libs__/xmlenc-0.52.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3352/base64-2.3.8.jar" "__spark_libs__/base64-2.3.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3458/hadoop-yarn-client-2.7.1.jar" "__spark_libs__/hadoop-yarn-client-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3339/hadoop-yarn-server-web-proxy-2.7.1.jar" "__spark_libs__/hadoop-yarn-server-web-proxy-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3363/jersey-media-jaxb-2.22.2.jar" "__spark_libs__/jersey-media-jaxb-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3461/RoaringBitmap-0.5.11.jar" "__spark_libs__/RoaringBitmap-0.5.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/usercache/fairy/filecache/11649/stats-etlr-1.0.jar" "__app__.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3410/jul-to-slf4j-1.7.16.jar" "__spark_libs__/jul-to-slf4j-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3297/scala-compiler-2.11.8.jar" "__spark_libs__/scala-compiler-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3372/mesos-0.21.1-shaded-protobuf.jar" "__spark_libs__/mesos-0.21.1-shaded-protobuf.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3286/hadoop-client-2.7.1.jar" "__spark_libs__/hadoop-client-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3293/objenesis-2.1.jar" "__spark_libs__/objenesis-2.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3313/gson-2.2.4.jar" "__spark_libs__/gson-2.2.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3341/ivy-2.4.0.jar" "__spark_libs__/ivy-2.4.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3362/spark-catalyst_2.11-2.0.1.jar" "__spark_libs__/spark-catalyst_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3471/antlr4-runtime-4.5.3.jar" "__spark_libs__/antlr4-runtime-4.5.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3379/stringtemplate-3.2.1.jar" "__spark_libs__/stringtemplate-3.2.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3273/commons-beanutils-core-1.8.0.jar" "__spark_libs__/commons-beanutils-core-1.8.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3388/snappy-0.2.jar" "__spark_libs__/snappy-0.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3380/jsp-api-2.1.jar" "__spark_libs__/jsp-api-2.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3275/jetty-util-6.1.26.jar" "__spark_libs__/jetty-util-6.1.26.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3281/metrics-json-3.1.2.jar" "__spark_libs__/metrics-json-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3328/janino-2.7.8.jar" "__spark_libs__/janino-2.7.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3425/commons-compiler-2.7.6.jar" "__spark_libs__/commons-compiler-2.7.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3277/super-csv-2.2.0.jar" "__spark_libs__/super-csv-2.2.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3360/calcite-avatica-1.2.0-incubating.jar" "__spark_libs__/calcite-avatica-1.2.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3465/mx4j-3.0.2.jar" "__spark_libs__/mx4j-3.0.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3337/breeze_2.11-0.11.2.jar" "__spark_libs__/breeze_2.11-0.11.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3307/commons-math3-3.4.1.jar" "__spark_libs__/commons-math3-3.4.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3336/jackson-mapper-asl-1.9.13.jar" "__spark_libs__/jackson-mapper-asl-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3473/spire_2.11-0.7.4.jar" "__spark_libs__/spire_2.11-0.7.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3398/hive-jdbc-1.2.1.spark2.jar" "__spark_libs__/hive-jdbc-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3457/jodd-core-3.5.2.jar" "__spark_libs__/jodd-core-3.5.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3407/spark-mllib_2.11-2.0.1.jar" "__spark_libs__/spark-mllib_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3319/jets3t-0.9.3.jar" "__spark_libs__/jets3t-0.9.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3439/parquet-column-1.7.0.jar" "__spark_libs__/parquet-column-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3346/parquet-hadoop-1.7.0.jar" "__spark_libs__/parquet-hadoop-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3374/curator-client-2.6.0.jar" "__spark_libs__/curator-client-2.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3436/curator-framework-2.6.0.jar" "__spark_libs__/curator-framework-2.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3427/kryo-shaded-3.0.3.jar" "__spark_libs__/kryo-shaded-3.0.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3332/datanucleus-core-3.2.10.jar" "__spark_libs__/datanucleus-core-3.2.10.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3451/arpack_combined_all-0.1.jar" "__spark_libs__/arpack_combined_all-0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3298/avro-1.7.7.jar" "__spark_libs__/avro-1.7.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3331/core-1.1.2.jar" "__spark_libs__/core-1.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3271/hadoop-mapreduce-client-core-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-core-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3385/jtransforms-2.4.0.jar" "__spark_libs__/jtransforms-2.4.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3411/minlog-1.3.0.jar" "__spark_libs__/minlog-1.3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3324/jackson-core-asl-1.9.13.jar" "__spark_libs__/jackson-core-asl-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3404/chill_2.11-0.8.0.jar" "__spark_libs__/chill_2.11-0.8.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3387/api-asn1-api-1.0.0-M20.jar" "__spark_libs__/api-asn1-api-1.0.0-M20.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3469/parquet-generator-1.7.0.jar" "__spark_libs__/parquet-generator-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3446/json4s-ast_2.11-3.2.11.jar" "__spark_libs__/json4s-ast_2.11-3.2.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3412/javax.annotation-api-1.2.jar" "__spark_libs__/javax.annotation-api-1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3418/hive-beeline-1.2.1.spark2.jar" "__spark_libs__/hive-beeline-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3285/json-20090211.jar" "__spark_libs__/json-20090211.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3326/opencsv-2.3.jar" "__spark_libs__/opencsv-2.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3448/jline-2.12.1.jar" "__spark_libs__/jline-2.12.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3477/hadoop-mapreduce-client-common-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3303/json4s-core_2.11-3.2.11.jar" "__spark_libs__/json4s-core_2.11-3.2.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3315/spark-repl_2.11-2.0.1.jar" "__spark_libs__/spark-repl_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3358/scala-library-2.11.8.jar" "__spark_libs__/scala-library-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3416/spark-sql_2.11-2.0.1.jar" "__spark_libs__/spark-sql_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3311/mysql-connector-java-5.0.8-bin.jar" "__spark_libs__/mysql-connector-java-5.0.8-bin.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3422/joda-time-2.9.3.jar" "__spark_libs__/joda-time-2.9.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3288/javax.servlet-api-3.1.0.jar" "__spark_libs__/javax.servlet-api-3.1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3354/hk2-locator-2.4.0-b34.jar" "__spark_libs__/hk2-locator-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3423/jsr305-1.3.9.jar" "__spark_libs__/jsr305-1.3.9.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3419/spark-network-common_2.11-2.0.1.jar" "__spark_libs__/spark-network-common_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3460/javolution-5.5.1.jar" "__spark_libs__/javolution-5.5.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3440/parquet-jackson-1.7.0.jar" "__spark_libs__/parquet-jackson-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3329/breeze-macros_2.11-0.11.2.jar" "__spark_libs__/breeze-macros_2.11-0.11.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3399/hadoop-auth-2.7.1.jar" "__spark_libs__/hadoop-auth-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3334/slf4j-api-1.7.16.jar" "__spark_libs__/slf4j-api-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3462/lift-json_2.11-2.6.3.jar" "__spark_libs__/lift-json_2.11-2.6.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3340/slf4j-log4j12-1.7.16.jar" "__spark_libs__/slf4j-log4j12-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3396/parquet-format-2.3.0-incubating.jar" "__spark_libs__/parquet-format-2.3.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3289/javax.inject-2.4.0-b34.jar" "__spark_libs__/javax.inject-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3393/py4j-0.10.3.jar" "__spark_libs__/py4j-0.10.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3283/jackson-annotations-2.6.5.jar" "__spark_libs__/jackson-annotations-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3444/hadoop-mapreduce-client-shuffle-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-shuffle-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3343/chill-java-0.8.0.jar" "__spark_libs__/chill-java-0.8.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3378/commons-lang-2.6.jar" "__spark_libs__/commons-lang-2.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3342/bonecp-0.8.0.RELEASE.jar" "__spark_libs__/bonecp-0.8.0.RELEASE.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3312/JavaEWAH-0.3.2.jar" "__spark_libs__/JavaEWAH-0.3.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3344/jersey-bundle-1.19.1.jar" "__spark_libs__/jersey-bundle-1.19.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3377/aopalliance-repackaged-2.4.0-b34.jar" "__spark_libs__/aopalliance-repackaged-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3348/ST4-4.0.4.jar" "__spark_libs__/ST4-4.0.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3417/api-util-1.0.0-M20.jar" "__spark_libs__/api-util-1.0.0-M20.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3401/parquet-common-1.7.0.jar" "__spark_libs__/parquet-common-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3367/antlr-runtime-3.4.jar" "__spark_libs__/antlr-runtime-3.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
# Creating copy of launch script
cp "launch_container.sh" "/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000008/launch_container.sh"
chmod 640 "/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000008/launch_container.sh"
# Determining directory contents
echo "ls -l:" 1>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000008/directory.info"
ls -l 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000008/directory.info"
echo "find -L . -maxdepth 5 -ls:" 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000008/directory.info"
find -L . -maxdepth 5 -ls 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000008/directory.info"
echo "broken symlinks(find -L . -maxdepth 5 -type l -ls):" 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000008/directory.info"
find -L . -maxdepth 5 -type l -ls 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000008/directory.info"
exec /bin/bash -c "$JAVA_HOME/bin/java -server -Xmx3072m '-Dhdp.version=2.4.2.0-258' -Djava.io.tmpdir=$PWD/tmp '-Dspark.history.ui.port=18080' '-Dspark.ui.port=0' '-Dspark.driver.port=19210' -Dspark.yarn.app.container.log.dir=/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000008 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.60.43.33:19210 --executor-id 7 --hostname c427.hadoop.gda.lo --cores 1 --app-id application_1494395298335_303847 --user-class-path file:$PWD/__app__.jar 1> /hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000008/stdout 2> /hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000008/stderr"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
End of LogType:launch_container.sh

LogType:stderr
Log Upload Time:Tue May 16 11:12:18 +0700 2017
LogLength:900
Log Contents:
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/hadoop/yarn/local/filecache/3340/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/hdp/2.4.2.0-258/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
17/05/16 11:10:55 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 1395@c427.hadoop.gda.lo
17/05/16 11:10:55 INFO util.SignalUtils: Registered signal handler for TERM
17/05/16 11:10:55 INFO util.SignalUtils: Registered signal handler for HUP
17/05/16 11:10:55 INFO util.SignalUtils: Registered signal handler for INT
17/05/16 11:10:55 ERROR executor.CoarseGrainedExecutorBackend: RECEIVED SIGNAL TERM
End of LogType:stderr

LogType:stdout
Log Upload Time:Tue May 16 11:12:18 +0700 2017
LogLength:0
Log Contents:
End of LogType:stdout



Container: container_e81_1494395298335_303847_01_000006 on c427.hadoop.gda.lo_45454
=====================================================================================
LogType:directory.info
Log Upload Time:Tue May 16 11:12:18 +0700 2017
LogLength:28883
Log Contents:
ls -l:
total 112
lrwxrwxrwx 1 yarn hadoop    69 May 16 11:10 __app__.jar -> /hadoop/yarn/local/usercache/fairy/filecache/11649/stats-etlr-1.0.jar
-rw-r--r-- 1 yarn hadoop    87 May 16 11:10 container_tokens
-rwx------ 1 yarn hadoop   676 May 16 11:10 default_container_executor_session.sh
-rwx------ 1 yarn hadoop   730 May 16 11:10 default_container_executor.sh
-rwx------ 1 yarn hadoop 75554 May 16 11:10 launch_container.sh
lrwxrwxrwx 1 yarn hadoop    69 May 16 11:10 __spark_conf__ -> /hadoop/yarn/local/usercache/fairy/filecache/11650/__spark_conf__.zip
drwxr-xr-x 2 yarn hadoop 12288 May 16 11:10 __spark_libs__
drwx--x--- 2 yarn hadoop  4096 May 16 11:10 tmp
find -L . -maxdepth 5 -ls:
14286850    4 drwx--x---   4 yarn     hadoop       4096 May 16 11:10 .
14286854    4 -rw-r--r--   1 yarn     hadoop         12 May 16 11:10 ./.container_tokens.crc
11142301    4 drwx------   2 yarn     hadoop       4096 May 16 11:10 ./__spark_conf__
11142407    4 -r-x------   1 yarn     hadoop        945 May 16 11:10 ./__spark_conf__/taskcontroller.cfg
11142317    4 -r-x------   1 yarn     hadoop        884 May 16 11:10 ./__spark_conf__/ssl-client.xml
11142349    8 -r-x------   1 yarn     hadoop       4221 May 16 11:10 ./__spark_conf__/task-log4j.properties
11142388    4 -r-x------   1 yarn     hadoop       2268 May 16 11:10 ./__spark_conf__/ssl-server.xml.example
11142311    0 -r-x------   1 yarn     hadoop          0 May 16 11:10 ./__spark_conf__/yarn.exclude
11142314    4 -r-x------   1 yarn     hadoop       2250 May 16 11:10 ./__spark_conf__/yarn-env.cmd
11142375    4 -r-x------   1 yarn     hadoop        229 May 16 11:10 ./__spark_conf__/slaves
11142369    4 -r-x------   1 yarn     hadoop       1527 May 16 11:10 ./__spark_conf__/kms-env.sh
11142347    4 -r-x------   1 yarn     hadoop       2490 May 16 11:10 ./__spark_conf__/hadoop-metrics.properties
11142366    4 -r-x------   1 yarn     hadoop        690 May 16 11:10 ./__spark_conf__/mapred-env.sh
11142386    4 -r-x------   1 yarn     hadoop       1000 May 16 11:10 ./__spark_conf__/ssl-server.xml
11142323    4 -r-x------   1 yarn     hadoop       3518 May 16 11:10 ./__spark_conf__/kms-acls.xml
11142307   12 -r-x------   1 yarn     hadoop       8699 May 16 11:10 ./__spark_conf__/log4j.properties
11142341    8 -r-x------   1 yarn     hadoop       6572 May 16 11:10 ./__spark_conf__/hdfs-site.xml
11142309   20 -r-x------   1 yarn     hadoop      16530 May 16 11:10 ./__spark_conf__/yarn-site.xml
11142344    4 -r-x------   1 yarn     hadoop       1019 May 16 11:10 ./__spark_conf__/container-executor.cfg
11142304    8 -r-x------   1 yarn     hadoop       5467 May 16 11:10 ./__spark_conf__/hadoop-env.sh
11142329    4 -r-x------   1 yarn     hadoop       2358 May 16 11:10 ./__spark_conf__/topology_script.py
11142380    4 -r-x------   1 yarn     hadoop        951 May 16 11:10 ./__spark_conf__/mapred-env.cmd
11142310    4 -r-x------   1 yarn     hadoop       3979 May 16 11:10 ./__spark_conf__/hadoop-env.cmd
11142367    4 -r-x------   1 yarn     hadoop       1602 May 16 11:10 ./__spark_conf__/health_check
11142313    4 -r-x------   1 yarn     hadoop       1631 May 16 11:10 ./__spark_conf__/kms-log4j.properties
11142302    8 -r-x------   1 yarn     hadoop       6836 May 16 11:10 ./__spark_conf__/mapred-site.xml
11142378    4 -r-x------   1 yarn     hadoop        724 May 16 11:10 ./__spark_conf__/topology_mappings.data
11142392    8 -r-x------   1 yarn     hadoop       4113 May 16 11:10 ./__spark_conf__/mapred-queues.xml.template
11142327    4 -r-x------   1 yarn     hadoop        758 May 16 11:10 ./__spark_conf__/mapred-site.xml.template
11142345    4 -r-x------   1 yarn     hadoop       1020 May 16 11:10 ./__spark_conf__/commons-logging.properties
11142406    8 -r-x------   1 yarn     hadoop       5511 May 16 11:10 ./__spark_conf__/kms-site.xml
11142368    4 -r-x------   1 yarn     hadoop       2316 May 16 11:10 ./__spark_conf__/ssl-client.xml.example
11142408   36 -r-x------   1 yarn     hadoop      34061 May 16 11:10 ./__spark_conf__/__spark_conf__.properties
11142372    4 -r-x------   1 yarn     hadoop       1308 May 16 11:10 ./__spark_conf__/hadoop-policy.xml
11142308    4 -r-x------   1 yarn     hadoop       2302 May 16 11:10 ./__spark_conf__/hadoop-metrics2.properties
11142339    8 -r-x------   1 yarn     hadoop       5006 May 16 11:10 ./__spark_conf__/yarn-env.sh
11142322    8 -r-x------   1 yarn     hadoop       5404 May 16 11:10 ./__spark_conf__/capacity-scheduler.xml
11142331    4 -r-x------   1 yarn     hadoop       1335 May 16 11:10 ./__spark_conf__/configuration.xsl
11142312    8 -r-x------   1 yarn     hadoop       4197 May 16 11:10 ./__spark_conf__/core-site.xml
14286858    4 -rw-r--r--   1 yarn     hadoop         16 May 16 11:10 ./.default_container_executor_session.sh.crc
14286855   76 -rwx------   1 yarn     hadoop      75554 May 16 11:10 ./launch_container.sh
14286860    4 -rw-r--r--   1 yarn     hadoop         16 May 16 11:10 ./.default_container_executor.sh.crc
14286853    4 -rw-r--r--   1 yarn     hadoop         87 May 16 11:10 ./container_tokens
11142202 1152 -r-x------   1 yarn     hadoop    1178454 May 16 11:10 ./__app__.jar
14286861   12 drwxr-xr-x   2 yarn     hadoop      12288 May 16 11:10 ./__spark_libs__
11928380 2044 -rwxr-xr-x   1 yarn     hadoop    2090370 Mar 27 13:37 ./__spark_libs__/spark-streaming_2.11-2.0.1.jar
11928483   92 -rwxr-xr-x   1 yarn     hadoop      93407 Mar 27 13:37 ./__spark_libs__/pyrolite-4.9.jar
11928357   96 -rwxr-xr-x   1 yarn     hadoop      95806 Mar 27 13:37 ./__spark_libs__/javax.servlet-api-3.1.0.jar
11929220  736 -rwxr-xr-x   1 yarn     hadoop     753012 Mar 27 13:37 ./__spark_libs__/hadoop-mapreduce-client-common-2.7.1.jar
11928757 1204 -rwxr-xr-x   1 yarn     hadoop    1229125 Mar 27 13:37 ./__spark_libs__/xercesImpl-2.9.1.jar
11928327 3440 -rwxr-xr-x   1 yarn     hadoop    3519262 Mar 27 13:37 ./__spark_libs__/calcite-core-1.2.0-incubating.jar
11928719  356 -rwxr-xr-x   1 yarn     hadoop     363908 Mar 27 13:37 ./__spark_libs__/hadoop-yarn-server-common-2.7.1.jar
11929137 3352 -rwxr-xr-x   1 yarn     hadoop    3431544 Mar 27 13:37 ./__spark_libs__/hadoop-common-2.7.1.jar
11928457  176 -rwxr-xr-x   1 yarn     hadoop     178947 Mar 27 13:37 ./__spark_libs__/hk2-api-2.4.0-b34.jar
11929124   44 -rwxr-xr-x   1 yarn     hadoop      45015 Mar 27 13:37 ./__spark_libs__/hadoop-mapreduce-client-shuffle-2.7.1.jar
11928626   88 -rwxr-xr-x   1 yarn     hadoop      86811 Mar 27 13:37 ./__spark_libs__/spire-macros_2.11-0.7.4.jar
11928732  232 -rwxr-xr-x   1 yarn     hadoop     236880 Mar 27 13:37 ./__spark_libs__/lz4-1.3.0.jar
11929169  388 -rwxr-xr-x   1 yarn     hadoop     395195 Mar 27 13:37 ./__spark_libs__/javolution-5.5.1.jar
11929158   68 -rwxr-xr-x   1 yarn     hadoop      69409 Mar 27 13:37 ./__spark_libs__/activation-1.1.1.jar
11928524   52 -rwxr-xr-x   1 yarn     hadoop      50619 Mar 27 13:37 ./__spark_libs__/chill-java-0.8.0.jar
11928698  480 -rwxr-xr-x   1 yarn     hadoop     489884 Mar 27 13:37 ./__spark_libs__/log4j-1.2.17.jar
11928361    8 -rwxr-xr-x   1 yarn     hadoop       5950 Mar 27 13:37 ./__spark_libs__/javax.inject-2.4.0-b34.jar
11928831  320 -rwxr-xr-x   1 yarn     hadoop     326724 Mar 27 13:37 ./__spark_libs__/httpcore-4.4.4.jar
11928851   24 -rwxr-xr-x   1 yarn     hadoop      20852 Mar 27 13:37 ./__spark_libs__/metrics-graphite-3.1.2.jar
11928434  188 -rwxr-xr-x   1 yarn     hadoop     190432 Mar 27 13:37 ./__spark_libs__/gson-2.2.4.jar
11928498  656 -rwxr-xr-x   1 yarn     hadoop     669589 Mar 27 13:37 ./__spark_libs__/spark-yarn_2.11-2.0.1.jar
11928377 8068 -rwxr-xr-x   1 yarn     hadoop    8260573 Mar 27 13:37 ./__spark_libs__/hadoop-hdfs-2.7.1.jar
11928572  192 -rwxr-xr-x   1 yarn     hadoop     192993 Mar 27 13:37 ./__spark_libs__/avro-ipc-1.7.7.jar
11929211 2008 -rwxr-xr-x   1 yarn     hadoop    2054931 Mar 27 13:37 ./__spark_libs__/netty-all-4.0.29.Final.jar
11928683   24 -rwxr-xr-x   1 yarn     hadoop      23346 Mar 27 13:37 ./__spark_libs__/stax-api-1.0-2.jar
11928813   20 -rwxr-xr-x   1 yarn     hadoop      18482 Mar 27 13:37 ./__spark_libs__/eigenbase-properties-1.1.5.jar
11928722  576 -rwxr-xr-x   1 yarn     hadoop     588337 Mar 27 13:37 ./__spark_libs__/commons-collections-3.2.2.jar
11928477  600 -rwxr-xr-x   1 yarn     hadoop     613299 Mar 27 13:37 ./__spark_libs__/janino-2.7.8.jar
11928460 1024 -rwxr-xr-x   1 yarn     hadoop    1045744 Mar 27 13:37 ./__spark_libs__/leveldbjni-all-1.8.jar
11928441  932 -rwxr-xr-x   1 yarn     hadoop     951701 Mar 27 13:37 ./__spark_libs__/jersey-server-2.22.2.jar
11928622 2780 -rwxr-xr-x   1 yarn     hadoop    2842667 Mar 27 13:37 ./__spark_libs__/bcprov-jdk15on-1.51.jar
11928768  136 -rwxr-xr-x   1 yarn     hadoop     138464 Mar 27 13:37 ./__spark_libs__/hive-beeline-1.2.1.spark2.jar
11928464  228 -rwxr-xr-x   1 yarn     hadoop     232248 Mar 27 13:37 ./__spark_libs__/jackson-core-asl-1.9.13.jar
11928480  136 -rwxr-xr-x   1 yarn     hadoop     135552 Mar 27 13:37 ./__spark_libs__/breeze-macros_2.11-0.11.2.jar
11928691  116 -rwxr-xr-x   1 yarn     hadoop     114913 Mar 27 13:37 ./__spark_libs__/py4j-0.10.3.jar
11928342   40 -rwxr-xr-x   1 yarn     hadoop      40817 Mar 27 13:37 ./__spark_libs__/hive-cli-1.2.1.spark2.jar
11928330   64 -rwxr-xr-x   1 yarn     hadoop      65012 Mar 27 13:37 ./__spark_libs__/guice-servlet-3.0.jar
11927553  164 -rwxr-xr-x   1 yarn     hadoop     164422 Mar 27 13:37 ./__spark_libs__/core-1.1.2.jar
11929150 2140 -rwxr-xr-x   1 yarn     hadoop    2189117 Mar 27 13:37 ./__spark_libs__/guava-14.0.1.jar
11929206 7108 -rwxr-xr-x   1 yarn     hadoop    7276083 Mar 27 13:37 ./__spark_libs__/spire_2.11-0.7.4.jar
11928679  952 -rwxr-xr-x   1 yarn     hadoop     971310 Mar 27 13:37 ./__spark_libs__/jersey-guava-2.22.2.jar
11928349    4 -rwxr-xr-x   1 yarn     hadoop       2545 Mar 27 13:37 ./__spark_libs__/hadoop-client-2.7.1.jar
11928443  284 -rwxr-xr-x   1 yarn     hadoop     290506 Mar 27 13:37 ./__spark_libs__/univocity-parsers-2.1.1.jar
11928402  576 -rwxr-xr-x   1 yarn     hadoop     589462 Mar 27 13:37 ./__spark_libs__/json4s-core_2.11-3.2.11.jar
11929143 1168 -rwxr-xr-x   1 yarn     hadoop    1194003 Mar 27 13:37 ./__spark_libs__/arpack_combined_all-0.1.jar
11928631 1204 -rwxr-xr-x   1 yarn     hadoop    1230201 Mar 27 13:37 ./__spark_libs__/netty-3.8.0.Final.jar
11928382  280 -rwxr-xr-x   1 yarn     hadoop     284220 Mar 27 13:37 ./__spark_libs__/commons-lang-2.6.jar
11928674   20 -rwxr-xr-x   1 yarn     hadoop      16560 Mar 27 13:37 ./__spark_libs__/api-asn1-api-1.0.0-M20.jar
11928350   48 -rwxr-xr-x   1 yarn     hadoop      45944 Mar 27 13:37 ./__spark_libs__/json-20090211.jar
11928788   36 -rwxr-xr-x   1 yarn     hadoop      33015 Mar 27 13:37 ./__spark_libs__/jsr305-1.3.9.jar
11928635   68 -rwxr-xr-x   1 yarn     hadoop      68866 Mar 27 13:37 ./__spark_libs__/curator-client-2.6.0.jar
11928620   20 -rwxr-xr-x   1 yarn     hadoop      17385 Mar 27 13:37 ./__spark_libs__/hadoop-annotations-2.7.1.jar
11928715   24 -rwxr-xr-x   1 yarn     hadoop      21575 Mar 27 13:37 ./__spark_libs__/parquet-common-1.7.0.jar
11928314  180 -rwxr-xr-x   1 yarn     hadoop     180736 Mar 27 13:37 ./__spark_libs__/avro-mapred-1.7.7-hadoop2.jar
11928654 5380 -rwxr-xr-x   1 yarn     hadoop    5505200 Mar 27 13:37 ./__spark_libs__/hive-metastore-1.2.1.spark2.jar
11928420 2732 -rwxr-xr-x   1 yarn     hadoop    2796935 Mar 27 13:37 ./__spark_libs__/parquet-hadoop-bundle-1.6.0.jar
11928365   96 -rwxr-xr-x   1 yarn     hadoop      96221 Mar 27 13:37 ./__spark_libs__/commons-pool-1.5.4.jar
11928501  764 -rwxr-xr-x   1 yarn     hadoop     780664 Mar 27 13:37 ./__spark_libs__/jackson-mapper-asl-1.9.13.jar
11928766   80 -rwxr-xr-x   1 yarn     hadoop      79912 Mar 27 13:37 ./__spark_libs__/api-util-1.0.0-M20.jar
11928320  776 -rwxr-xr-x   1 yarn     hadoop     792964 Mar 27 13:37 ./__spark_libs__/zookeeper-3.4.6.jar
11928396  640 -rwxr-xr-x   1 yarn     hadoop     654216 Mar 27 13:37 ./__spark_libs__/pmml-model-1.2.15.jar
11928605  524 -rwxr-xr-x   1 yarn     hadoop     533455 Mar 27 13:37 ./__spark_libs__/protobuf-java-2.5.0.jar
11929121  668 -rwxr-xr-x   1 yarn     hadoop     680106 Mar 27 13:37 ./__spark_libs__/spark-graphx_2.11-2.0.1.jar
11928339   56 -rwxr-xr-x   1 yarn     hadoop      55511 Mar 27 13:37 ./__spark_libs__/spark-network-shuffle_2.11-2.0.1.jar
11929172  200 -rwxr-xr-x   1 yarn     hadoop     201928 Mar 27 13:37 ./__spark_libs__/RoaringBitmap-0.5.11.jar
11928655  100 -rwxr-xr-x   1 yarn     hadoop     100636 Mar 27 13:37 ./__spark_libs__/jsp-api-2.1.jar
11929173  468 -rwxr-xr-x   1 yarn     hadoop     477970 Mar 27 13:37 ./__spark_libs__/lift-json_2.11-2.6.3.jar
11929123   28 -rwxr-xr-x   1 yarn     hadoop      27084 Mar 27 13:37 ./__spark_libs__/jackson-xc-1.9.13.jar
11929154 1144 -rwxr-xr-x   1 yarn     hadoop    1171380 Mar 27 13:37 ./__spark_libs__/jackson-databind-2.6.5.jar
11928522    4 -rwxr-xr-x   1 yarn     hadoop       2497 Mar 27 13:37 ./__spark_libs__/javax.inject-1.jar
11928685  116 -rwxr-xr-x   1 yarn     hadoop     115534 Mar 27 13:37 ./__spark_libs__/javax.ws.rs-api-2.0.1.jar
11928535  208 -rwxr-xr-x   1 yarn     hadoop     209622 Mar 27 13:37 ./__spark_libs__/parquet-hadoop-1.7.0.jar
11928471   20 -rwxr-xr-x   1 yarn     hadoop      19827 Mar 27 13:37 ./__spark_libs__/opencsv-2.3.jar
11929181 1024 -rwxr-xr-x   1 yarn     hadoop    1044511 Mar 27 13:37 ./__spark_libs__/spark-hive_2.11-2.0.1.jar
11928650  148 -rwxr-xr-x   1 yarn     hadoop     148627 Mar 27 13:37 ./__spark_libs__/stringtemplate-3.2.1.jar
11928318  176 -rwxr-xr-x   1 yarn     hadoop     177131 Mar 27 13:37 ./__spark_libs__/jetty-util-6.1.26.jar
11929223    8 -rwxr-xr-x   1 yarn     hadoop       5310 Mar 27 13:37 ./__spark_libs__/pmml-schema-1.2.15.jar
11929159  420 -rwxr-xr-x   1 yarn     hadoop     427780 Mar 27 13:37 ./__spark_libs__/jodd-core-3.5.2.jar
11928399  188 -rwxr-xr-x   1 yarn     hadoop     188671 Mar 27 13:37 ./__spark_libs__/commons-beanutils-1.7.0.jar
11928566  528 -rwxr-xr-x   1 yarn     hadoop     539912 Mar 27 13:37 ./__spark_libs__/jetty-6.1.26.jar
11928546 11232 -rwxr-xr-x   1 yarn     hadoop   11498852 Mar 27 13:37 ./__spark_libs__/hive-exec-1.2.1.spark2.jar
11929217  508 -rwxr-xr-x   1 yarn     hadoop     516127 Mar 27 13:37 ./__spark_libs__/hadoop-mapreduce-client-app-2.7.1.jar
11928821  700 -rwxr-xr-x   1 yarn     hadoop     714194 Mar 27 13:37 ./__spark_libs__/javassist-3.18.1-GA.jar
11928492  280 -rwxr-xr-x   1 yarn     hadoop     284184 Mar 27 13:37 ./__spark_libs__/commons-codec-1.10.jar
11928408   68 -rwxr-xr-x   1 yarn     hadoop      66270 Mar 27 13:37 ./__spark_libs__/jersey-container-servlet-core-2.22.2.jar
11928701  380 -rwxr-xr-x   1 yarn     hadoop     387188 Mar 27 13:37 ./__spark_libs__/parquet-format-2.3.0-incubating.jar
11928593 4468 -rwxr-xr-x   1 yarn     hadoop    4573750 Mar 27 13:37 ./__spark_libs__/scala-reflect-2.11.8.jar
11928423   32 -rwxr-xr-x   1 yarn     hadoop      29555 Mar 27 13:37 ./__spark_libs__/paranamer-2.3.jar
11928445  244 -rwxr-xr-x   1 yarn     hadoop     248171 Mar 27 13:37 ./__spark_libs__/curator-recipes-2.6.0.jar
11929127   84 -rwxr-xr-x   1 yarn     hadoop      82421 Mar 27 13:37 ./__spark_libs__/json4s-ast_2.11-3.2.11.jar
11928468  144 -rwxr-xr-x   1 yarn     hadoop     143602 Mar 27 13:37 ./__spark_libs__/commons-digester-1.8.jar
11928487 1848 -rwxr-xr-x   1 yarn     hadoop    1890075 Mar 27 13:37 ./__spark_libs__/datanucleus-core-3.2.10.jar
11929178  120 -rwxr-xr-x   1 yarn     hadoop     118973 Mar 27 13:37 ./__spark_libs__/hk2-utils-2.4.0-b34.jar
11928730 2304 -rwxr-xr-x   1 yarn     hadoop    2355465 Mar 27 13:37 ./__spark_libs__/spark-network-common_2.11-2.0.1.jar
11928417   40 -rwxr-xr-x   1 yarn     hadoop      39280 Mar 27 13:37 ./__spark_libs__/metrics-jvm-3.1.2.jar
11929193   44 -rwxr-xr-x   1 yarn     hadoop      41123 Mar 27 13:37 ./__spark_libs__/commons-cli-1.2.jar
11928662  416 -rwxr-xr-x   1 yarn     hadoop     423753 Mar 27 13:37 ./__spark_libs__/scala-parser-combinators_2.11-1.0.4.jar
11928311  204 -rwxr-xr-x   1 yarn     hadoop     206035 Mar 27 13:37 ./__spark_libs__/commons-beanutils-core-1.8.0.jar
11928474   20 -rwxr-xr-x   1 yarn     hadoop      18098 Mar 27 13:37 ./__spark_libs__/jersey-container-servlet-2.22.2.jar
11928712  404 -rwxr-xr-x   1 yarn     hadoop     412739 Mar 27 13:37 ./__spark_libs__/commons-lang3-3.3.2.jar
11928369   16 -rwxr-xr-x   1 yarn     hadoop      15305 Mar 27 13:37 ./__spark_libs__/spark-tags_2.11-2.0.1.jar
11929196   24 -rwxr-xr-x   1 yarn     hadoop      21243 Mar 27 13:37 ./__spark_libs__/parquet-generator-1.7.0.jar
11929167   16 -rwxr-xr-x   1 yarn     hadoop      15071 Mar 27 13:37 ./__spark_libs__/jta-1.1.jar
11929198  172 -rwxr-xr-x   1 yarn     hadoop     174351 Mar 27 13:37 ./__spark_libs__/stream-2.7.0.jar
11928433  256 -rwxr-xr-x   1 yarn     hadoop     258876 Mar 27 13:37 ./__spark_libs__/jackson-core-2.6.5.jar
11928786  616 -rwxr-xr-x   1 yarn     hadoop     627814 Mar 27 13:37 ./__spark_libs__/joda-time-2.9.3.jar
11928618  104 -rwxr-xr-x   1 yarn     hadoop     105134 Mar 27 13:37 ./__spark_libs__/jaxb-api-2.2.2.jar
11928372   44 -rwxr-xr-x   1 yarn     hadoop      41755 Mar 27 13:37 ./__spark_libs__/objenesis-2.1.jar
11928447   44 -rwxr-xr-x   1 yarn     hadoop      41263 Mar 27 13:37 ./__spark_libs__/jackson-module-paranamer-2.6.5.jar
11928415 1988 -rwxr-xr-x   1 yarn     hadoop    2035066 Mar 27 13:37 ./__spark_libs__/commons-math3-3.4.1.jar
11928791   16 -rwxr-xr-x   1 yarn     hadoop      15010 Mar 27 13:37 ./__spark_libs__/xmlenc-0.52.jar
11928801   32 -rwxr-xr-x   1 yarn     hadoop      30595 Mar 27 13:37 ./__spark_libs__/commons-compiler-2.7.6.jar
11929163  140 -rwxr-xr-x   1 yarn     hadoop     142631 Mar 27 13:37 ./__spark_libs__/hadoop-yarn-client-2.7.1.jar
11928599   72 -rwxr-xr-x   1 yarn     hadoop      72733 Mar 27 13:37 ./__spark_libs__/jersey-media-jaxb-2.22.2.jar
11928776   64 -rwxr-xr-x   1 yarn     hadoop      65261 Mar 27 13:37 ./__spark_libs__/oro-2.0.8.jar
11928637   20 -rwxr-xr-x   1 yarn     hadoop      20235 Mar 27 13:37 ./__spark_libs__/osgi-resource-locator-1.0.1.jar
11929214  164 -rwxr-xr-x   1 yarn     hadoop     167421 Mar 27 13:37 ./__spark_libs__/jersey-client-2.22.2.jar
11928453  160 -rwxr-xr-x   1 yarn     hadoop     160519 Mar 27 13:37 ./__spark_libs__/commons-dbcp-1.4.jar
11928578  184 -rwxr-xr-x   1 yarn     hadoop     185140 Mar 27 13:37 ./__spark_libs__/commons-io-2.4.jar
11928670  684 -rwxr-xr-x   1 yarn     hadoop     698375 Mar 27 13:37 ./__spark_libs__/jersey-common-2.22.2.jar
11928539  232 -rwxr-xr-x   1 yarn     hadoop     236660 Mar 27 13:37 ./__spark_libs__/ST4-4.0.4.jar
11928866  184 -rwxr-xr-x   1 yarn     hadoop     185245 Mar 27 13:37 ./__spark_libs__/curator-framework-2.6.0.jar
11929113   96 -rwxr-xr-x   1 yarn     hadoop      94672 Mar 27 13:37 ./__spark_libs__/xz-1.0.jar
11929185  400 -rwxr-xr-x   1 yarn     hadoop     409467 Mar 27 13:37 ./__spark_libs__/mx4j-3.0.2.jar
11929082 1024 -rwxr-xr-x   1 yarn     hadoop    1048110 Mar 27 13:37 ./__spark_libs__/parquet-jackson-1.7.0.jar
11928336   16 -rwxr-xr-x   1 yarn     hadoop      15827 Mar 27 13:37 ./__spark_libs__/metrics-json-3.1.2.jar
11928745    8 -rwxr-xr-x   1 yarn     hadoop       4596 Mar 27 13:37 ./__spark_libs__/jul-to-slf4j-1.7.16.jar
11928485  112 -rwxr-xr-x   1 yarn     hadoop     110600 Mar 27 13:37 ./__spark_libs__/bonecp-0.8.0.RELEASE.jar
11928569  180 -rwxr-xr-x   1 yarn     hadoop     181271 Mar 27 13:37 ./__spark_libs__/hk2-locator-2.4.0-b34.jar
11928740  200 -rwxr-xr-x   1 yarn     hadoop     201124 Mar 27 13:37 ./__spark_libs__/jdo-api-3.0.1.jar
11928516  436 -rwxr-xr-x   1 yarn     hadoop     442406 Mar 27 13:37 ./__spark_libs__/calcite-linq4j-1.2.0-incubating.jar
11928307  224 -rwxr-xr-x   1 yarn     hadoop     227712 Mar 27 13:37 ./__spark_libs__/libthrift-0.9.2.jar
11928724  220 -rwxr-xr-x   1 yarn     hadoop     223573 Mar 27 13:37 ./__spark_libs__/chill_2.11-0.8.0.jar
11928643   16 -rwxr-xr-x   1 yarn     hadoop      14766 Mar 27 13:37 ./__spark_libs__/aopalliance-repackaged-2.4.0-b34.jar
11928667  280 -rwxr-xr-x   1 yarn     hadoop     285447 Mar 27 13:37 ./__spark_libs__/parquet-encoding-1.7.0.jar
11928518 1256 -rwxr-xr-x   1 yarn     hadoop    1282424 Mar 27 13:37 ./__spark_libs__/ivy-2.4.0.jar
11928584  236 -rwxr-xr-x   1 yarn     hadoop     241367 Mar 27 13:37 ./__spark_libs__/commons-compress-1.4.1.jar
11929075  512 -rwxr-xr-x   1 yarn     hadoop     521157 Mar 27 13:37 ./__spark_libs__/mail-1.4.7.jar
11928737  300 -rwxr-xr-x   1 yarn     hadoop     305001 Mar 27 13:37 ./__spark_libs__/commons-httpclient-3.1.jar
11928695  292 -rwxr-xr-x   1 yarn     hadoop     298829 Mar 27 13:37 ./__spark_libs__/commons-configuration-1.6.jar
11928753 11452 -rwxr-xr-x   1 yarn     hadoop   11723537 Mar 27 13:37 ./__spark_libs__/spark-core_2.11-2.0.1.jar
11928411  144 -rwxr-xr-x   1 yarn     hadoop     144660 Mar 27 13:37 ./__spark_libs__/xbean-asm5-shaded-4.4.jar
11929130 1972 -rwxr-xr-x   1 yarn     hadoop    2015514 Mar 27 13:37 ./__spark_libs__/hadoop-yarn-api-2.7.1.jar
11928705  100 -rwxr-xr-x   1 yarn     hadoop     100680 Mar 27 13:37 ./__spark_libs__/hive-jdbc-1.2.1.spark2.jar
11928614  164 -rwxr-xr-x   1 yarn     hadoop     164368 Mar 27 13:37 ./__spark_libs__/antlr-runtime-3.4.jar
11928450 1996 -rwxr-xr-x   1 yarn     hadoop    2041628 Mar 27 13:37 ./__spark_libs__/jets3t-0.9.3.jar
11928324   92 -rwxr-xr-x   1 yarn     hadoop      93210 Mar 27 13:37 ./__spark_libs__/super-csv-2.2.0.jar
11929186 1772 -rwxr-xr-x   1 yarn     hadoop    1814309 Mar 27 13:37 ./__spark_libs__/spark-hive-thriftserver_2.11-2.0.1.jar
11928367   64 -rwxr-xr-x   1 yarn     hadoop      63777 Mar 27 13:37 ./__spark_libs__/validation-api-1.1.0.Final.jar
11928550   20 -rwxr-xr-x   1 yarn     hadoop      17008 Mar 27 13:37 ./__spark_libs__/base64-2.3.8.jar
11928629 1248 -rwxr-xr-x   1 yarn     hadoop    1277883 Mar 27 13:37 ./__spark_libs__/mesos-0.21.1-shaded-protobuf.jar
11928392   40 -rwxr-xr-x   1 yarn     hadoop      40341 Mar 27 13:37 ./__spark_libs__/json4s-jackson_2.11-3.2.11.jar
11928587  256 -rwxr-xr-x   1 yarn     hadoop     258370 Mar 27 13:37 ./__spark_libs__/calcite-avatica-1.2.0-incubating.jar
11928603   32 -rwxr-xr-x   1 yarn     hadoop      29540 Mar 27 13:37 ./__spark_libs__/spark-sketch_2.11-2.0.1.jar
11928527 1592 -rwxr-xr-x   1 yarn     hadoop    1627065 Mar 27 13:37 ./__spark_libs__/jersey-bundle-1.19.1.jar
11928405  176 -rwxr-xr-x   1 yarn     hadoop     177832 Mar 27 13:37 ./__spark_libs__/spark-mllib-local_2.11-2.0.1.jar
11928763 6144 -rwxr-xr-x   1 yarn     hadoop    6290315 Mar 27 13:37 ./__spark_libs__/spark-sql_2.11-2.0.1.jar
11928438   64 -rwxr-xr-x   1 yarn     hadoop      63316 Mar 27 13:37 ./__spark_libs__/spark-repl_2.11-2.0.1.jar
11928582 5612 -rwxr-xr-x   1 yarn     hadoop    5744974 Mar 27 13:37 ./__spark_libs__/scala-library-2.11.8.jar
11928808  696 -rwxr-xr-x   1 yarn     hadoop     710492 Mar 27 13:37 ./__spark_libs__/guice-3.0.jar
11929189   28 -rwxr-xr-x   1 yarn     hadoop      26514 Mar 27 13:37 ./__spark_libs__/stax-api-1.0.1.jar
11928551    8 -rwxr-xr-x   1 yarn     hadoop       4467 Mar 27 13:37 ./__spark_libs__/aopalliance-1.0.jar
11928591 6716 -rwxr-xr-x   1 yarn     hadoop    6873892 Mar 27 13:37 ./__spark_libs__/spark-catalyst_2.11-2.0.1.jar
11929024  896 -rwxr-xr-x   1 yarn     hadoop     917052 Mar 27 13:37 ./__spark_libs__/parquet-column-1.7.0.jar
11928390   20 -rwxr-xr-x   1 yarn     hadoop      16430 Mar 27 13:37 ./__spark_libs__/jcl-over-slf4j-1.7.16.jar
11928534   80 -rwxr-xr-x   1 yarn     hadoop      79845 Mar 27 13:37 ./__spark_libs__/compress-lzf-1.0.3.jar
11928608  436 -rwxr-xr-x   1 yarn     hadoop     445288 Mar 27 13:37 ./__spark_libs__/antlr-2.7.7.jar
11928751   28 -rwxr-xr-x   1 yarn     hadoop      26366 Mar 27 13:37 ./__spark_libs__/javax.annotation-api-1.2.jar
11928726  676 -rwxr-xr-x   1 yarn     hadoop     691479 Mar 27 13:37 ./__spark_libs__/apacheds-kerberos-codec-2.0.0-M15.jar
11928689 1616 -rwxr-xr-x   1 yarn     hadoop    1654097 Mar 27 13:37 ./__spark_libs__/hadoop-yarn-common-2.7.1.jar
11928666  748 -rwxr-xr-x   1 yarn     hadoop     764569 Mar 27 13:37 ./__spark_libs__/jtransforms-2.4.0.jar
11928490   12 -rwxr-xr-x   1 yarn     hadoop       9939 Mar 27 13:37 ./__spark_libs__/slf4j-log4j12-1.7.16.jar
11928747    8 -rwxr-xr-x   1 yarn     hadoop       5711 Mar 27 13:37 ./__spark_libs__/minlog-1.3.0.jar
11928677   48 -rwxr-xr-x   1 yarn     hadoop      48720 Mar 27 13:37 ./__spark_libs__/snappy-0.2.jar
11928353  636 -rwxr-xr-x   1 yarn     hadoop     648678 Mar 27 13:37 ./__spark_libs__/scala-xml_2.11-1.0.2.jar
11928781 1768 -rwxr-xr-x   1 yarn     hadoop    1809447 Mar 27 13:37 ./__spark_libs__/datanucleus-rdbms-3.2.9.jar
11928733 5712 -rwxr-xr-x   1 yarn     hadoop    5847591 Mar 27 13:37 ./__spark_libs__/spark-mllib_2.11-2.0.1.jar
11928426  532 -rwxr-xr-x   1 yarn     hadoop     540852 Mar 27 13:37 ./__spark_libs__/mysql-connector-java-5.0.8-bin.jar
11928495   40 -rwxr-xr-x   1 yarn     hadoop      40509 Mar 27 13:37 ./__spark_libs__/slf4j-api-1.7.16.jar
11929076   64 -rwxr-xr-x   1 yarn     hadoop      62050 Mar 27 13:37 ./__spark_libs__/commons-logging-1.1.3.jar
11929135  212 -rwxr-xr-x   1 yarn     hadoop     213911 Mar 27 13:37 ./__spark_libs__/jline-2.12.1.jar
11928462  788 -rwxr-xr-x   1 yarn     hadoop     802818 Mar 27 13:37 ./__spark_libs__/scalap-2.11.8.jar
11928504 13136 -rwxr-xr-x   1 yarn     hadoop   13448966 Mar 27 13:37 ./__spark_libs__/breeze_2.11-0.11.2.jar
11929139  504 -rwxr-xr-x   1 yarn     hadoop     515604 Mar 27 13:37 ./__spark_libs__/jackson-module-scala_2.11-2.6.5.jar
11928803   12 -rwxr-xr-x   1 yarn     hadoop      10023 Mar 27 13:37 ./__spark_libs__/java-xmlbuilder-1.0.jar
11929133  440 -rwxr-xr-x   1 yarn     hadoop     448794 Mar 27 13:37 ./__spark_libs__/apache-log4j-extras-1.2.17.jar
11928341   48 -rwxr-xr-x   1 yarn     hadoop      46983 Mar 27 13:37 ./__spark_libs__/jackson-annotations-2.6.5.jar
11928333   44 -rwxr-xr-x   1 yarn     hadoop      44925 Mar 27 13:37 ./__spark_libs__/apacheds-i18n-2.0.0-M15.jar
11928300  112 -rwxr-xr-x   1 yarn     hadoop     112558 Mar 27 13:37 ./__spark_libs__/metrics-core-3.1.2.jar
11928544  720 -rwxr-xr-x   1 yarn     hadoop     736658 Mar 27 13:37 ./__spark_libs__/httpclient-4.5.2.jar
11929114   20 -rwxr-xr-x   1 yarn     hadoop      18336 Mar 27 13:37 ./__spark_libs__/jackson-jaxrs-1.9.13.jar
11928770  352 -rwxr-xr-x   1 yarn     hadoop     358390 Mar 27 13:37 ./__spark_libs__/kryo-shaded-3.0.3.jar
11928387  428 -rwxr-xr-x   1 yarn     hadoop     436303 Mar 27 13:37 ./__spark_libs__/avro-1.7.7.jar
11928507   68 -rwxr-xr-x   1 yarn     hadoop      65653 Mar 27 13:37 ./__spark_libs__/spark-launcher_2.11-2.0.1.jar
11928706   12 -rwxr-xr-x   1 yarn     hadoop      12131 Mar 27 13:37 ./__spark_libs__/jpam-1.1.jar
11929207  308 -rwxr-xr-x   1 yarn     hadoop     313686 Mar 27 13:37 ./__spark_libs__/libfb303-0.9.2.jar
11928384 15128 -rwxr-xr-x   1 yarn     hadoop   15487351 Mar 27 13:37 ./__spark_libs__/scala-compiler-2.11.8.jar
11928819 1032 -rwxr-xr-x   1 yarn     hadoop    1056168 Mar 27 13:37 ./__spark_libs__/snappy-java-1.1.2.6.jar
11928660   44 -rwxr-xr-x   1 yarn     hadoop      41070 Mar 27 13:37 ./__spark_libs__/spark-unsafe_2.11-2.0.1.jar
11928710   72 -rwxr-xr-x   1 yarn     hadoop      70688 Mar 27 13:37 ./__spark_libs__/hadoop-auth-2.7.1.jar
11928778 3152 -rwxr-xr-x   1 yarn     hadoop    3224708 Mar 27 13:37 ./__spark_libs__/derby-10.12.1.1.jar
11928510   32 -rwxr-xr-x   1 yarn     hadoop      32145 Mar 27 13:37 ./__spark_libs__/hadoop-yarn-server-web-proxy-2.7.1.jar
11928641  332 -rwxr-xr-x   1 yarn     hadoop     339666 Mar 27 13:37 ./__spark_libs__/datanucleus-api-jdo-3.2.6.jar
11928362 1444 -rwxr-xr-x   1 yarn     hadoop    1475955 Mar 27 13:37 ./__spark_libs__/htrace-core-3.1.0-incubating.jar
11929145   40 -rwxr-xr-x   1 yarn     hadoop      38134 Mar 27 13:37 ./__spark_libs__/hadoop-mapreduce-client-jobclient-2.7.1.jar
11928304 1512 -rwxr-xr-x   1 yarn     hadoop    1544875 Mar 27 13:37 ./__spark_libs__/hadoop-mapreduce-client-core-2.7.1.jar
11929199  296 -rwxr-xr-x   1 yarn     hadoop     302248 Mar 27 13:37 ./__spark_libs__/antlr4-runtime-4.5.3.jar
11928429   20 -rwxr-xr-x   1 yarn     hadoop      16993 Mar 27 13:37 ./__spark_libs__/JavaEWAH-0.3.2.jar
11928760  208 -rwxr-xr-x   1 yarn     hadoop     212453 Mar 27 13:37 ./__spark_libs__/commons-net-2.2.jar
14286856    4 -rw-r--r--   1 yarn     hadoop        600 May 16 11:10 ./.launch_container.sh.crc
14286857    4 -rwx------   1 yarn     hadoop        676 May 16 11:10 ./default_container_executor_session.sh
14286859    4 -rwx------   1 yarn     hadoop        730 May 16 11:10 ./default_container_executor.sh
14286851    4 drwx--x---   2 yarn     hadoop       4096 May 16 11:10 ./tmp
broken symlinks(find -L . -maxdepth 5 -type l -ls):
End of LogType:directory.info

LogType:launch_container.sh
Log Upload Time:Tue May 16 11:12:18 +0700 2017
LogLength:75554
Log Contents:
#!/bin/bash

export SPARK_YARN_STAGING_DIR="hdfs://c408.hadoop.gda.lo:8020/user/fairy/.sparkStaging/application_1494395298335_303847"
export LOCAL_DIRS="/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847"
export HADOOP_CONF_DIR="/usr/hdp/current/hadoop-client/conf"
export NM_HTTP_PORT="8042"
export JAVA_HOME="/usr/jdk64/jdk1.8.0_92/"
export LOG_DIRS="/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000006"
export NM_AUX_SERVICE_mapreduce_shuffle="AAA0+gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=
"
export NM_PORT="45454"
export USER="fairy"
export HADOOP_YARN_HOME="/usr/hdp/current/hadoop-yarn-nodemanager"
export CLASSPATH="$PWD:$PWD/__spark_conf__:$PWD/__spark_libs__/*:$HADOOP_CONF_DIR:/usr/hdp/current/hadoop-client/*:/usr/hdp/current/hadoop-client/lib/*:/usr/hdp/current/hadoop-hdfs-client/*:/usr/hdp/current/hadoop-hdfs-client/lib/*:/usr/hdp/current/hadoop-yarn-client/*:/usr/hdp/current/hadoop-yarn-client/lib/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.4.2.0-258/hadoop/lib/hadoop-lzo-0.6.0.2.4.2.0-258.jar:/etc/hadoop/conf/secure"
export SPARK_LOG_URL_STDOUT="http://c427.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_01_000006/fairy/stdout?start=-4096"
export NM_HOST="c427.hadoop.gda.lo"
export SPARK_YARN_MODE="true"
export HADOOP_TOKEN_FILE_LOCATION="/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_01_000006/container_tokens"
export NM_AUX_SERVICE_spark_shuffle=""
export SPARK_USER="fairy"
export SPARK_LOG_URL_STDERR="http://c427.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_01_000006/fairy/stderr?start=-4096"
export LOGNAME="fairy"
export JVM_PID="$$"
export PWD="/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_01_000006"
export HOME="/home/"
export CONTAINER_ID="container_e81_1494395298335_303847_01_000006"
export MALLOC_ARENA_MAX="4"
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3310/paranamer-2.3.jar" "__spark_libs__/paranamer-2.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3355/avro-ipc-1.7.7.jar" "__spark_libs__/avro-ipc-1.7.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3296/spark-streaming_2.11-2.0.1.jar" "__spark_libs__/spark-streaming_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3445/spark-graphx_2.11-2.0.1.jar" "__spark_libs__/spark-graphx_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3449/hadoop-common-2.7.1.jar" "__spark_libs__/hadoop-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3284/hive-cli-1.2.1.spark2.jar" "__spark_libs__/hive-cli-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3299/jcl-over-slf4j-1.7.16.jar" "__spark_libs__/jcl-over-slf4j-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3384/parquet-encoding-1.7.0.jar" "__spark_libs__/parquet-encoding-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3306/xbean-asm5-shaded-4.4.jar" "__spark_libs__/xbean-asm5-shaded-4.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3468/commons-cli-1.2.jar" "__spark_libs__/commons-cli-1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3353/jetty-6.1.26.jar" "__spark_libs__/jetty-6.1.26.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3282/spark-network-shuffle_2.11-2.0.1.jar" "__spark_libs__/spark-network-shuffle_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3316/jersey-server-2.22.2.jar" "__spark_libs__/jersey-server-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3276/zookeeper-3.4.6.jar" "__spark_libs__/zookeeper-3.4.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3317/curator-recipes-2.6.0.jar" "__spark_libs__/curator-recipes-2.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3322/hk2-api-2.4.0-b34.jar" "__spark_libs__/hk2-api-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3450/jackson-module-scala_2.11-2.6.5.jar" "__spark_libs__/jackson-module-scala_2.11-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3415/commons-net-2.2.jar" "__spark_libs__/commons-net-2.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3413/spark-core_2.11-2.0.1.jar" "__spark_libs__/spark-core_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3349/leveldbjni-all-1.8.jar" "__spark_libs__/leveldbjni-all-1.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3435/metrics-graphite-3.1.2.jar" "__spark_libs__/metrics-graphite-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3375/hadoop-annotations-2.7.1.jar" "__spark_libs__/hadoop-annotations-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3309/parquet-hadoop-bundle-1.6.0.jar" "__spark_libs__/parquet-hadoop-bundle-1.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3438/mail-1.4.7.jar" "__spark_libs__/mail-1.4.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3389/jersey-guava-2.22.2.jar" "__spark_libs__/jersey-guava-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3442/jackson-jaxrs-1.9.13.jar" "__spark_libs__/jackson-jaxrs-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3383/scala-parser-combinators_2.11-1.0.4.jar" "__spark_libs__/scala-parser-combinators_2.11-1.0.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3463/hk2-utils-2.4.0-b34.jar" "__spark_libs__/hk2-utils-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3318/jackson-module-paranamer-2.6.5.jar" "__spark_libs__/jackson-module-paranamer-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3274/avro-mapred-1.7.7-hadoop2.jar" "__spark_libs__/avro-mapred-1.7.7-hadoop2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3428/guice-3.0.jar" "__spark_libs__/guice-3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3302/commons-beanutils-1.7.0.jar" "__spark_libs__/commons-beanutils-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3441/xz-1.0.jar" "__spark_libs__/xz-1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3408/commons-httpclient-3.1.jar" "__spark_libs__/commons-httpclient-3.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3272/libthrift-0.9.2.jar" "__spark_libs__/libthrift-0.9.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3290/htrace-core-3.1.0-incubating.jar" "__spark_libs__/htrace-core-3.1.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3292/spark-tags_2.11-2.0.1.jar" "__spark_libs__/spark-tags_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3376/datanucleus-api-jdo-3.2.6.jar" "__spark_libs__/datanucleus-api-jdo-3.2.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3478/pmml-schema-1.2.15.jar" "__spark_libs__/pmml-schema-1.2.15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3434/osgi-resource-locator-1.0.1.jar" "__spark_libs__/osgi-resource-locator-1.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3347/compress-lzf-1.0.3.jar" "__spark_libs__/compress-lzf-1.0.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3437/commons-logging-1.1.3.jar" "__spark_libs__/commons-logging-1.1.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3350/hive-exec-1.2.1.spark2.jar" "__spark_libs__/hive-exec-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3280/apacheds-i18n-2.0.0-M15.jar" "__spark_libs__/apacheds-i18n-2.0.0-M15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3403/commons-collections-3.2.2.jar" "__spark_libs__/commons-collections-3.2.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3351/aopalliance-1.0.jar" "__spark_libs__/aopalliance-1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3366/antlr-2.7.7.jar" "__spark_libs__/antlr-2.7.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3335/spark-yarn_2.11-2.0.1.jar" "__spark_libs__/spark-yarn_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3287/scala-xml_2.11-1.0.2.jar" "__spark_libs__/scala-xml_2.11-1.0.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3314/jackson-core-2.6.5.jar" "__spark_libs__/jackson-core-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3361/scala-reflect-2.11.8.jar" "__spark_libs__/scala-reflect-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3447/hadoop-yarn-api-2.7.1.jar" "__spark_libs__/hadoop-yarn-api-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3414/xercesImpl-2.9.1.jar" "__spark_libs__/xercesImpl-2.9.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3400/commons-lang3-3.3.2.jar" "__spark_libs__/commons-lang3-3.3.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3432/snappy-java-1.1.2.6.jar" "__spark_libs__/snappy-java-1.1.2.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3390/stax-api-1.0-2.jar" "__spark_libs__/stax-api-1.0-2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3452/hadoop-mapreduce-client-jobclient-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-jobclient-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3278/calcite-core-1.2.0-incubating.jar" "__spark_libs__/calcite-core-1.2.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3368/commons-io-2.4.jar" "__spark_libs__/commons-io-2.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3294/validation-api-1.1.0.Final.jar" "__spark_libs__/validation-api-1.1.0.Final.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3300/json4s-jackson_2.11-3.2.11.jar" "__spark_libs__/json4s-jackson_2.11-3.2.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3382/spark-unsafe_2.11-2.0.1.jar" "__spark_libs__/spark-unsafe_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3325/commons-digester-1.8.jar" "__spark_libs__/commons-digester-1.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3321/univocity-parsers-2.1.1.jar" "__spark_libs__/univocity-parsers-2.1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3373/netty-3.8.0.Final.jar" "__spark_libs__/netty-3.8.0.Final.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3409/jdo-api-3.0.1.jar" "__spark_libs__/jdo-api-3.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3464/spark-hive_2.11-2.0.1.jar" "__spark_libs__/spark-hive_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3320/commons-dbcp-1.4.jar" "__spark_libs__/commons-dbcp-1.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3338/spark-launcher_2.11-2.0.1.jar" "__spark_libs__/spark-launcher_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3356/httpclient-4.5.2.jar" "__spark_libs__/httpclient-4.5.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3270/metrics-core-3.1.2.jar" "__spark_libs__/metrics-core-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3308/metrics-jvm-3.1.2.jar" "__spark_libs__/metrics-jvm-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3455/jackson-databind-2.6.5.jar" "__spark_libs__/jackson-databind-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3472/libfb303-0.9.2.jar" "__spark_libs__/libfb303-0.9.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3433/httpcore-4.4.4.jar" "__spark_libs__/httpcore-4.4.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3330/pyrolite-4.9.jar" "__spark_libs__/pyrolite-4.9.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3475/jersey-client-2.22.2.jar" "__spark_libs__/jersey-client-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3365/protobuf-java-2.5.0.jar" "__spark_libs__/protobuf-java-2.5.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3405/apacheds-kerberos-codec-2.0.0-M15.jar" "__spark_libs__/apacheds-kerberos-codec-2.0.0-M15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3323/scalap-2.11.8.jar" "__spark_libs__/scalap-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3429/oro-2.0.8.jar" "__spark_libs__/oro-2.0.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3391/javax.ws.rs-api-2.0.1.jar" "__spark_libs__/javax.ws.rs-api-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3295/hadoop-hdfs-2.7.1.jar" "__spark_libs__/hadoop-hdfs-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/usercache/fairy/filecache/11650/__spark_conf__.zip" "__spark_conf__"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3370/bcprov-jdk15on-1.51.jar" "__spark_libs__/bcprov-jdk15on-1.51.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3327/jersey-container-servlet-2.22.2.jar" "__spark_libs__/jersey-container-servlet-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3466/spark-hive-thriftserver_2.11-2.0.1.jar" "__spark_libs__/spark-hive-thriftserver_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3470/stream-2.7.0.jar" "__spark_libs__/stream-2.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3431/eigenbase-properties-1.1.5.jar" "__spark_libs__/eigenbase-properties-1.1.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3369/jaxb-api-2.2.2.jar" "__spark_libs__/jaxb-api-2.2.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3371/spire-macros_2.11-0.7.4.jar" "__spark_libs__/spire-macros_2.11-0.7.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3291/commons-pool-1.5.4.jar" "__spark_libs__/commons-pool-1.5.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3392/hadoop-yarn-common-2.7.1.jar" "__spark_libs__/hadoop-yarn-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3454/guava-14.0.1.jar" "__spark_libs__/guava-14.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3304/spark-mllib-local_2.11-2.0.1.jar" "__spark_libs__/spark-mllib-local_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3426/java-xmlbuilder-1.0.jar" "__spark_libs__/java-xmlbuilder-1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3456/activation-1.1.1.jar" "__spark_libs__/activation-1.1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3476/hadoop-mapreduce-client-app-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-app-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3359/commons-compress-1.4.1.jar" "__spark_libs__/commons-compress-1.4.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3467/stax-api-1.0.1.jar" "__spark_libs__/stax-api-1.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3381/hive-metastore-1.2.1.spark2.jar" "__spark_libs__/hive-metastore-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3394/commons-configuration-1.6.jar" "__spark_libs__/commons-configuration-1.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3345/javax.inject-1.jar" "__spark_libs__/javax.inject-1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3305/jersey-container-servlet-core-2.22.2.jar" "__spark_libs__/jersey-container-servlet-core-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3402/hadoop-yarn-server-common-2.7.1.jar" "__spark_libs__/hadoop-yarn-server-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3364/spark-sketch_2.11-2.0.1.jar" "__spark_libs__/spark-sketch_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3357/calcite-linq4j-1.2.0-incubating.jar" "__spark_libs__/calcite-linq4j-1.2.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3333/commons-codec-1.10.jar" "__spark_libs__/commons-codec-1.10.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3406/lz4-1.3.0.jar" "__spark_libs__/lz4-1.3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3421/datanucleus-rdbms-3.2.9.jar" "__spark_libs__/datanucleus-rdbms-3.2.9.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3395/log4j-1.2.17.jar" "__spark_libs__/log4j-1.2.17.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3474/netty-all-4.0.29.Final.jar" "__spark_libs__/netty-all-4.0.29.Final.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3459/jta-1.1.jar" "__spark_libs__/jta-1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3397/jpam-1.1.jar" "__spark_libs__/jpam-1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3279/guice-servlet-3.0.jar" "__spark_libs__/guice-servlet-3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3443/jackson-xc-1.9.13.jar" "__spark_libs__/jackson-xc-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3386/jersey-common-2.22.2.jar" "__spark_libs__/jersey-common-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3301/pmml-model-1.2.15.jar" "__spark_libs__/pmml-model-1.2.15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3430/javassist-3.18.1-GA.jar" "__spark_libs__/javassist-3.18.1-GA.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3453/apache-log4j-extras-1.2.17.jar" "__spark_libs__/apache-log4j-extras-1.2.17.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3420/derby-10.12.1.1.jar" "__spark_libs__/derby-10.12.1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3424/xmlenc-0.52.jar" "__spark_libs__/xmlenc-0.52.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3352/base64-2.3.8.jar" "__spark_libs__/base64-2.3.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3458/hadoop-yarn-client-2.7.1.jar" "__spark_libs__/hadoop-yarn-client-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3339/hadoop-yarn-server-web-proxy-2.7.1.jar" "__spark_libs__/hadoop-yarn-server-web-proxy-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3363/jersey-media-jaxb-2.22.2.jar" "__spark_libs__/jersey-media-jaxb-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3461/RoaringBitmap-0.5.11.jar" "__spark_libs__/RoaringBitmap-0.5.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/usercache/fairy/filecache/11649/stats-etlr-1.0.jar" "__app__.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3410/jul-to-slf4j-1.7.16.jar" "__spark_libs__/jul-to-slf4j-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3297/scala-compiler-2.11.8.jar" "__spark_libs__/scala-compiler-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3372/mesos-0.21.1-shaded-protobuf.jar" "__spark_libs__/mesos-0.21.1-shaded-protobuf.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3286/hadoop-client-2.7.1.jar" "__spark_libs__/hadoop-client-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3293/objenesis-2.1.jar" "__spark_libs__/objenesis-2.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3313/gson-2.2.4.jar" "__spark_libs__/gson-2.2.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3341/ivy-2.4.0.jar" "__spark_libs__/ivy-2.4.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3362/spark-catalyst_2.11-2.0.1.jar" "__spark_libs__/spark-catalyst_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3471/antlr4-runtime-4.5.3.jar" "__spark_libs__/antlr4-runtime-4.5.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3379/stringtemplate-3.2.1.jar" "__spark_libs__/stringtemplate-3.2.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3273/commons-beanutils-core-1.8.0.jar" "__spark_libs__/commons-beanutils-core-1.8.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3388/snappy-0.2.jar" "__spark_libs__/snappy-0.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3380/jsp-api-2.1.jar" "__spark_libs__/jsp-api-2.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3275/jetty-util-6.1.26.jar" "__spark_libs__/jetty-util-6.1.26.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3281/metrics-json-3.1.2.jar" "__spark_libs__/metrics-json-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3328/janino-2.7.8.jar" "__spark_libs__/janino-2.7.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3425/commons-compiler-2.7.6.jar" "__spark_libs__/commons-compiler-2.7.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3277/super-csv-2.2.0.jar" "__spark_libs__/super-csv-2.2.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3360/calcite-avatica-1.2.0-incubating.jar" "__spark_libs__/calcite-avatica-1.2.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3465/mx4j-3.0.2.jar" "__spark_libs__/mx4j-3.0.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3337/breeze_2.11-0.11.2.jar" "__spark_libs__/breeze_2.11-0.11.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3307/commons-math3-3.4.1.jar" "__spark_libs__/commons-math3-3.4.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3336/jackson-mapper-asl-1.9.13.jar" "__spark_libs__/jackson-mapper-asl-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3473/spire_2.11-0.7.4.jar" "__spark_libs__/spire_2.11-0.7.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3398/hive-jdbc-1.2.1.spark2.jar" "__spark_libs__/hive-jdbc-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3457/jodd-core-3.5.2.jar" "__spark_libs__/jodd-core-3.5.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3407/spark-mllib_2.11-2.0.1.jar" "__spark_libs__/spark-mllib_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3319/jets3t-0.9.3.jar" "__spark_libs__/jets3t-0.9.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3439/parquet-column-1.7.0.jar" "__spark_libs__/parquet-column-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3346/parquet-hadoop-1.7.0.jar" "__spark_libs__/parquet-hadoop-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3374/curator-client-2.6.0.jar" "__spark_libs__/curator-client-2.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3436/curator-framework-2.6.0.jar" "__spark_libs__/curator-framework-2.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3427/kryo-shaded-3.0.3.jar" "__spark_libs__/kryo-shaded-3.0.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3332/datanucleus-core-3.2.10.jar" "__spark_libs__/datanucleus-core-3.2.10.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3451/arpack_combined_all-0.1.jar" "__spark_libs__/arpack_combined_all-0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3298/avro-1.7.7.jar" "__spark_libs__/avro-1.7.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3331/core-1.1.2.jar" "__spark_libs__/core-1.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3271/hadoop-mapreduce-client-core-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-core-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3385/jtransforms-2.4.0.jar" "__spark_libs__/jtransforms-2.4.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3411/minlog-1.3.0.jar" "__spark_libs__/minlog-1.3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3324/jackson-core-asl-1.9.13.jar" "__spark_libs__/jackson-core-asl-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3404/chill_2.11-0.8.0.jar" "__spark_libs__/chill_2.11-0.8.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3387/api-asn1-api-1.0.0-M20.jar" "__spark_libs__/api-asn1-api-1.0.0-M20.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3469/parquet-generator-1.7.0.jar" "__spark_libs__/parquet-generator-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3446/json4s-ast_2.11-3.2.11.jar" "__spark_libs__/json4s-ast_2.11-3.2.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3412/javax.annotation-api-1.2.jar" "__spark_libs__/javax.annotation-api-1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3418/hive-beeline-1.2.1.spark2.jar" "__spark_libs__/hive-beeline-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3285/json-20090211.jar" "__spark_libs__/json-20090211.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3326/opencsv-2.3.jar" "__spark_libs__/opencsv-2.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3448/jline-2.12.1.jar" "__spark_libs__/jline-2.12.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3477/hadoop-mapreduce-client-common-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3303/json4s-core_2.11-3.2.11.jar" "__spark_libs__/json4s-core_2.11-3.2.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3315/spark-repl_2.11-2.0.1.jar" "__spark_libs__/spark-repl_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3358/scala-library-2.11.8.jar" "__spark_libs__/scala-library-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3416/spark-sql_2.11-2.0.1.jar" "__spark_libs__/spark-sql_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3311/mysql-connector-java-5.0.8-bin.jar" "__spark_libs__/mysql-connector-java-5.0.8-bin.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3422/joda-time-2.9.3.jar" "__spark_libs__/joda-time-2.9.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3288/javax.servlet-api-3.1.0.jar" "__spark_libs__/javax.servlet-api-3.1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3354/hk2-locator-2.4.0-b34.jar" "__spark_libs__/hk2-locator-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3423/jsr305-1.3.9.jar" "__spark_libs__/jsr305-1.3.9.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3419/spark-network-common_2.11-2.0.1.jar" "__spark_libs__/spark-network-common_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3460/javolution-5.5.1.jar" "__spark_libs__/javolution-5.5.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3440/parquet-jackson-1.7.0.jar" "__spark_libs__/parquet-jackson-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3329/breeze-macros_2.11-0.11.2.jar" "__spark_libs__/breeze-macros_2.11-0.11.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3399/hadoop-auth-2.7.1.jar" "__spark_libs__/hadoop-auth-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3334/slf4j-api-1.7.16.jar" "__spark_libs__/slf4j-api-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3462/lift-json_2.11-2.6.3.jar" "__spark_libs__/lift-json_2.11-2.6.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3340/slf4j-log4j12-1.7.16.jar" "__spark_libs__/slf4j-log4j12-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3396/parquet-format-2.3.0-incubating.jar" "__spark_libs__/parquet-format-2.3.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3289/javax.inject-2.4.0-b34.jar" "__spark_libs__/javax.inject-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3393/py4j-0.10.3.jar" "__spark_libs__/py4j-0.10.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3283/jackson-annotations-2.6.5.jar" "__spark_libs__/jackson-annotations-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3444/hadoop-mapreduce-client-shuffle-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-shuffle-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3343/chill-java-0.8.0.jar" "__spark_libs__/chill-java-0.8.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3378/commons-lang-2.6.jar" "__spark_libs__/commons-lang-2.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3342/bonecp-0.8.0.RELEASE.jar" "__spark_libs__/bonecp-0.8.0.RELEASE.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3312/JavaEWAH-0.3.2.jar" "__spark_libs__/JavaEWAH-0.3.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3344/jersey-bundle-1.19.1.jar" "__spark_libs__/jersey-bundle-1.19.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3377/aopalliance-repackaged-2.4.0-b34.jar" "__spark_libs__/aopalliance-repackaged-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3348/ST4-4.0.4.jar" "__spark_libs__/ST4-4.0.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3417/api-util-1.0.0-M20.jar" "__spark_libs__/api-util-1.0.0-M20.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3401/parquet-common-1.7.0.jar" "__spark_libs__/parquet-common-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3367/antlr-runtime-3.4.jar" "__spark_libs__/antlr-runtime-3.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
# Creating copy of launch script
cp "launch_container.sh" "/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000006/launch_container.sh"
chmod 640 "/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000006/launch_container.sh"
# Determining directory contents
echo "ls -l:" 1>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000006/directory.info"
ls -l 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000006/directory.info"
echo "find -L . -maxdepth 5 -ls:" 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000006/directory.info"
find -L . -maxdepth 5 -ls 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000006/directory.info"
echo "broken symlinks(find -L . -maxdepth 5 -type l -ls):" 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000006/directory.info"
find -L . -maxdepth 5 -type l -ls 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000006/directory.info"
exec /bin/bash -c "$JAVA_HOME/bin/java -server -Xmx3072m '-Dhdp.version=2.4.2.0-258' -Djava.io.tmpdir=$PWD/tmp '-Dspark.history.ui.port=18080' '-Dspark.ui.port=0' '-Dspark.driver.port=19210' -Dspark.yarn.app.container.log.dir=/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000006 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.60.43.33:19210 --executor-id 5 --hostname c427.hadoop.gda.lo --cores 1 --app-id application_1494395298335_303847 --user-class-path file:$PWD/__app__.jar 1> /hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000006/stdout 2> /hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000006/stderr"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
End of LogType:launch_container.sh

LogType:stderr
Log Upload Time:Tue May 16 11:12:18 +0700 2017
LogLength:1496
Log Contents:
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/hadoop/yarn/local/filecache/3340/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/hdp/2.4.2.0-258/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 922@c427.hadoop.gda.lo
17/05/16 11:10:53 INFO util.SignalUtils: Registered signal handler for TERM
17/05/16 11:10:53 INFO util.SignalUtils: Registered signal handler for HUP
17/05/16 11:10:53 INFO util.SignalUtils: Registered signal handler for INT
17/05/16 11:10:54 INFO spark.SecurityManager: Changing view acls to: yarn,fairy
17/05/16 11:10:54 INFO spark.SecurityManager: Changing modify acls to: yarn,fairy
17/05/16 11:10:54 INFO spark.SecurityManager: Changing view acls groups to: 
17/05/16 11:10:54 INFO spark.SecurityManager: Changing modify acls groups to: 
17/05/16 11:10:54 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(yarn, fairy); groups with view permissions: Set(); users  with modify permissions: Set(yarn, fairy); groups with modify permissions: Set()
17/05/16 11:10:55 ERROR executor.CoarseGrainedExecutorBackend: RECEIVED SIGNAL TERM
End of LogType:stderr

LogType:stdout
Log Upload Time:Tue May 16 11:12:18 +0700 2017
LogLength:0
Log Contents:
End of LogType:stdout



Container: container_e81_1494395298335_303847_01_000002 on c432.hadoop.gda.lo_45454
=====================================================================================
LogType:directory.info
Log Upload Time:Tue May 16 11:12:19 +0700 2017
LogLength:28883
Log Contents:
ls -l:
total 112
lrwxrwxrwx 1 yarn hadoop    69 May 16 11:10 __app__.jar -> /hadoop/yarn/local/usercache/fairy/filecache/11434/stats-etlr-1.0.jar
-rw-r--r-- 1 yarn hadoop    87 May 16 11:10 container_tokens
-rwx------ 1 yarn hadoop   676 May 16 11:10 default_container_executor_session.sh
-rwx------ 1 yarn hadoop   730 May 16 11:10 default_container_executor.sh
-rwx------ 1 yarn hadoop 75554 May 16 11:10 launch_container.sh
lrwxrwxrwx 1 yarn hadoop    69 May 16 11:10 __spark_conf__ -> /hadoop/yarn/local/usercache/fairy/filecache/11435/__spark_conf__.zip
drwxr-xr-x 2 yarn hadoop 12288 May 16 11:10 __spark_libs__
drwx--x--- 2 yarn hadoop  4096 May 16 11:10 tmp
find -L . -maxdepth 5 -ls:
12451842    4 drwx--x---   4 yarn     hadoop       4096 May 16 11:10 .
23463040    4 drwx------   2 yarn     hadoop       4096 May 16 11:10 ./__spark_conf__
23463082   36 -r-x------   1 yarn     hadoop      34061 May 16 11:10 ./__spark_conf__/__spark_conf__.properties
23463054    4 -r-x------   1 yarn     hadoop        758 May 16 11:10 ./__spark_conf__/mapred-site.xml.template
23463048    8 -r-x------   1 yarn     hadoop       4197 May 16 11:10 ./__spark_conf__/core-site.xml
23463081    4 -r-x------   1 yarn     hadoop        945 May 16 11:10 ./__spark_conf__/taskcontroller.cfg
23463052    8 -r-x------   1 yarn     hadoop       5404 May 16 11:10 ./__spark_conf__/capacity-scheduler.xml
23463068    4 -r-x------   1 yarn     hadoop        229 May 16 11:10 ./__spark_conf__/slaves
23463058    8 -r-x------   1 yarn     hadoop       6572 May 16 11:10 ./__spark_conf__/hdfs-site.xml
23463065    4 -r-x------   1 yarn     hadoop       2316 May 16 11:10 ./__spark_conf__/ssl-client.xml.example
23463055    4 -r-x------   1 yarn     hadoop       2358 May 16 11:10 ./__spark_conf__/topology_script.py
23463064    4 -r-x------   1 yarn     hadoop       1602 May 16 11:10 ./__spark_conf__/health_check
23463061    4 -r-x------   1 yarn     hadoop       2490 May 16 11:10 ./__spark_conf__/hadoop-metrics.properties
23463057    8 -r-x------   1 yarn     hadoop       5006 May 16 11:10 ./__spark_conf__/yarn-env.sh
23463044    4 -r-x------   1 yarn     hadoop       2302 May 16 11:10 ./__spark_conf__/hadoop-metrics2.properties
23463051    4 -r-x------   1 yarn     hadoop        884 May 16 11:10 ./__spark_conf__/ssl-client.xml
23463079    8 -r-x------   1 yarn     hadoop       4113 May 16 11:10 ./__spark_conf__/mapred-queues.xml.template
23463043   12 -r-x------   1 yarn     hadoop       8699 May 16 11:10 ./__spark_conf__/log4j.properties
23463056    4 -r-x------   1 yarn     hadoop       1335 May 16 11:10 ./__spark_conf__/configuration.xsl
23463060    4 -r-x------   1 yarn     hadoop       1020 May 16 11:10 ./__spark_conf__/commons-logging.properties
23463063    4 -r-x------   1 yarn     hadoop        690 May 16 11:10 ./__spark_conf__/mapred-env.sh
23463077    4 -r-x------   1 yarn     hadoop       1000 May 16 11:10 ./__spark_conf__/ssl-server.xml
23463066    4 -r-x------   1 yarn     hadoop       1527 May 16 11:10 ./__spark_conf__/kms-env.sh
23463059    4 -r-x------   1 yarn     hadoop       1019 May 16 11:10 ./__spark_conf__/container-executor.cfg
23463080    8 -r-x------   1 yarn     hadoop       5511 May 16 11:10 ./__spark_conf__/kms-site.xml
23463049    4 -r-x------   1 yarn     hadoop       1631 May 16 11:10 ./__spark_conf__/kms-log4j.properties
23463045   20 -r-x------   1 yarn     hadoop      16530 May 16 11:10 ./__spark_conf__/yarn-site.xml
23463076    4 -r-x------   1 yarn     hadoop        951 May 16 11:10 ./__spark_conf__/mapred-env.cmd
23463062    8 -r-x------   1 yarn     hadoop       4221 May 16 11:10 ./__spark_conf__/task-log4j.properties
23463050    4 -r-x------   1 yarn     hadoop       2250 May 16 11:10 ./__spark_conf__/yarn-env.cmd
23463078    4 -r-x------   1 yarn     hadoop       2268 May 16 11:10 ./__spark_conf__/ssl-server.xml.example
23463067    4 -r-x------   1 yarn     hadoop       1308 May 16 11:10 ./__spark_conf__/hadoop-policy.xml
23463042    8 -r-x------   1 yarn     hadoop       5467 May 16 11:10 ./__spark_conf__/hadoop-env.sh
23463075    4 -r-x------   1 yarn     hadoop        724 May 16 11:10 ./__spark_conf__/topology_mappings.data
23463047    0 -r-x------   1 yarn     hadoop          0 May 16 11:10 ./__spark_conf__/yarn.exclude
23463046    4 -r-x------   1 yarn     hadoop       3979 May 16 11:10 ./__spark_conf__/hadoop-env.cmd
23463053    4 -r-x------   1 yarn     hadoop       3518 May 16 11:10 ./__spark_conf__/kms-acls.xml
23463041    8 -r-x------   1 yarn     hadoop       6836 May 16 11:10 ./__spark_conf__/mapred-site.xml
12451845    4 -rw-r--r--   1 yarn     hadoop         87 May 16 11:10 ./container_tokens
12451851    4 -rwx------   1 yarn     hadoop        730 May 16 11:10 ./default_container_executor.sh
12451848    4 -rw-r--r--   1 yarn     hadoop        600 May 16 11:10 ./.launch_container.sh.crc
12451849    4 -rwx------   1 yarn     hadoop        676 May 16 11:10 ./default_container_executor_session.sh
12451846    4 -rw-r--r--   1 yarn     hadoop         12 May 16 11:10 ./.container_tokens.crc
12451852    4 -rw-r--r--   1 yarn     hadoop         16 May 16 11:10 ./.default_container_executor.sh.crc
12451853   12 drwxr-xr-x   2 yarn     hadoop      12288 May 16 11:10 ./__spark_libs__
25428328   64 -rwxr-xr-x   1 yarn     hadoop      65012 Mar 27 13:40 ./__spark_libs__/guice-servlet-3.0.jar
25428341   56 -rwxr-xr-x   1 yarn     hadoop      55511 Mar 27 13:40 ./__spark_libs__/spark-network-shuffle_2.11-2.0.1.jar
25429640 1144 -rwxr-xr-x   1 yarn     hadoop    1171380 Mar 27 13:40 ./__spark_libs__/jackson-databind-2.6.5.jar
25429587 1024 -rwxr-xr-x   1 yarn     hadoop    1048110 Mar 27 13:40 ./__spark_libs__/parquet-jackson-1.7.0.jar
25429590   96 -rwxr-xr-x   1 yarn     hadoop      94672 Mar 27 13:40 ./__spark_libs__/xz-1.0.jar
25429415  352 -rwxr-xr-x   1 yarn     hadoop     358390 Mar 27 13:40 ./__spark_libs__/kryo-shaded-3.0.3.jar
25429551   20 -rwxr-xr-x   1 yarn     hadoop      18482 Mar 27 13:40 ./__spark_libs__/eigenbase-properties-1.1.5.jar
25429522   36 -rwxr-xr-x   1 yarn     hadoop      33015 Mar 27 13:40 ./__spark_libs__/jsr305-1.3.9.jar
25428335   16 -rwxr-xr-x   1 yarn     hadoop      15827 Mar 27 13:40 ./__spark_libs__/metrics-json-3.1.2.jar
25429082   24 -rwxr-xr-x   1 yarn     hadoop      21575 Mar 27 13:40 ./__spark_libs__/parquet-common-1.7.0.jar
25428814  332 -rwxr-xr-x   1 yarn     hadoop     339666 Mar 27 13:40 ./__spark_libs__/datanucleus-api-jdo-3.2.6.jar
25429636 1772 -rwxr-xr-x   1 yarn     hadoop    1814309 Mar 27 13:40 ./__spark_libs__/spark-hive-thriftserver_2.11-2.0.1.jar
25429695  172 -rwxr-xr-x   1 yarn     hadoop     174351 Mar 27 13:40 ./__spark_libs__/stream-2.7.0.jar
25428849   68 -rwxr-xr-x   1 yarn     hadoop      68866 Mar 27 13:40 ./__spark_libs__/curator-client-2.6.0.jar
25428426    8 -rwxr-xr-x   1 yarn     hadoop       5950 Mar 27 13:40 ./__spark_libs__/javax.inject-2.4.0-b34.jar
25428564  256 -rwxr-xr-x   1 yarn     hadoop     258876 Mar 27 13:40 ./__spark_libs__/jackson-core-2.6.5.jar
25428731   80 -rwxr-xr-x   1 yarn     hadoop      79845 Mar 27 13:40 ./__spark_libs__/compress-lzf-1.0.3.jar
25428758  180 -rwxr-xr-x   1 yarn     hadoop     181271 Mar 27 13:40 ./__spark_libs__/hk2-locator-2.4.0-b34.jar
25428996   12 -rwxr-xr-x   1 yarn     hadoop      12131 Mar 27 13:40 ./__spark_libs__/jpam-1.1.jar
25429831    8 -rwxr-xr-x   1 yarn     hadoop       5310 Mar 27 13:40 ./__spark_libs__/pmml-schema-1.2.15.jar
25429231  300 -rwxr-xr-x   1 yarn     hadoop     305001 Mar 27 13:40 ./__spark_libs__/commons-httpclient-3.1.jar
25428686 13136 -rwxr-xr-x   1 yarn     hadoop   13448966 Mar 27 13:40 ./__spark_libs__/breeze_2.11-0.11.2.jar
25428721    4 -rwxr-xr-x   1 yarn     hadoop       2497 Mar 27 13:40 ./__spark_libs__/javax.inject-1.jar
25428968  480 -rwxr-xr-x   1 yarn     hadoop     489884 Mar 27 13:40 ./__spark_libs__/log4j-1.2.17.jar
25428647   20 -rwxr-xr-x   1 yarn     hadoop      18098 Mar 27 13:40 ./__spark_libs__/jersey-container-servlet-2.22.2.jar
25429257    8 -rwxr-xr-x   1 yarn     hadoop       4596 Mar 27 13:40 ./__spark_libs__/jul-to-slf4j-1.7.16.jar
25428844 5380 -rwxr-xr-x   1 yarn     hadoop    5505200 Mar 27 13:40 ./__spark_libs__/hive-metastore-1.2.1.spark2.jar
25428902   44 -rwxr-xr-x   1 yarn     hadoop      41070 Mar 27 13:40 ./__spark_libs__/spark-unsafe_2.11-2.0.1.jar
25428771  236 -rwxr-xr-x   1 yarn     hadoop     241367 Mar 27 13:40 ./__spark_libs__/commons-compress-1.4.1.jar
25428307 2044 -rwxr-xr-x   1 yarn     hadoop    2090370 Mar 27 13:40 ./__spark_libs__/spark-streaming_2.11-2.0.1.jar
25428804 6716 -rwxr-xr-x   1 yarn     hadoop    6873892 Mar 27 13:40 ./__spark_libs__/spark-catalyst_2.11-2.0.1.jar
25428538 1988 -rwxr-xr-x   1 yarn     hadoop    2035066 Mar 27 13:40 ./__spark_libs__/commons-math3-3.4.1.jar
25429554 1032 -rwxr-xr-x   1 yarn     hadoop    1056168 Mar 27 13:40 ./__spark_libs__/snappy-java-1.1.2.6.jar
25429500   80 -rwxr-xr-x   1 yarn     hadoop      79912 Mar 27 13:40 ./__spark_libs__/api-util-1.0.0-M20.jar
25428765  720 -rwxr-xr-x   1 yarn     hadoop     736658 Mar 27 13:40 ./__spark_libs__/httpclient-4.5.2.jar
25428302  224 -rwxr-xr-x   1 yarn     hadoop     227712 Mar 27 13:40 ./__spark_libs__/libthrift-0.9.2.jar
25428839 1204 -rwxr-xr-x   1 yarn     hadoop    1230201 Mar 27 13:40 ./__spark_libs__/netty-3.8.0.Final.jar
25428817  524 -rwxr-xr-x   1 yarn     hadoop     533455 Mar 27 13:40 ./__spark_libs__/protobuf-java-2.5.0.jar
25428480  428 -rwxr-xr-x   1 yarn     hadoop     436303 Mar 27 13:40 ./__spark_libs__/avro-1.7.7.jar
25428548   32 -rwxr-xr-x   1 yarn     hadoop      29555 Mar 27 13:40 ./__spark_libs__/paranamer-2.3.jar
25428857   16 -rwxr-xr-x   1 yarn     hadoop      14766 Mar 27 13:40 ./__spark_libs__/aopalliance-repackaged-2.4.0-b34.jar
25429595   44 -rwxr-xr-x   1 yarn     hadoop      45015 Mar 27 13:40 ./__spark_libs__/hadoop-mapreduce-client-shuffle-2.7.1.jar
25429671  120 -rwxr-xr-x   1 yarn     hadoop     118973 Mar 27 13:40 ./__spark_libs__/hk2-utils-2.4.0-b34.jar
25429619 3352 -rwxr-xr-x   1 yarn     hadoop    3431544 Mar 27 13:40 ./__spark_libs__/hadoop-common-2.7.1.jar
25428458   44 -rwxr-xr-x   1 yarn     hadoop      41755 Mar 27 13:40 ./__spark_libs__/objenesis-2.1.jar
25428491   40 -rwxr-xr-x   1 yarn     hadoop      40341 Mar 27 13:40 ./__spark_libs__/json4s-jackson_2.11-3.2.11.jar
25429100  440 -rwxr-xr-x   1 yarn     hadoop     448794 Mar 27 13:40 ./__spark_libs__/apache-log4j-extras-1.2.17.jar
25428601  284 -rwxr-xr-x   1 yarn     hadoop     290506 Mar 27 13:40 ./__spark_libs__/univocity-parsers-2.1.1.jar
25428495  640 -rwxr-xr-x   1 yarn     hadoop     654216 Mar 27 13:40 ./__spark_libs__/pmml-model-1.2.15.jar
25428945   24 -rwxr-xr-x   1 yarn     hadoop      23346 Mar 27 13:40 ./__spark_libs__/stax-api-1.0-2.jar
25429183  232 -rwxr-xr-x   1 yarn     hadoop     236880 Mar 27 13:40 ./__spark_libs__/lz4-1.3.0.jar
25428802  104 -rwxr-xr-x   1 yarn     hadoop     105134 Mar 27 13:40 ./__spark_libs__/jaxb-api-2.2.2.jar
25429647  420 -rwxr-xr-x   1 yarn     hadoop     427780 Mar 27 13:40 ./__spark_libs__/jodd-core-3.5.2.jar
25429467 1204 -rwxr-xr-x   1 yarn     hadoop    1229125 Mar 27 13:40 ./__spark_libs__/xercesImpl-2.9.1.jar
25428935   48 -rwxr-xr-x   1 yarn     hadoop      48720 Mar 27 13:40 ./__spark_libs__/snappy-0.2.jar
25428333   44 -rwxr-xr-x   1 yarn     hadoop      44925 Mar 27 13:40 ./__spark_libs__/apacheds-i18n-2.0.0-M15.jar
25428673  280 -rwxr-xr-x   1 yarn     hadoop     284184 Mar 27 13:40 ./__spark_libs__/commons-codec-1.10.jar
25428269  204 -rwxr-xr-x   1 yarn     hadoop     206035 Mar 27 13:40 ./__spark_libs__/commons-beanutils-core-1.8.0.jar
25428761 4468 -rwxr-xr-x   1 yarn     hadoop    4573750 Mar 27 13:40 ./__spark_libs__/scala-reflect-2.11.8.jar
25429016  100 -rwxr-xr-x   1 yarn     hadoop     100680 Mar 27 13:40 ./__spark_libs__/hive-jdbc-1.2.1.spark2.jar
25429802 2008 -rwxr-xr-x   1 yarn     hadoop    2054931 Mar 27 13:40 ./__spark_libs__/netty-all-4.0.29.Final.jar
25428433 1444 -rwxr-xr-x   1 yarn     hadoop    1475955 Mar 27 13:40 ./__spark_libs__/htrace-core-3.1.0-incubating.jar
25428819  436 -rwxr-xr-x   1 yarn     hadoop     445288 Mar 27 13:40 ./__spark_libs__/antlr-2.7.7.jar
25429790  308 -rwxr-xr-x   1 yarn     hadoop     313686 Mar 27 13:40 ./__spark_libs__/libfb303-0.9.2.jar
25429067  404 -rwxr-xr-x   1 yarn     hadoop     412739 Mar 27 13:40 ./__spark_libs__/commons-lang3-3.3.2.jar
25428695   32 -rwxr-xr-x   1 yarn     hadoop      32145 Mar 27 13:40 ./__spark_libs__/hadoop-yarn-server-web-proxy-2.7.1.jar
25428452   16 -rwxr-xr-x   1 yarn     hadoop      15305 Mar 27 13:40 ./__spark_libs__/spark-tags_2.11-2.0.1.jar
25429809  164 -rwxr-xr-x   1 yarn     hadoop     167421 Mar 27 13:40 ./__spark_libs__/jersey-client-2.22.2.jar
25429112  576 -rwxr-xr-x   1 yarn     hadoop     588337 Mar 27 13:40 ./__spark_libs__/commons-collections-3.2.2.jar
25428657   92 -rwxr-xr-x   1 yarn     hadoop      93407 Mar 27 13:40 ./__spark_libs__/pyrolite-4.9.jar
25429610   84 -rwxr-xr-x   1 yarn     hadoop      82421 Mar 27 13:40 ./__spark_libs__/json4s-ast_2.11-3.2.11.jar
25428345   48 -rwxr-xr-x   1 yarn     hadoop      45944 Mar 27 13:40 ./__spark_libs__/json-20090211.jar
25428587   44 -rwxr-xr-x   1 yarn     hadoop      41263 Mar 27 13:40 ./__spark_libs__/jackson-module-paranamer-2.6.5.jar
25429585  896 -rwxr-xr-x   1 yarn     hadoop     917052 Mar 27 13:40 ./__spark_libs__/parquet-column-1.7.0.jar
25428568   64 -rwxr-xr-x   1 yarn     hadoop      63316 Mar 27 13:40 ./__spark_libs__/spark-repl_2.11-2.0.1.jar
25428417   96 -rwxr-xr-x   1 yarn     hadoop      95806 Mar 27 13:40 ./__spark_libs__/javax.servlet-api-3.1.0.jar
25428752   20 -rwxr-xr-x   1 yarn     hadoop      17008 Mar 27 13:40 ./__spark_libs__/base64-2.3.8.jar
25428697   12 -rwxr-xr-x   1 yarn     hadoop       9939 Mar 27 13:40 ./__spark_libs__/slf4j-log4j12-1.7.16.jar
25428811   32 -rwxr-xr-x   1 yarn     hadoop      29540 Mar 27 13:40 ./__spark_libs__/spark-sketch_2.11-2.0.1.jar
25428462   64 -rwxr-xr-x   1 yarn     hadoop      63777 Mar 27 13:40 ./__spark_libs__/validation-api-1.1.0.Final.jar
25428921  684 -rwxr-xr-x   1 yarn     hadoop     698375 Mar 27 13:40 ./__spark_libs__/jersey-common-2.22.2.jar
25429205 5712 -rwxr-xr-x   1 yarn     hadoop    5847591 Mar 27 13:40 ./__spark_libs__/spark-mllib_2.11-2.0.1.jar
25429827  736 -rwxr-xr-x   1 yarn     hadoop     753012 Mar 27 13:40 ./__spark_libs__/hadoop-mapreduce-client-common-2.7.1.jar
25428551  532 -rwxr-xr-x   1 yarn     hadoop     540852 Mar 27 13:40 ./__spark_libs__/mysql-connector-java-5.0.8-bin.jar
25429090  356 -rwxr-xr-x   1 yarn     hadoop     363908 Mar 27 13:40 ./__spark_libs__/hadoop-yarn-server-common-2.7.1.jar
25428281  112 -rwxr-xr-x   1 yarn     hadoop     112558 Mar 27 13:40 ./__spark_libs__/metrics-core-3.1.2.jar
25428835 2780 -rwxr-xr-x   1 yarn     hadoop    2842667 Mar 27 13:40 ./__spark_libs__/bcprov-jdk15on-1.51.jar
25429624  504 -rwxr-xr-x   1 yarn     hadoop     515604 Mar 27 13:40 ./__spark_libs__/jackson-module-scala_2.11-2.6.5.jar
25428742 1024 -rwxr-xr-x   1 yarn     hadoop    1045744 Mar 27 13:40 ./__spark_libs__/leveldbjni-all-1.8.jar
25428293 1512 -rwxr-xr-x   1 yarn     hadoop    1544875 Mar 27 13:40 ./__spark_libs__/hadoop-mapreduce-client-core-2.7.1.jar
25429504  136 -rwxr-xr-x   1 yarn     hadoop     138464 Mar 27 13:40 ./__spark_libs__/hive-beeline-1.2.1.spark2.jar
25428724  208 -rwxr-xr-x   1 yarn     hadoop     209622 Mar 27 13:40 ./__spark_libs__/parquet-hadoop-1.7.0.jar
25428449   96 -rwxr-xr-x   1 yarn     hadoop      96221 Mar 27 13:40 ./__spark_libs__/commons-pool-1.5.4.jar
25428472 15128 -rwxr-xr-x   1 yarn     hadoop   15487351 Mar 27 13:40 ./__spark_libs__/scala-compiler-2.11.8.jar
25429805  508 -rwxr-xr-x   1 yarn     hadoop     516127 Mar 27 13:40 ./__spark_libs__/hadoop-mapreduce-client-app-2.7.1.jar
25428641   20 -rwxr-xr-x   1 yarn     hadoop      19827 Mar 27 13:40 ./__spark_libs__/opencsv-2.3.jar
25429626 1168 -rwxr-xr-x   1 yarn     hadoop    1194003 Mar 27 13:40 ./__spark_libs__/arpack_combined_all-0.1.jar
25428876  280 -rwxr-xr-x   1 yarn     hadoop     284220 Mar 27 13:40 ./__spark_libs__/commons-lang-2.6.jar
25428682  656 -rwxr-xr-x   1 yarn     hadoop     669589 Mar 27 13:40 ./__spark_libs__/spark-yarn_2.11-2.0.1.jar
25429576   64 -rwxr-xr-x   1 yarn     hadoop      62050 Mar 27 13:40 ./__spark_libs__/commons-logging-1.1.3.jar
25428684  764 -rwxr-xr-x   1 yarn     hadoop     780664 Mar 27 13:40 ./__spark_libs__/jackson-mapper-asl-1.9.13.jar
25429496 6144 -rwxr-xr-x   1 yarn     hadoop    6290315 Mar 27 13:40 ./__spark_libs__/spark-sql_2.11-2.0.1.jar
25429493  208 -rwxr-xr-x   1 yarn     hadoop     212453 Mar 27 13:40 ./__spark_libs__/commons-net-2.2.jar
25428822  164 -rwxr-xr-x   1 yarn     hadoop     164368 Mar 27 13:40 ./__spark_libs__/antlr-runtime-3.4.jar
25428532   68 -rwxr-xr-x   1 yarn     hadoop      66270 Mar 27 13:40 ./__spark_libs__/jersey-container-servlet-core-2.22.2.jar
25429573  184 -rwxr-xr-x   1 yarn     hadoop     185245 Mar 27 13:40 ./__spark_libs__/curator-framework-2.6.0.jar
25428623  176 -rwxr-xr-x   1 yarn     hadoop     178947 Mar 27 13:40 ./__spark_libs__/hk2-api-2.4.0-b34.jar
25429796 7108 -rwxr-xr-x   1 yarn     hadoop    7276083 Mar 27 13:40 ./__spark_libs__/spire_2.11-0.7.4.jar
25428852   20 -rwxr-xr-x   1 yarn     hadoop      17385 Mar 27 13:40 ./__spark_libs__/hadoop-annotations-2.7.1.jar
25428911  280 -rwxr-xr-x   1 yarn     hadoop     285447 Mar 27 13:40 ./__spark_libs__/parquet-encoding-1.7.0.jar
25428536   20 -rwxr-xr-x   1 yarn     hadoop      16993 Mar 27 13:40 ./__spark_libs__/JavaEWAH-0.3.2.jar
25429524   16 -rwxr-xr-x   1 yarn     hadoop      15010 Mar 27 13:40 ./__spark_libs__/xmlenc-0.52.jar
25428951 1616 -rwxr-xr-x   1 yarn     hadoop    1654097 Mar 27 13:40 ./__spark_libs__/hadoop-yarn-common-2.7.1.jar
25429509 3152 -rwxr-xr-x   1 yarn     hadoop    3224708 Mar 27 13:40 ./__spark_libs__/derby-10.12.1.1.jar
25428763  192 -rwxr-xr-x   1 yarn     hadoop     192993 Mar 27 13:40 ./__spark_libs__/avro-ipc-1.7.7.jar
25428562  188 -rwxr-xr-x   1 yarn     hadoop     190432 Mar 27 13:40 ./__spark_libs__/gson-2.2.4.jar
25428316  176 -rwxr-xr-x   1 yarn     hadoop     177131 Mar 27 13:40 ./__spark_libs__/jetty-util-6.1.26.jar
25428749    8 -rwxr-xr-x   1 yarn     hadoop       4467 Mar 27 13:40 ./__spark_libs__/aopalliance-1.0.jar
25428497  188 -rwxr-xr-x   1 yarn     hadoop     188671 Mar 27 13:40 ./__spark_libs__/commons-beanutils-1.7.0.jar
25428708 1256 -rwxr-xr-x   1 yarn     hadoop    1282424 Mar 27 13:40 ./__spark_libs__/ivy-2.4.0.jar
25429010  676 -rwxr-xr-x   1 yarn     hadoop     691479 Mar 27 13:40 ./__spark_libs__/apacheds-kerberos-codec-2.0.0-M15.jar
25429644   68 -rwxr-xr-x   1 yarn     hadoop      69409 Mar 27 13:40 ./__spark_libs__/activation-1.1.1.jar
25428927   20 -rwxr-xr-x   1 yarn     hadoop      16560 Mar 27 13:40 ./__spark_libs__/api-asn1-api-1.0.0-M20.jar
25429519  616 -rwxr-xr-x   1 yarn     hadoop     627814 Mar 27 13:40 ./__spark_libs__/joda-time-2.9.3.jar
25428841 1248 -rwxr-xr-x   1 yarn     hadoop    1277883 Mar 27 13:40 ./__spark_libs__/mesos-0.21.1-shaded-protobuf.jar
25429638 2140 -rwxr-xr-x   1 yarn     hadoop    2189117 Mar 27 13:40 ./__spark_libs__/guava-14.0.1.jar
25428887  148 -rwxr-xr-x   1 yarn     hadoop     148627 Mar 27 13:40 ./__spark_libs__/stringtemplate-3.2.1.jar
25428318   48 -rwxr-xr-x   1 yarn     hadoop      46983 Mar 27 13:40 ./__spark_libs__/jackson-annotations-2.6.5.jar
25429593   20 -rwxr-xr-x   1 yarn     hadoop      18336 Mar 27 13:40 ./__spark_libs__/jackson-jaxrs-1.9.13.jar
25428540   40 -rwxr-xr-x   1 yarn     hadoop      39280 Mar 27 13:40 ./__spark_libs__/metrics-jvm-3.1.2.jar
25429659  388 -rwxr-xr-x   1 yarn     hadoop     395195 Mar 27 13:40 ./__spark_libs__/javolution-5.5.1.jar
25428658  136 -rwxr-xr-x   1 yarn     hadoop     135552 Mar 27 13:40 ./__spark_libs__/breeze-macros_2.11-0.11.2.jar
25428808   72 -rwxr-xr-x   1 yarn     hadoop      72733 Mar 27 13:40 ./__spark_libs__/jersey-media-jaxb-2.22.2.jar
25428837   88 -rwxr-xr-x   1 yarn     hadoop      86811 Mar 27 13:40 ./__spark_libs__/spire-macros_2.11-0.7.4.jar
25429531   32 -rwxr-xr-x   1 yarn     hadoop      30595 Mar 27 13:40 ./__spark_libs__/commons-compiler-2.7.6.jar
25429269    8 -rwxr-xr-x   1 yarn     hadoop       5711 Mar 27 13:40 ./__spark_libs__/minlog-1.3.0.jar
25428320  776 -rwxr-xr-x   1 yarn     hadoop     792964 Mar 27 13:40 ./__spark_libs__/zookeeper-3.4.6.jar
25429516 1768 -rwxr-xr-x   1 yarn     hadoop    1809447 Mar 27 13:40 ./__spark_libs__/datanucleus-rdbms-3.2.9.jar
25429652  140 -rwxr-xr-x   1 yarn     hadoop     142631 Mar 27 13:40 ./__spark_libs__/hadoop-yarn-client-2.7.1.jar
25428739  232 -rwxr-xr-x   1 yarn     hadoop     236660 Mar 27 13:40 ./__spark_libs__/ST4-4.0.4.jar
25429535   12 -rwxr-xr-x   1 yarn     hadoop      10023 Mar 27 13:40 ./__spark_libs__/java-xmlbuilder-1.0.jar
25428666 1848 -rwxr-xr-x   1 yarn     hadoop    1890075 Mar 27 13:40 ./__spark_libs__/datanucleus-core-3.2.10.jar
25428745 11232 -rwxr-xr-x   1 yarn     hadoop   11498852 Mar 27 13:40 ./__spark_libs__/hive-exec-1.2.1.spark2.jar
25428631  228 -rwxr-xr-x   1 yarn     hadoop     232248 Mar 27 13:40 ./__spark_libs__/jackson-core-asl-1.9.13.jar
25428310  180 -rwxr-xr-x   1 yarn     hadoop     180736 Mar 27 13:40 ./__spark_libs__/avro-mapred-1.7.7-hadoop2.jar
25428937  952 -rwxr-xr-x   1 yarn     hadoop     971310 Mar 27 13:40 ./__spark_libs__/jersey-guava-2.22.2.jar
25429598   28 -rwxr-xr-x   1 yarn     hadoop      27084 Mar 27 13:40 ./__spark_libs__/jackson-xc-1.9.13.jar
25429743   24 -rwxr-xr-x   1 yarn     hadoop      21243 Mar 27 13:40 ./__spark_libs__/parquet-generator-1.7.0.jar
25428692   68 -rwxr-xr-x   1 yarn     hadoop      65653 Mar 27 13:40 ./__spark_libs__/spark-launcher_2.11-2.0.1.jar
25429608  668 -rwxr-xr-x   1 yarn     hadoop     680106 Mar 27 13:40 ./__spark_libs__/spark-graphx_2.11-2.0.1.jar
25428908  416 -rwxr-xr-x   1 yarn     hadoop     423753 Mar 27 13:40 ./__spark_libs__/scala-parser-combinators_2.11-1.0.4.jar
25429583  512 -rwxr-xr-x   1 yarn     hadoop     521157 Mar 27 13:40 ./__spark_libs__/mail-1.4.7.jar
25429548  700 -rwxr-xr-x   1 yarn     hadoop     714194 Mar 27 13:40 ./__spark_libs__/javassist-3.18.1-GA.jar
25428667  164 -rwxr-xr-x   1 yarn     hadoop     164422 Mar 27 13:40 ./__spark_libs__/core-1.1.2.jar
25429655   16 -rwxr-xr-x   1 yarn     hadoop      15071 Mar 27 13:40 ./__spark_libs__/jta-1.1.jar
25428714   52 -rwxr-xr-x   1 yarn     hadoop      50619 Mar 27 13:40 ./__spark_libs__/chill-java-0.8.0.jar
25429668  468 -rwxr-xr-x   1 yarn     hadoop     477970 Mar 27 13:40 ./__spark_libs__/lift-json_2.11-2.6.3.jar
25428499  144 -rwxr-xr-x   1 yarn     hadoop     144660 Mar 27 13:40 ./__spark_libs__/xbean-asm5-shaded-4.4.jar
25428628  788 -rwxr-xr-x   1 yarn     hadoop     802818 Mar 27 13:40 ./__spark_libs__/scalap-2.11.8.jar
25429629   40 -rwxr-xr-x   1 yarn     hadoop      38134 Mar 27 13:40 ./__spark_libs__/hadoop-mapreduce-client-jobclient-2.7.1.jar
25429558  320 -rwxr-xr-x   1 yarn     hadoop     326724 Mar 27 13:40 ./__spark_libs__/httpcore-4.4.4.jar
25429545   64 -rwxr-xr-x   1 yarn     hadoop      65261 Mar 27 13:40 ./__spark_libs__/oro-2.0.8.jar
25429786  296 -rwxr-xr-x   1 yarn     hadoop     302248 Mar 27 13:40 ./__spark_libs__/antlr4-runtime-4.5.3.jar
25428596  160 -rwxr-xr-x   1 yarn     hadoop     160519 Mar 27 13:40 ./__spark_libs__/commons-dbcp-1.4.jar
25429003  380 -rwxr-xr-x   1 yarn     hadoop     387188 Mar 27 13:40 ./__spark_libs__/parquet-format-2.3.0-incubating.jar
25428826  184 -rwxr-xr-x   1 yarn     hadoop     185140 Mar 27 13:40 ./__spark_libs__/commons-io-2.4.jar
25429381   28 -rwxr-xr-x   1 yarn     hadoop      26366 Mar 27 13:40 ./__spark_libs__/javax.annotation-api-1.2.jar
25429119  220 -rwxr-xr-x   1 yarn     hadoop     223573 Mar 27 13:40 ./__spark_libs__/chill_2.11-0.8.0.jar
25428314 3440 -rwxr-xr-x   1 yarn     hadoop    3519262 Mar 27 13:40 ./__spark_libs__/calcite-core-1.2.0-incubating.jar
25428719 1592 -rwxr-xr-x   1 yarn     hadoop    1627065 Mar 27 13:40 ./__spark_libs__/jersey-bundle-1.19.1.jar
25429700   28 -rwxr-xr-x   1 yarn     hadoop      26514 Mar 27 13:40 ./__spark_libs__/stax-api-1.0.1.jar
25428409    4 -rwxr-xr-x   1 yarn     hadoop       2545 Mar 27 13:40 ./__spark_libs__/hadoop-client-2.7.1.jar
25428271   40 -rwxr-xr-x   1 yarn     hadoop      40817 Mar 27 13:40 ./__spark_libs__/hive-cli-1.2.1.spark2.jar
25428322   92 -rwxr-xr-x   1 yarn     hadoop      93210 Mar 27 13:40 ./__spark_libs__/super-csv-2.2.0.jar
25429680 1024 -rwxr-xr-x   1 yarn     hadoop    1044511 Mar 27 13:40 ./__spark_libs__/spark-hive_2.11-2.0.1.jar
25429510 2304 -rwxr-xr-x   1 yarn     hadoop    2355465 Mar 27 13:40 ./__spark_libs__/spark-network-common_2.11-2.0.1.jar
25429234  200 -rwxr-xr-x   1 yarn     hadoop     201124 Mar 27 13:40 ./__spark_libs__/jdo-api-3.0.1.jar
25428955  116 -rwxr-xr-x   1 yarn     hadoop     114913 Mar 27 13:40 ./__spark_libs__/py4j-0.10.3.jar
25428636  144 -rwxr-xr-x   1 yarn     hadoop     143602 Mar 27 13:40 ./__spark_libs__/commons-digester-1.8.jar
25428466   20 -rwxr-xr-x   1 yarn     hadoop      16430 Mar 27 13:40 ./__spark_libs__/jcl-over-slf4j-1.7.16.jar
25429612 1972 -rwxr-xr-x   1 yarn     hadoop    2015514 Mar 27 13:40 ./__spark_libs__/hadoop-yarn-api-2.7.1.jar
25428411  636 -rwxr-xr-x   1 yarn     hadoop     648678 Mar 27 13:40 ./__spark_libs__/scala-xml_2.11-1.0.2.jar
25428755  528 -rwxr-xr-x   1 yarn     hadoop     539912 Mar 27 13:40 ./__spark_libs__/jetty-6.1.26.jar
25429616  212 -rwxr-xr-x   1 yarn     hadoop     213911 Mar 27 13:40 ./__spark_libs__/jline-2.12.1.jar
25428892  100 -rwxr-xr-x   1 yarn     hadoop     100636 Mar 27 13:40 ./__spark_libs__/jsp-api-2.1.jar
25429759   44 -rwxr-xr-x   1 yarn     hadoop      41123 Mar 27 13:40 ./__spark_libs__/commons-cli-1.2.jar
25428579  932 -rwxr-xr-x   1 yarn     hadoop     951701 Mar 27 13:40 ./__spark_libs__/jersey-server-2.22.2.jar
25428770 5612 -rwxr-xr-x   1 yarn     hadoop    5744974 Mar 27 13:40 ./__spark_libs__/scala-library-2.11.8.jar
25429563   20 -rwxr-xr-x   1 yarn     hadoop      20235 Mar 27 13:40 ./__spark_libs__/osgi-resource-locator-1.0.1.jar
25428505  176 -rwxr-xr-x   1 yarn     hadoop     177832 Mar 27 13:40 ./__spark_libs__/spark-mllib-local_2.11-2.0.1.jar
25428585 1996 -rwxr-xr-x   1 yarn     hadoop    2041628 Mar 27 13:40 ./__spark_libs__/jets3t-0.9.3.jar
25428916  748 -rwxr-xr-x   1 yarn     hadoop     764569 Mar 27 13:40 ./__spark_libs__/jtransforms-2.4.0.jar
25428582  244 -rwxr-xr-x   1 yarn     hadoop     248171 Mar 27 13:40 ./__spark_libs__/curator-recipes-2.6.0.jar
25429047   72 -rwxr-xr-x   1 yarn     hadoop      70688 Mar 27 13:40 ./__spark_libs__/hadoop-auth-2.7.1.jar
25428678   40 -rwxr-xr-x   1 yarn     hadoop      40509 Mar 27 13:40 ./__spark_libs__/slf4j-api-1.7.16.jar
25428464 8068 -rwxr-xr-x   1 yarn     hadoop    8260573 Mar 27 13:40 ./__spark_libs__/hadoop-hdfs-2.7.1.jar
25428475  576 -rwxr-xr-x   1 yarn     hadoop     589462 Mar 27 13:40 ./__spark_libs__/json4s-core_2.11-3.2.11.jar
25428655  600 -rwxr-xr-x   1 yarn     hadoop     613299 Mar 27 13:40 ./__spark_libs__/janino-2.7.8.jar
25428545 2732 -rwxr-xr-x   1 yarn     hadoop    2796935 Mar 27 13:40 ./__spark_libs__/parquet-hadoop-bundle-1.6.0.jar
25428784  256 -rwxr-xr-x   1 yarn     hadoop     258370 Mar 27 13:40 ./__spark_libs__/calcite-avatica-1.2.0-incubating.jar
25429541   24 -rwxr-xr-x   1 yarn     hadoop      20852 Mar 27 13:40 ./__spark_libs__/metrics-graphite-3.1.2.jar
25428961  292 -rwxr-xr-x   1 yarn     hadoop     298829 Mar 27 13:40 ./__spark_libs__/commons-configuration-1.6.jar
25429445 11452 -rwxr-xr-x   1 yarn     hadoop   11723537 Mar 27 13:40 ./__spark_libs__/spark-core_2.11-2.0.1.jar
25428767  436 -rwxr-xr-x   1 yarn     hadoop     442406 Mar 27 13:40 ./__spark_libs__/calcite-linq4j-1.2.0-incubating.jar
25429664  200 -rwxr-xr-x   1 yarn     hadoop     201928 Mar 27 13:40 ./__spark_libs__/RoaringBitmap-0.5.11.jar
25429543  696 -rwxr-xr-x   1 yarn     hadoop     710492 Mar 27 13:40 ./__spark_libs__/guice-3.0.jar
25429684  400 -rwxr-xr-x   1 yarn     hadoop     409467 Mar 27 13:40 ./__spark_libs__/mx4j-3.0.2.jar
25428949  116 -rwxr-xr-x   1 yarn     hadoop     115534 Mar 27 13:40 ./__spark_libs__/javax.ws.rs-api-2.0.1.jar
25428711  112 -rwxr-xr-x   1 yarn     hadoop     110600 Mar 27 13:40 ./__spark_libs__/bonecp-0.8.0.RELEASE.jar
12451843    4 drwx--x---   2 yarn     hadoop       4096 May 16 11:10 ./tmp
23463035 1152 -r-x------   1 yarn     hadoop    1178454 May 16 11:10 ./__app__.jar
12451850    4 -rw-r--r--   1 yarn     hadoop         16 May 16 11:10 ./.default_container_executor_session.sh.crc
12451847   76 -rwx------   1 yarn     hadoop      75554 May 16 11:10 ./launch_container.sh
broken symlinks(find -L . -maxdepth 5 -type l -ls):
End of LogType:directory.info

LogType:launch_container.sh
Log Upload Time:Tue May 16 11:12:19 +0700 2017
LogLength:75554
Log Contents:
#!/bin/bash

export SPARK_YARN_STAGING_DIR="hdfs://c408.hadoop.gda.lo:8020/user/fairy/.sparkStaging/application_1494395298335_303847"
export LOCAL_DIRS="/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847"
export HADOOP_CONF_DIR="/usr/hdp/current/hadoop-client/conf"
export NM_HTTP_PORT="8042"
export JAVA_HOME="/usr/jdk64/jdk1.8.0_92/"
export LOG_DIRS="/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000002"
export NM_AUX_SERVICE_mapreduce_shuffle="AAA0+gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=
"
export NM_PORT="45454"
export USER="fairy"
export HADOOP_YARN_HOME="/usr/hdp/current/hadoop-yarn-nodemanager"
export CLASSPATH="$PWD:$PWD/__spark_conf__:$PWD/__spark_libs__/*:$HADOOP_CONF_DIR:/usr/hdp/current/hadoop-client/*:/usr/hdp/current/hadoop-client/lib/*:/usr/hdp/current/hadoop-hdfs-client/*:/usr/hdp/current/hadoop-hdfs-client/lib/*:/usr/hdp/current/hadoop-yarn-client/*:/usr/hdp/current/hadoop-yarn-client/lib/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.4.2.0-258/hadoop/lib/hadoop-lzo-0.6.0.2.4.2.0-258.jar:/etc/hadoop/conf/secure"
export SPARK_LOG_URL_STDOUT="http://c432.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_01_000002/fairy/stdout?start=-4096"
export NM_HOST="c432.hadoop.gda.lo"
export SPARK_YARN_MODE="true"
export HADOOP_TOKEN_FILE_LOCATION="/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_01_000002/container_tokens"
export NM_AUX_SERVICE_spark_shuffle=""
export SPARK_USER="fairy"
export SPARK_LOG_URL_STDERR="http://c432.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_01_000002/fairy/stderr?start=-4096"
export LOGNAME="fairy"
export JVM_PID="$$"
export PWD="/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_01_000002"
export HOME="/home/"
export CONTAINER_ID="container_e81_1494395298335_303847_01_000002"
export MALLOC_ARENA_MAX="4"
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3440/hadoop-client-2.7.1.jar" "__spark_libs__/hadoop-client-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3503/leveldbjni-all-1.8.jar" "__spark_libs__/leveldbjni-all-1.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3630/hadoop-mapreduce-client-app-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-app-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3527/netty-3.8.0.Final.jar" "__spark_libs__/netty-3.8.0.Final.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3458/spark-mllib-local_2.11-2.0.1.jar" "__spark_libs__/spark-mllib-local_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3528/curator-client-2.6.0.jar" "__spark_libs__/curator-client-2.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3576/joda-time-2.9.3.jar" "__spark_libs__/joda-time-2.9.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3462/metrics-jvm-3.1.2.jar" "__spark_libs__/metrics-jvm-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3486/datanucleus-core-3.2.10.jar" "__spark_libs__/datanucleus-core-3.2.10.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3564/jul-to-slf4j-1.7.16.jar" "__spark_libs__/jul-to-slf4j-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3558/chill_2.11-0.8.0.jar" "__spark_libs__/chill_2.11-0.8.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3425/hadoop-mapreduce-client-core-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-core-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3557/commons-collections-3.2.2.jar" "__spark_libs__/commons-collections-3.2.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3485/core-1.1.2.jar" "__spark_libs__/core-1.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3471/curator-recipes-2.6.0.jar" "__spark_libs__/curator-recipes-2.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3520/antlr-2.7.7.jar" "__spark_libs__/antlr-2.7.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3454/json4s-jackson_2.11-3.2.11.jar" "__spark_libs__/json4s-jackson_2.11-3.2.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3501/compress-lzf-1.0.3.jar" "__spark_libs__/compress-lzf-1.0.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3480/opencsv-2.3.jar" "__spark_libs__/opencsv-2.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3594/parquet-jackson-1.7.0.jar" "__spark_libs__/parquet-jackson-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3620/spark-hive-thriftserver_2.11-2.0.1.jar" "__spark_libs__/spark-hive-thriftserver_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3546/hadoop-yarn-common-2.7.1.jar" "__spark_libs__/hadoop-yarn-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3579/commons-compiler-2.7.6.jar" "__spark_libs__/commons-compiler-2.7.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3445/commons-pool-1.5.4.jar" "__spark_libs__/commons-pool-1.5.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3616/lift-json_2.11-2.6.3.jar" "__spark_libs__/lift-json_2.11-2.6.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3631/hadoop-mapreduce-client-common-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3441/scala-xml_2.11-1.0.2.jar" "__spark_libs__/scala-xml_2.11-1.0.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3516/spark-catalyst_2.11-2.0.1.jar" "__spark_libs__/spark-catalyst_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3592/mail-1.4.7.jar" "__spark_libs__/mail-1.4.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3491/breeze_2.11-0.11.2.jar" "__spark_libs__/breeze_2.11-0.11.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/usercache/fairy/filecache/11435/__spark_conf__.zip" "__spark_conf__"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3484/pyrolite-4.9.jar" "__spark_libs__/pyrolite-4.9.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3622/commons-cli-1.2.jar" "__spark_libs__/commons-cli-1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3497/chill-java-0.8.0.jar" "__spark_libs__/chill-java-0.8.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3463/parquet-hadoop-bundle-1.6.0.jar" "__spark_libs__/parquet-hadoop-bundle-1.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3531/aopalliance-repackaged-2.4.0-b34.jar" "__spark_libs__/aopalliance-repackaged-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3593/parquet-column-1.7.0.jar" "__spark_libs__/parquet-column-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3545/javax.ws.rs-api-2.0.1.jar" "__spark_libs__/javax.ws.rs-api-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3555/parquet-common-1.7.0.jar" "__spark_libs__/parquet-common-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3581/kryo-shaded-3.0.3.jar" "__spark_libs__/kryo-shaded-3.0.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/usercache/fairy/filecache/11434/stats-etlr-1.0.jar" "__app__.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3429/jetty-util-6.1.26.jar" "__spark_libs__/jetty-util-6.1.26.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3625/antlr4-runtime-4.5.3.jar" "__spark_libs__/antlr4-runtime-4.5.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3492/spark-launcher_2.11-2.0.1.jar" "__spark_libs__/spark-launcher_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3588/osgi-resource-locator-1.0.1.jar" "__spark_libs__/osgi-resource-locator-1.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3578/xmlenc-0.52.jar" "__spark_libs__/xmlenc-0.52.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3585/eigenbase-properties-1.1.5.jar" "__spark_libs__/eigenbase-properties-1.1.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3605/arpack_combined_all-0.1.jar" "__spark_libs__/arpack_combined_all-0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3574/derby-10.12.1.1.jar" "__spark_libs__/derby-10.12.1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3434/apacheds-i18n-2.0.0-M15.jar" "__spark_libs__/apacheds-i18n-2.0.0-M15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3587/httpcore-4.4.4.jar" "__spark_libs__/httpcore-4.4.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3537/scala-parser-combinators_2.11-1.0.4.jar" "__spark_libs__/scala-parser-combinators_2.11-1.0.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3561/spark-mllib_2.11-2.0.1.jar" "__spark_libs__/spark-mllib_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3430/zookeeper-3.4.6.jar" "__spark_libs__/zookeeper-3.4.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3510/httpclient-4.5.2.jar" "__spark_libs__/httpclient-4.5.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3506/base64-2.3.8.jar" "__spark_libs__/base64-2.3.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3615/RoaringBitmap-0.5.11.jar" "__spark_libs__/RoaringBitmap-0.5.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3487/commons-codec-1.10.jar" "__spark_libs__/commons-codec-1.10.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3477/scalap-2.11.8.jar" "__spark_libs__/scalap-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3428/avro-mapred-1.7.7-hadoop2.jar" "__spark_libs__/avro-mapred-1.7.7-hadoop2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3632/pmml-schema-1.2.15.jar" "__spark_libs__/pmml-schema-1.2.15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3513/commons-compress-1.4.1.jar" "__spark_libs__/commons-compress-1.4.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3427/commons-beanutils-core-1.8.0.jar" "__spark_libs__/commons-beanutils-core-1.8.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3469/spark-repl_2.11-2.0.1.jar" "__spark_libs__/spark-repl_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3500/parquet-hadoop-1.7.0.jar" "__spark_libs__/parquet-hadoop-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3572/hive-beeline-1.2.1.spark2.jar" "__spark_libs__/hive-beeline-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3470/jersey-server-2.22.2.jar" "__spark_libs__/jersey-server-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3568/xercesImpl-2.9.1.jar" "__spark_libs__/xercesImpl-2.9.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3476/hk2-api-2.4.0-b34.jar" "__spark_libs__/hk2-api-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3598/hadoop-mapreduce-client-shuffle-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-shuffle-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3451/scala-compiler-2.11.8.jar" "__spark_libs__/scala-compiler-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3614/javolution-5.5.1.jar" "__spark_libs__/javolution-5.5.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3591/commons-logging-1.1.3.jar" "__spark_libs__/commons-logging-1.1.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3617/hk2-utils-2.4.0-b34.jar" "__spark_libs__/hk2-utils-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3467/gson-2.2.4.jar" "__spark_libs__/gson-2.2.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3447/objenesis-2.1.jar" "__spark_libs__/objenesis-2.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3461/commons-math3-3.4.1.jar" "__spark_libs__/commons-math3-3.4.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3473/jets3t-0.9.3.jar" "__spark_libs__/jets3t-0.9.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3567/spark-core_2.11-2.0.1.jar" "__spark_libs__/spark-core_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3424/metrics-core-3.1.2.jar" "__spark_libs__/metrics-core-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3483/breeze-macros_2.11-0.11.2.jar" "__spark_libs__/breeze-macros_2.11-0.11.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3608/guava-14.0.1.jar" "__spark_libs__/guava-14.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3571/api-util-1.0.0-M20.jar" "__spark_libs__/api-util-1.0.0-M20.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3628/netty-all-4.0.29.Final.jar" "__spark_libs__/netty-all-4.0.29.Final.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3509/avro-ipc-1.7.7.jar" "__spark_libs__/avro-ipc-1.7.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3522/commons-io-2.4.jar" "__spark_libs__/commons-io-2.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3482/janino-2.7.8.jar" "__spark_libs__/janino-2.7.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3488/slf4j-api-1.7.16.jar" "__spark_libs__/slf4j-api-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3475/univocity-parsers-2.1.1.jar" "__spark_libs__/univocity-parsers-2.1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3511/calcite-linq4j-1.2.0-incubating.jar" "__spark_libs__/calcite-linq4j-1.2.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3479/commons-digester-1.8.jar" "__spark_libs__/commons-digester-1.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3523/jaxb-api-2.2.2.jar" "__spark_libs__/jaxb-api-2.2.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3551/jpam-1.1.jar" "__spark_libs__/jpam-1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3507/jetty-6.1.26.jar" "__spark_libs__/jetty-6.1.26.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3449/hadoop-hdfs-2.7.1.jar" "__spark_libs__/hadoop-hdfs-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3432/calcite-core-1.2.0-incubating.jar" "__spark_libs__/calcite-core-1.2.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3514/calcite-avatica-1.2.0-incubating.jar" "__spark_libs__/calcite-avatica-1.2.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3508/hk2-locator-2.4.0-b34.jar" "__spark_libs__/hk2-locator-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3540/jersey-common-2.22.2.jar" "__spark_libs__/jersey-common-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3532/commons-lang-2.6.jar" "__spark_libs__/commons-lang-2.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3573/spark-network-common_2.11-2.0.1.jar" "__spark_libs__/spark-network-common_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3460/xbean-asm5-shaded-4.4.jar" "__spark_libs__/xbean-asm5-shaded-4.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3584/javassist-3.18.1-GA.jar" "__spark_libs__/javassist-3.18.1-GA.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3575/datanucleus-rdbms-3.2.9.jar" "__spark_libs__/datanucleus-rdbms-3.2.9.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3502/ST4-4.0.4.jar" "__spark_libs__/ST4-4.0.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3621/stax-api-1.0.1.jar" "__spark_libs__/stax-api-1.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3468/jackson-core-2.6.5.jar" "__spark_libs__/jackson-core-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3624/stream-2.7.0.jar" "__spark_libs__/stream-2.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3548/commons-configuration-1.6.jar" "__spark_libs__/commons-configuration-1.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3436/spark-network-shuffle_2.11-2.0.1.jar" "__spark_libs__/spark-network-shuffle_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3518/spark-sketch_2.11-2.0.1.jar" "__spark_libs__/spark-sketch_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3538/parquet-encoding-1.7.0.jar" "__spark_libs__/parquet-encoding-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3563/jdo-api-3.0.1.jar" "__spark_libs__/jdo-api-3.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3601/hadoop-yarn-api-2.7.1.jar" "__spark_libs__/hadoop-yarn-api-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3582/guice-3.0.jar" "__spark_libs__/guice-3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3439/json-20090211.jar" "__spark_libs__/json-20090211.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3525/spire-macros_2.11-0.7.4.jar" "__spark_libs__/spire-macros_2.11-0.7.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3607/apache-log4j-extras-1.2.17.jar" "__spark_libs__/apache-log4j-extras-1.2.17.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3565/minlog-1.3.0.jar" "__spark_libs__/minlog-1.3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3589/metrics-graphite-3.1.2.jar" "__spark_libs__/metrics-graphite-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3490/jackson-mapper-asl-1.9.13.jar" "__spark_libs__/jackson-mapper-asl-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3442/javax.servlet-api-3.1.0.jar" "__spark_libs__/javax.servlet-api-3.1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3626/libfb303-0.9.2.jar" "__spark_libs__/libfb303-0.9.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3446/spark-tags_2.11-2.0.1.jar" "__spark_libs__/spark-tags_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3431/super-csv-2.2.0.jar" "__spark_libs__/super-csv-2.2.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3534/jsp-api-2.1.jar" "__spark_libs__/jsp-api-2.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3524/bcprov-jdk15on-1.51.jar" "__spark_libs__/bcprov-jdk15on-1.51.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3560/lz4-1.3.0.jar" "__spark_libs__/lz4-1.3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3474/commons-dbcp-1.4.jar" "__spark_libs__/commons-dbcp-1.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3464/paranamer-2.3.jar" "__spark_libs__/paranamer-2.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3552/hive-jdbc-1.2.1.spark2.jar" "__spark_libs__/hive-jdbc-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3535/hive-metastore-1.2.1.spark2.jar" "__spark_libs__/hive-metastore-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3566/javax.annotation-api-1.2.jar" "__spark_libs__/javax.annotation-api-1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3603/hadoop-common-2.7.1.jar" "__spark_libs__/hadoop-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3435/metrics-json-3.1.2.jar" "__spark_libs__/metrics-json-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3554/commons-lang3-3.3.2.jar" "__spark_libs__/commons-lang3-3.3.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3580/java-xmlbuilder-1.0.jar" "__spark_libs__/java-xmlbuilder-1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3586/snappy-java-1.1.2.6.jar" "__spark_libs__/snappy-java-1.1.2.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3542/snappy-0.2.jar" "__spark_libs__/snappy-0.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3610/activation-1.1.1.jar" "__spark_libs__/activation-1.1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3544/stax-api-1.0-2.jar" "__spark_libs__/stax-api-1.0-2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3611/jodd-core-3.5.2.jar" "__spark_libs__/jodd-core-3.5.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3519/protobuf-java-2.5.0.jar" "__spark_libs__/protobuf-java-2.5.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3569/commons-net-2.2.jar" "__spark_libs__/commons-net-2.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3559/apacheds-kerberos-codec-2.0.0-M15.jar" "__spark_libs__/apacheds-kerberos-codec-2.0.0-M15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3604/jackson-module-scala_2.11-2.6.5.jar" "__spark_libs__/jackson-module-scala_2.11-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3556/hadoop-yarn-server-common-2.7.1.jar" "__spark_libs__/hadoop-yarn-server-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3496/bonecp-0.8.0.RELEASE.jar" "__spark_libs__/bonecp-0.8.0.RELEASE.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3450/spark-streaming_2.11-2.0.1.jar" "__spark_libs__/spark-streaming_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3489/spark-yarn_2.11-2.0.1.jar" "__spark_libs__/spark-yarn_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3570/spark-sql_2.11-2.0.1.jar" "__spark_libs__/spark-sql_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3459/jersey-container-servlet-core-2.22.2.jar" "__spark_libs__/jersey-container-servlet-core-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3498/jersey-bundle-1.19.1.jar" "__spark_libs__/jersey-bundle-1.19.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3499/javax.inject-1.jar" "__spark_libs__/javax.inject-1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3495/ivy-2.4.0.jar" "__spark_libs__/ivy-2.4.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3549/log4j-1.2.17.jar" "__spark_libs__/log4j-1.2.17.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3536/spark-unsafe_2.11-2.0.1.jar" "__spark_libs__/spark-unsafe_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3515/scala-reflect-2.11.8.jar" "__spark_libs__/scala-reflect-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3609/jackson-databind-2.6.5.jar" "__spark_libs__/jackson-databind-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3444/htrace-core-3.1.0-incubating.jar" "__spark_libs__/htrace-core-3.1.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3629/jersey-client-2.22.2.jar" "__spark_libs__/jersey-client-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3472/jackson-module-paranamer-2.6.5.jar" "__spark_libs__/jackson-module-paranamer-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3526/mesos-0.21.1-shaded-protobuf.jar" "__spark_libs__/mesos-0.21.1-shaded-protobuf.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3533/stringtemplate-3.2.1.jar" "__spark_libs__/stringtemplate-3.2.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3602/jline-2.12.1.jar" "__spark_libs__/jline-2.12.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3623/parquet-generator-1.7.0.jar" "__spark_libs__/parquet-generator-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3619/mx4j-3.0.2.jar" "__spark_libs__/mx4j-3.0.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3590/curator-framework-2.6.0.jar" "__spark_libs__/curator-framework-2.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3493/hadoop-yarn-server-web-proxy-2.7.1.jar" "__spark_libs__/hadoop-yarn-server-web-proxy-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3512/scala-library-2.11.8.jar" "__spark_libs__/scala-library-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3494/slf4j-log4j12-1.7.16.jar" "__spark_libs__/slf4j-log4j12-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3562/commons-httpclient-3.1.jar" "__spark_libs__/commons-httpclient-3.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3529/hadoop-annotations-2.7.1.jar" "__spark_libs__/hadoop-annotations-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3426/libthrift-0.9.2.jar" "__spark_libs__/libthrift-0.9.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3455/pmml-model-1.2.15.jar" "__spark_libs__/pmml-model-1.2.15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3453/jcl-over-slf4j-1.7.16.jar" "__spark_libs__/jcl-over-slf4j-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3627/spire_2.11-0.7.4.jar" "__spark_libs__/spire_2.11-0.7.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3452/avro-1.7.7.jar" "__spark_libs__/avro-1.7.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3438/hive-cli-1.2.1.spark2.jar" "__spark_libs__/hive-cli-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3456/commons-beanutils-1.7.0.jar" "__spark_libs__/commons-beanutils-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3517/jersey-media-jaxb-2.22.2.jar" "__spark_libs__/jersey-media-jaxb-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3466/JavaEWAH-0.3.2.jar" "__spark_libs__/JavaEWAH-0.3.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3481/jersey-container-servlet-2.22.2.jar" "__spark_libs__/jersey-container-servlet-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3583/oro-2.0.8.jar" "__spark_libs__/oro-2.0.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3521/antlr-runtime-3.4.jar" "__spark_libs__/antlr-runtime-3.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3448/validation-api-1.1.0.Final.jar" "__spark_libs__/validation-api-1.1.0.Final.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3504/hive-exec-1.2.1.spark2.jar" "__spark_libs__/hive-exec-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3597/jackson-xc-1.9.13.jar" "__spark_libs__/jackson-xc-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3550/parquet-format-2.3.0-incubating.jar" "__spark_libs__/parquet-format-2.3.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3457/json4s-core_2.11-3.2.11.jar" "__spark_libs__/json4s-core_2.11-3.2.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3478/jackson-core-asl-1.9.13.jar" "__spark_libs__/jackson-core-asl-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3596/jackson-jaxrs-1.9.13.jar" "__spark_libs__/jackson-jaxrs-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3606/hadoop-mapreduce-client-jobclient-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-jobclient-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3543/jersey-guava-2.22.2.jar" "__spark_libs__/jersey-guava-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3465/mysql-connector-java-5.0.8-bin.jar" "__spark_libs__/mysql-connector-java-5.0.8-bin.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3613/jta-1.1.jar" "__spark_libs__/jta-1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3443/javax.inject-2.4.0-b34.jar" "__spark_libs__/javax.inject-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3433/guice-servlet-3.0.jar" "__spark_libs__/guice-servlet-3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3505/aopalliance-1.0.jar" "__spark_libs__/aopalliance-1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3530/datanucleus-api-jdo-3.2.6.jar" "__spark_libs__/datanucleus-api-jdo-3.2.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3437/jackson-annotations-2.6.5.jar" "__spark_libs__/jackson-annotations-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3595/xz-1.0.jar" "__spark_libs__/xz-1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3547/py4j-0.10.3.jar" "__spark_libs__/py4j-0.10.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3539/jtransforms-2.4.0.jar" "__spark_libs__/jtransforms-2.4.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3553/hadoop-auth-2.7.1.jar" "__spark_libs__/hadoop-auth-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3600/json4s-ast_2.11-3.2.11.jar" "__spark_libs__/json4s-ast_2.11-3.2.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3612/hadoop-yarn-client-2.7.1.jar" "__spark_libs__/hadoop-yarn-client-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3618/spark-hive_2.11-2.0.1.jar" "__spark_libs__/spark-hive_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3577/jsr305-1.3.9.jar" "__spark_libs__/jsr305-1.3.9.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3599/spark-graphx_2.11-2.0.1.jar" "__spark_libs__/spark-graphx_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3541/api-asn1-api-1.0.0-M20.jar" "__spark_libs__/api-asn1-api-1.0.0-M20.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
# Creating copy of launch script
cp "launch_container.sh" "/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000002/launch_container.sh"
chmod 640 "/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000002/launch_container.sh"
# Determining directory contents
echo "ls -l:" 1>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000002/directory.info"
ls -l 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000002/directory.info"
echo "find -L . -maxdepth 5 -ls:" 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000002/directory.info"
find -L . -maxdepth 5 -ls 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000002/directory.info"
echo "broken symlinks(find -L . -maxdepth 5 -type l -ls):" 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000002/directory.info"
find -L . -maxdepth 5 -type l -ls 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000002/directory.info"
exec /bin/bash -c "$JAVA_HOME/bin/java -server -Xmx3072m '-Dhdp.version=2.4.2.0-258' -Djava.io.tmpdir=$PWD/tmp '-Dspark.history.ui.port=18080' '-Dspark.ui.port=0' '-Dspark.driver.port=19210' -Dspark.yarn.app.container.log.dir=/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000002 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.60.43.33:19210 --executor-id 1 --hostname c432.hadoop.gda.lo --cores 1 --app-id application_1494395298335_303847 --user-class-path file:$PWD/__app__.jar 1> /hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000002/stdout 2> /hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000002/stderr"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
End of LogType:launch_container.sh

LogType:stderr
Log Upload Time:Tue May 16 11:12:19 +0700 2017
LogLength:12253
Log Contents:
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/hadoop/yarn/local/filecache/3494/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/hdp/2.4.2.0-258/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
17/05/16 11:10:35 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 55308@c432.hadoop.gda.lo
17/05/16 11:10:35 INFO util.SignalUtils: Registered signal handler for TERM
17/05/16 11:10:35 INFO util.SignalUtils: Registered signal handler for HUP
17/05/16 11:10:35 INFO util.SignalUtils: Registered signal handler for INT
17/05/16 11:10:36 INFO spark.SecurityManager: Changing view acls to: yarn,fairy
17/05/16 11:10:36 INFO spark.SecurityManager: Changing modify acls to: yarn,fairy
17/05/16 11:10:36 INFO spark.SecurityManager: Changing view acls groups to: 
17/05/16 11:10:36 INFO spark.SecurityManager: Changing modify acls groups to: 
17/05/16 11:10:36 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(yarn, fairy); groups with view permissions: Set(); users  with modify permissions: Set(yarn, fairy); groups with modify permissions: Set()
17/05/16 11:10:37 INFO client.TransportClientFactory: Successfully created connection to /10.60.43.33:19210 after 108 ms (0 ms spent in bootstraps)
17/05/16 11:10:37 INFO spark.SecurityManager: Changing view acls to: yarn,fairy
17/05/16 11:10:37 INFO spark.SecurityManager: Changing modify acls to: yarn,fairy
17/05/16 11:10:37 INFO spark.SecurityManager: Changing view acls groups to: 
17/05/16 11:10:37 INFO spark.SecurityManager: Changing modify acls groups to: 
17/05/16 11:10:37 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(yarn, fairy); groups with view permissions: Set(); users  with modify permissions: Set(yarn, fairy); groups with modify permissions: Set()
17/05/16 11:10:37 INFO client.TransportClientFactory: Successfully created connection to /10.60.43.33:19210 after 2 ms (0 ms spent in bootstraps)
17/05/16 11:10:37 INFO storage.DiskBlockManager: Created local directory at /hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/blockmgr-2274a80a-9952-42f8-b90d-41d7aec6245a
17/05/16 11:10:37 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MB
17/05/16 11:10:37 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@10.60.43.33:19210
17/05/16 11:10:37 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver
17/05/16 11:10:37 INFO executor.Executor: Starting executor ID 1 on host c432.hadoop.gda.lo
17/05/16 11:10:38 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 14031.
17/05/16 11:10:38 INFO netty.NettyBlockTransferService: Server created on c432.hadoop.gda.lo:14031
17/05/16 11:10:38 INFO storage.BlockManager: external shuffle service port = 7337
17/05/16 11:10:38 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(1, c432.hadoop.gda.lo, 14031)
17/05/16 11:10:38 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(1, c432.hadoop.gda.lo, 14031)
17/05/16 11:10:38 INFO storage.BlockManager: Registering executor with local external shuffle service.
17/05/16 11:10:38 INFO client.TransportClientFactory: Successfully created connection to c432.hadoop.gda.lo/10.60.43.32:7337 after 2 ms (0 ms spent in bootstraps)
17/05/16 11:10:51 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 3
17/05/16 11:10:51 INFO executor.Executor: Running task 3.0 in stage 1.0 (TID 3)
17/05/16 11:10:51 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 0
17/05/16 11:10:51 INFO client.TransportClientFactory: Successfully created connection to /10.60.43.33:5141 after 3 ms (0 ms spent in bootstraps)
17/05/16 11:10:51 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.2 KB, free 1458.6 MB)
17/05/16 11:10:51 INFO broadcast.TorrentBroadcast: Reading broadcast variable 0 took 237 ms
17/05/16 11:10:51 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 30.3 KB, free 1458.6 MB)
17/05/16 11:10:52 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
17/05/16 11:10:52 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.60.43.33:19210)
17/05/16 11:10:52 INFO spark.MapOutputTrackerWorker: Got the output locations
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
17/05/16 11:10:52 INFO codegen.CodeGenerator: Code generated in 524.467831 ms
17/05/16 11:10:52 INFO codegen.CodeGenerator: Code generated in 21.272935 ms
17/05/16 11:10:52 INFO codegen.CodeGenerator: Code generated in 21.207213 ms
17/05/16 11:10:52 INFO codegen.CodeGenerator: Code generated in 18.643098 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 3.0 in stage 1.0 (TID 3). 4219 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 99
17/05/16 11:10:53 INFO executor.Executor: Running task 99.0 in stage 1.0 (TID 99)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 99.0 in stage 1.0 (TID 99). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 110
17/05/16 11:10:53 INFO executor.Executor: Running task 110.0 in stage 1.0 (TID 110)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 110.0 in stage 1.0 (TID 110). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 118
17/05/16 11:10:53 INFO executor.Executor: Running task 118.0 in stage 1.0 (TID 118)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 118.0 in stage 1.0 (TID 118). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 125
17/05/16 11:10:53 INFO executor.Executor: Running task 125.0 in stage 1.0 (TID 125)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 125.0 in stage 1.0 (TID 125). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 132
17/05/16 11:10:53 INFO executor.Executor: Running task 132.0 in stage 1.0 (TID 132)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 132.0 in stage 1.0 (TID 132). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 139
17/05/16 11:10:53 INFO executor.Executor: Running task 139.0 in stage 1.0 (TID 139)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 139.0 in stage 1.0 (TID 139). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 145
17/05/16 11:10:53 INFO executor.Executor: Running task 145.0 in stage 1.0 (TID 145)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 145.0 in stage 1.0 (TID 145). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 150
17/05/16 11:10:53 INFO executor.Executor: Running task 150.0 in stage 1.0 (TID 150)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 150.0 in stage 1.0 (TID 150). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 156
17/05/16 11:10:53 INFO executor.Executor: Running task 156.0 in stage 1.0 (TID 156)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 156.0 in stage 1.0 (TID 156). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 161
17/05/16 11:10:53 INFO executor.Executor: Running task 161.0 in stage 1.0 (TID 161)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 161.0 in stage 1.0 (TID 161). 3414 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 165
17/05/16 11:10:53 INFO executor.Executor: Running task 165.0 in stage 1.0 (TID 165)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 165.0 in stage 1.0 (TID 165). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 172
17/05/16 11:10:53 INFO executor.Executor: Running task 172.0 in stage 1.0 (TID 172)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 172.0 in stage 1.0 (TID 172). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 195
17/05/16 11:10:53 INFO executor.Executor: Running task 195.0 in stage 1.0 (TID 195)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 195.0 in stage 1.0 (TID 195). 3327 bytes result sent to driver
17/05/16 11:10:54 INFO executor.CoarseGrainedExecutorBackend: Driver commanded a shutdown
17/05/16 11:10:54 INFO executor.CoarseGrainedExecutorBackend: Driver from 10.60.43.33:19210 disconnected during shutdown
17/05/16 11:10:54 INFO executor.CoarseGrainedExecutorBackend: Driver from 10.60.43.33:19210 disconnected during shutdown
17/05/16 11:10:54 INFO memory.MemoryStore: MemoryStore cleared
17/05/16 11:10:54 INFO storage.BlockManager: BlockManager stopped
17/05/16 11:10:54 INFO util.ShutdownHookManager: Shutdown hook called
End of LogType:stderr

LogType:stdout
Log Upload Time:Tue May 16 11:12:19 +0700 2017
LogLength:0
Log Contents:
End of LogType:stdout



Container: container_e81_1494395298335_303847_01_000001 on c433.hadoop.gda.lo_45454
=====================================================================================
LogType:directory.info
Log Upload Time:Tue May 16 11:12:21 +0700 2017
LogLength:28674
Log Contents:
ls -l:
total 112
lrwxrwxrwx 1 yarn hadoop    69 May 16 11:10 __app__.jar -> /hadoop/yarn/local/usercache/fairy/filecache/11521/stats-etlr-1.0.jar
-rw-r--r-- 1 yarn hadoop    71 May 16 11:10 container_tokens
-rwx------ 1 yarn hadoop   676 May 16 11:10 default_container_executor_session.sh
-rwx------ 1 yarn hadoop   730 May 16 11:10 default_container_executor.sh
-rwx------ 1 yarn hadoop 75361 May 16 11:10 launch_container.sh
lrwxrwxrwx 1 yarn hadoop    69 May 16 11:10 __spark_conf__ -> /hadoop/yarn/local/usercache/fairy/filecache/11522/__spark_conf__.zip
drwxr-xr-x 2 yarn hadoop 12288 May 16 11:10 __spark_libs__
drwx--x--- 2 yarn hadoop  4096 May 16 11:10 tmp
find -L . -maxdepth 5 -ls:
15861115    4 drwx--x---   4 yarn     hadoop       4096 May 16 11:10 .
15861118 1152 -r-x------   1 yarn     hadoop    1178454 May 16 11:10 ./__app__.jar
15861321    4 -rw-r--r--   1 yarn     hadoop         16 May 16 11:10 ./.default_container_executor_session.sh.crc
15861318    4 -rw-r--r--   1 yarn     hadoop        600 May 16 11:10 ./.launch_container.sh.crc
15861323    4 -rw-r--r--   1 yarn     hadoop         16 May 16 11:10 ./.default_container_executor.sh.crc
15861124    4 drwx--x---   2 yarn     hadoop       4096 May 16 11:10 ./tmp
15861320    4 -rwx------   1 yarn     hadoop        676 May 16 11:10 ./default_container_executor_session.sh
15861322    4 -rwx------   1 yarn     hadoop        730 May 16 11:10 ./default_container_executor.sh
15861316    4 -rw-r--r--   1 yarn     hadoop         12 May 16 11:10 ./.container_tokens.crc
15861128    4 -rw-r--r--   1 yarn     hadoop         71 May 16 11:10 ./container_tokens
15861317   76 -rwx------   1 yarn     hadoop      75361 May 16 11:10 ./launch_container.sh
15861324   12 drwxr-xr-x   2 yarn     hadoop      12288 May 16 11:10 ./__spark_libs__
5243081  136 -rwxr-xr-x   1 yarn     hadoop     135552 Mar 28 23:40 ./__spark_libs__/breeze-macros_2.11-0.11.2.jar
5243085 1848 -rwxr-xr-x   1 yarn     hadoop    1890075 Mar 28 23:40 ./__spark_libs__/datanucleus-core-3.2.10.jar
5243820 4468 -rwxr-xr-x   1 yarn     hadoop    4573750 Apr  4 16:10 ./__spark_libs__/scala-reflect-2.11.8.jar
6292483  136 -rwxr-xr-x   1 yarn     hadoop     138464 Apr  5 13:43 ./__spark_libs__/hive-beeline-1.2.1.spark2.jar
5243093  280 -rwxr-xr-x   1 yarn     hadoop     284184 Mar 28 23:40 ./__spark_libs__/commons-codec-1.10.jar
5243129    4 -rwxr-xr-x   1 yarn     hadoop       2497 Mar 28 23:40 ./__spark_libs__/javax.inject-1.jar
6292774  172 -rwxr-xr-x   1 yarn     hadoop     174351 Apr  5 13:43 ./__spark_libs__/stream-2.7.0.jar
6292609   12 -rwxr-xr-x   1 yarn     hadoop      10023 Apr  5 13:43 ./__spark_libs__/java-xmlbuilder-1.0.jar
6292272  748 -rwxr-xr-x   1 yarn     hadoop     764569 Apr  5 13:43 ./__spark_libs__/jtransforms-2.4.0.jar
5243017 2732 -rwxr-xr-x   1 yarn     hadoop    2796935 Mar 28 23:40 ./__spark_libs__/parquet-hadoop-bundle-1.6.0.jar
6292648   40 -rwxr-xr-x   1 yarn     hadoop      38134 Apr  5 13:43 ./__spark_libs__/hadoop-mapreduce-client-jobclient-2.7.1.jar
6292301  480 -rwxr-xr-x   1 yarn     hadoop     489884 Apr  5 13:43 ./__spark_libs__/log4j-1.2.17.jar
5242956  164 -rwxr-xr-x   1 yarn     hadoop     164422 Mar 28 23:40 ./__spark_libs__/core-1.1.2.jar
6292331   24 -rwxr-xr-x   1 yarn     hadoop      21575 Apr  5 13:43 ./__spark_libs__/parquet-common-1.7.0.jar
5243822  236 -rwxr-xr-x   1 yarn     hadoop     241367 Apr  4 16:10 ./__spark_libs__/commons-compress-1.4.1.jar
5243012 1988 -rwxr-xr-x   1 yarn     hadoop    2035066 Mar 28 23:40 ./__spark_libs__/commons-math3-3.4.1.jar
6292544   32 -rwxr-xr-x   1 yarn     hadoop      30595 Apr  5 13:43 ./__spark_libs__/commons-compiler-2.7.6.jar
5243135   80 -rwxr-xr-x   1 yarn     hadoop      79845 Mar 28 23:40 ./__spark_libs__/compress-lzf-1.0.3.jar
5243840  524 -rwxr-xr-x   1 yarn     hadoop     533455 Apr  4 16:10 ./__spark_libs__/protobuf-java-2.5.0.jar
6292196  176 -rwxr-xr-x   1 yarn     hadoop     177131 Apr  5 13:43 ./__spark_libs__/jetty-util-6.1.26.jar
5243027   20 -rwxr-xr-x   1 yarn     hadoop      16993 Mar 28 23:40 ./__spark_libs__/JavaEWAH-0.3.2.jar
6292184  180 -rwxr-xr-x   1 yarn     hadoop     180736 Apr  5 13:43 ./__spark_libs__/avro-mapred-1.7.7-hadoop2.jar
5242976 8068 -rwxr-xr-x   1 yarn     hadoop    8260573 Mar 28 23:40 ./__spark_libs__/hadoop-hdfs-2.7.1.jar
5243144 11232 -rwxr-xr-x   1 yarn     hadoop   11498852 Mar 28 23:40 ./__spark_libs__/hive-exec-1.2.1.spark2.jar
5243831 6716 -rwxr-xr-x   1 yarn     hadoop    6873892 Apr  4 16:10 ./__spark_libs__/spark-catalyst_2.11-2.0.1.jar
5242958 1444 -rwxr-xr-x   1 yarn     hadoop    1475955 Mar 28 23:40 ./__spark_libs__/htrace-core-3.1.0-incubating.jar
6292618  700 -rwxr-xr-x   1 yarn     hadoop     714194 Apr  5 13:43 ./__spark_libs__/javassist-3.18.1-GA.jar
6292739  200 -rwxr-xr-x   1 yarn     hadoop     201928 Apr  5 13:43 ./__spark_libs__/RoaringBitmap-0.5.11.jar
5243111   32 -rwxr-xr-x   1 yarn     hadoop      32145 Mar 28 23:40 ./__spark_libs__/hadoop-yarn-server-web-proxy-2.7.1.jar
5243057  176 -rwxr-xr-x   1 yarn     hadoop     178947 Mar 28 23:40 ./__spark_libs__/hk2-api-2.4.0-b34.jar
6292507  208 -rwxr-xr-x   1 yarn     hadoop     212453 Apr  5 13:43 ./__spark_libs__/commons-net-2.2.jar
5243000  176 -rwxr-xr-x   1 yarn     hadoop     177832 Mar 28 23:40 ./__spark_libs__/spark-mllib-local_2.11-2.0.1.jar
6292732   16 -rwxr-xr-x   1 yarn     hadoop      15071 Apr  5 13:43 ./__spark_libs__/jta-1.1.jar
5243147    8 -rwxr-xr-x   1 yarn     hadoop       4467 Mar 28 23:40 ./__spark_libs__/aopalliance-1.0.jar
5243066  228 -rwxr-xr-x   1 yarn     hadoop     232248 Mar 28 23:40 ./__spark_libs__/jackson-core-asl-1.9.13.jar
5242994  640 -rwxr-xr-x   1 yarn     hadoop     654216 Mar 28 23:40 ./__spark_libs__/pmml-model-1.2.15.jar
6292677 7108 -rwxr-xr-x   1 yarn     hadoop    7276083 Apr  5 13:43 ./__spark_libs__/spire_2.11-0.7.4.jar
5243852 2780 -rwxr-xr-x   1 yarn     hadoop    2842667 Apr  4 16:10 ./__spark_libs__/bcprov-jdk15on-1.51.jar
6292250  148 -rwxr-xr-x   1 yarn     hadoop     148627 Apr  5 13:43 ./__spark_libs__/stringtemplate-3.2.1.jar
6292379 11452 -rwxr-xr-x   1 yarn     hadoop   11723537 Apr  5 13:43 ./__spark_libs__/spark-core_2.11-2.0.1.jar
5243004   68 -rwxr-xr-x   1 yarn     hadoop      66270 Mar 28 23:40 ./__spark_libs__/jersey-container-servlet-core-2.22.2.jar
6292401   28 -rwxr-xr-x   1 yarn     hadoop      26366 Apr  5 13:43 ./__spark_libs__/javax.annotation-api-1.2.jar
6292290  116 -rwxr-xr-x   1 yarn     hadoop     115534 Apr  5 13:43 ./__spark_libs__/javax.ws.rs-api-2.0.1.jar
6292706 1144 -rwxr-xr-x   1 yarn     hadoop    1171380 Apr  5 13:43 ./__spark_libs__/jackson-databind-2.6.5.jar
5242952   96 -rwxr-xr-x   1 yarn     hadoop      95806 Mar 28 23:40 ./__spark_libs__/javax.servlet-api-3.1.0.jar
6292747  120 -rwxr-xr-x   1 yarn     hadoop     118973 Apr  5 13:43 ./__spark_libs__/hk2-utils-2.4.0-b34.jar
6292212   92 -rwxr-xr-x   1 yarn     hadoop      93210 Apr  5 13:43 ./__spark_libs__/super-csv-2.2.0.jar
6292653 1024 -rwxr-xr-x   1 yarn     hadoop    1048110 Apr  5 13:43 ./__spark_libs__/parquet-jackson-1.7.0.jar
6292218   64 -rwxr-xr-x   1 yarn     hadoop      65012 Apr  5 13:43 ./__spark_libs__/guice-servlet-3.0.jar
6292280 1616 -rwxr-xr-x   1 yarn     hadoop    1654097 Apr  5 13:43 ./__spark_libs__/hadoop-yarn-common-2.7.1.jar
5245267 1248 -rwxr-xr-x   1 yarn     hadoop    1277883 Apr  4 16:10 ./__spark_libs__/mesos-0.21.1-shaded-protobuf.jar
5243024  532 -rwxr-xr-x   1 yarn     hadoop     540852 Mar 28 23:40 ./__spark_libs__/mysql-connector-java-5.0.8-bin.jar
6292753 1772 -rwxr-xr-x   1 yarn     hadoop    1814309 Apr  5 13:43 ./__spark_libs__/spark-hive-thriftserver_2.11-2.0.1.jar
6292349  676 -rwxr-xr-x   1 yarn     hadoop     691479 Apr  5 13:43 ./__spark_libs__/apacheds-kerberos-codec-2.0.0-M15.jar
5242967   44 -rwxr-xr-x   1 yarn     hadoop      41755 Mar 28 23:40 ./__spark_libs__/objenesis-2.1.jar
5243099  656 -rwxr-xr-x   1 yarn     hadoop     669589 Mar 28 23:40 ./__spark_libs__/spark-yarn_2.11-2.0.1.jar
6292327  404 -rwxr-xr-x   1 yarn     hadoop     412739 Apr  5 13:43 ./__spark_libs__/commons-lang3-3.3.2.jar
6292750 1024 -rwxr-xr-x   1 yarn     hadoop    1044511 Apr  5 13:43 ./__spark_libs__/spark-hive_2.11-2.0.1.jar
6292658   96 -rwxr-xr-x   1 yarn     hadoop      94672 Apr  5 13:43 ./__spark_libs__/xz-1.0.jar
5243816 1512 -rwxr-xr-x   1 yarn     hadoop    1544875 Apr  4 16:10 ./__spark_libs__/hadoop-mapreduce-client-core-2.7.1.jar
5242988   20 -rwxr-xr-x   1 yarn     hadoop      16430 Mar 28 23:40 ./__spark_libs__/jcl-over-slf4j-1.7.16.jar
5243139 1024 -rwxr-xr-x   1 yarn     hadoop    1045744 Mar 28 23:40 ./__spark_libs__/leveldbjni-all-1.8.jar
5245277   20 -rwxr-xr-x   1 yarn     hadoop      17385 Apr  4 16:10 ./__spark_libs__/hadoop-annotations-2.7.1.jar
6292262   44 -rwxr-xr-x   1 yarn     hadoop      41070 Apr  5 13:43 ./__spark_libs__/spark-unsafe_2.11-2.0.1.jar
5242998  256 -rwxr-xr-x   1 yarn     hadoop     258876 Mar 28 23:40 ./__spark_libs__/jackson-core-2.6.5.jar
5243076   20 -rwxr-xr-x   1 yarn     hadoop      18098 Mar 28 23:40 ./__spark_libs__/jersey-container-servlet-2.22.2.jar
6292194  204 -rwxr-xr-x   1 yarn     hadoop     206035 Apr  5 13:43 ./__spark_libs__/commons-beanutils-core-1.8.0.jar
6292736  388 -rwxr-xr-x   1 yarn     hadoop     395195 Apr  5 13:43 ./__spark_libs__/javolution-5.5.1.jar
6292672   84 -rwxr-xr-x   1 yarn     hadoop      82421 Apr  5 13:43 ./__spark_libs__/json4s-ast_2.11-3.2.11.jar
6292644   64 -rwxr-xr-x   1 yarn     hadoop      62050 Apr  5 13:43 ./__spark_libs__/commons-logging-1.1.3.jar
5243072  144 -rwxr-xr-x   1 yarn     hadoop     143602 Mar 28 23:40 ./__spark_libs__/commons-digester-1.8.jar
5243061  788 -rwxr-xr-x   1 yarn     hadoop     802818 Mar 28 23:40 ./__spark_libs__/scalap-2.11.8.jar
6292804  736 -rwxr-xr-x   1 yarn     hadoop     753012 Apr  5 13:43 ./__spark_libs__/hadoop-mapreduce-client-common-2.7.1.jar
6292303   12 -rwxr-xr-x   1 yarn     hadoop      12131 Apr  5 13:43 ./__spark_libs__/jpam-1.1.jar
5243123   52 -rwxr-xr-x   1 yarn     hadoop      50619 Mar 28 23:40 ./__spark_libs__/chill-java-0.8.0.jar
5245269 1204 -rwxr-xr-x   1 yarn     hadoop    1230201 Apr  4 16:10 ./__spark_libs__/netty-3.8.0.Final.jar
5245264   88 -rwxr-xr-x   1 yarn     hadoop      86811 Apr  4 16:10 ./__spark_libs__/spire-macros_2.11-0.7.4.jar
5243042   44 -rwxr-xr-x   1 yarn     hadoop      41263 Mar 28 23:40 ./__spark_libs__/jackson-module-paranamer-2.6.5.jar
5243155  180 -rwxr-xr-x   1 yarn     hadoop     181271 Mar 28 23:40 ./__spark_libs__/hk2-locator-2.4.0-b34.jar
6292556 3152 -rwxr-xr-x   1 yarn     hadoop    3224708 Apr  5 13:43 ./__spark_libs__/derby-10.12.1.1.jar
6292236   40 -rwxr-xr-x   1 yarn     hadoop      40817 Apr  5 13:43 ./__spark_libs__/hive-cli-1.2.1.spark2.jar
6292559 1768 -rwxr-xr-x   1 yarn     hadoop    1809447 Apr  5 13:43 ./__spark_libs__/datanucleus-rdbms-3.2.9.jar
5243836   32 -rwxr-xr-x   1 yarn     hadoop      29540 Apr  4 16:10 ./__spark_libs__/spark-sketch_2.11-2.0.1.jar
6292623  184 -rwxr-xr-x   1 yarn     hadoop     185245 Apr  5 13:43 ./__spark_libs__/curator-framework-2.6.0.jar
6292780  308 -rwxr-xr-x   1 yarn     hadoop     313686 Apr  5 13:43 ./__spark_libs__/libfb303-0.9.2.jar
6292329  576 -rwxr-xr-x   1 yarn     hadoop     588337 Apr  5 13:43 ./__spark_libs__/commons-collections-3.2.2.jar
6292689 1168 -rwxr-xr-x   1 yarn     hadoop    1194003 Apr  5 13:43 ./__spark_libs__/arpack_combined_all-0.1.jar
5243106 13136 -rwxr-xr-x   1 yarn     hadoop   13448966 Mar 28 23:40 ./__spark_libs__/breeze_2.11-0.11.2.jar
5243132  208 -rwxr-xr-x   1 yarn     hadoop     209622 Mar 28 23:40 ./__spark_libs__/parquet-hadoop-1.7.0.jar
6292178   16 -rwxr-xr-x   1 yarn     hadoop      15827 Apr  5 13:43 ./__spark_libs__/metrics-json-3.1.2.jar
6292669  668 -rwxr-xr-x   1 yarn     hadoop     680106 Apr  5 13:43 ./__spark_libs__/spark-graphx_2.11-2.0.1.jar
5243150   20 -rwxr-xr-x   1 yarn     hadoop      17008 Mar 28 23:40 ./__spark_libs__/base64-2.3.8.jar
5242982 15128 -rwxr-xr-x   1 yarn     hadoop   15487351 Mar 28 23:40 ./__spark_libs__/scala-compiler-2.11.8.jar
6292660   20 -rwxr-xr-x   1 yarn     hadoop      18336 Apr  5 13:43 ./__spark_libs__/jackson-jaxrs-1.9.13.jar
6292246  280 -rwxr-xr-x   1 yarn     hadoop     284220 Apr  5 13:43 ./__spark_libs__/commons-lang-2.6.jar
6292363 5712 -rwxr-xr-x   1 yarn     hadoop    5847591 Apr  5 13:43 ./__spark_libs__/spark-mllib_2.11-2.0.1.jar
5243167  720 -rwxr-xr-x   1 yarn     hadoop     736658 Mar 28 23:40 ./__spark_libs__/httpclient-4.5.2.jar
5243141  232 -rwxr-xr-x   1 yarn     hadoop     236660 Mar 28 23:40 ./__spark_libs__/ST4-4.0.4.jar
6292675 1972 -rwxr-xr-x   1 yarn     hadoop    2015514 Apr  5 13:43 ./__spark_libs__/hadoop-yarn-api-2.7.1.jar
5243814 5612 -rwxr-xr-x   1 yarn     hadoop    5744974 Apr  4 16:10 ./__spark_libs__/scala-library-2.11.8.jar
6292368  300 -rwxr-xr-x   1 yarn     hadoop     305001 Apr  5 13:43 ./__spark_libs__/commons-httpclient-3.1.jar
5243003  576 -rwxr-xr-x   1 yarn     hadoop     589462 Mar 28 23:40 ./__spark_libs__/json4s-core_2.11-3.2.11.jar
5243019  188 -rwxr-xr-x   1 yarn     hadoop     190432 Mar 28 23:40 ./__spark_libs__/gson-2.2.4.jar
5243045  244 -rwxr-xr-x   1 yarn     hadoop     248171 Mar 28 23:40 ./__spark_libs__/curator-recipes-2.6.0.jar
6292616  696 -rwxr-xr-x   1 yarn     hadoop     710492 Apr  5 13:43 ./__spark_libs__/guice-3.0.jar
5242897  636 -rwxr-xr-x   1 yarn     hadoop     648678 Mar 28 23:40 ./__spark_libs__/scala-xml_2.11-1.0.2.jar
6292253  100 -rwxr-xr-x   1 yarn     hadoop     100636 Apr  5 13:43 ./__spark_libs__/jsp-api-2.1.jar
5243068   20 -rwxr-xr-x   1 yarn     hadoop      19827 Mar 28 23:40 ./__spark_libs__/opencsv-2.3.jar
6292587  616 -rwxr-xr-x   1 yarn     hadoop     627814 Apr  5 13:43 ./__spark_libs__/joda-time-2.9.3.jar
5243156  528 -rwxr-xr-x   1 yarn     hadoop     539912 Mar 28 23:40 ./__spark_libs__/jetty-6.1.26.jar
6292646  512 -rwxr-xr-x   1 yarn     hadoop     521157 Apr  5 13:43 ./__spark_libs__/mail-1.4.7.jar
6292374  200 -rwxr-xr-x   1 yarn     hadoop     201124 Apr  5 13:43 ./__spark_libs__/jdo-api-3.0.1.jar
5243108   68 -rwxr-xr-x   1 yarn     hadoop      65653 Mar 28 23:40 ./__spark_libs__/spark-launcher_2.11-2.0.1.jar
6292808    8 -rwxr-xr-x   1 yarn     hadoop       5310 Apr  5 13:43 ./__spark_libs__/pmml-schema-1.2.15.jar
5245273   68 -rwxr-xr-x   1 yarn     hadoop      68866 Apr  4 16:10 ./__spark_libs__/curator-client-2.6.0.jar
5243162  436 -rwxr-xr-x   1 yarn     hadoop     442406 Mar 28 23:40 ./__spark_libs__/calcite-linq4j-1.2.0-incubating.jar
5244220  164 -rwxr-xr-x   1 yarn     hadoop     164368 Apr  4 16:10 ./__spark_libs__/antlr-runtime-3.4.jar
6292243   16 -rwxr-xr-x   1 yarn     hadoop      14766 Apr  5 13:43 ./__spark_libs__/aopalliance-repackaged-2.4.0-b34.jar
5243022   12 -rwxr-xr-x   1 yarn     hadoop       9939 Mar 28 23:40 ./__spark_libs__/slf4j-log4j12-1.7.16.jar
6292526 6144 -rwxr-xr-x   1 yarn     hadoop    6290315 Apr  5 13:43 ./__spark_libs__/spark-sql_2.11-2.0.1.jar
6292295  116 -rwxr-xr-x   1 yarn     hadoop     114913 Apr  5 13:43 ./__spark_libs__/py4j-0.10.3.jar
5242921    4 -rwxr-xr-x   1 yarn     hadoop       2545 Mar 28 23:40 ./__spark_libs__/hadoop-client-2.7.1.jar
6292305  380 -rwxr-xr-x   1 yarn     hadoop     387188 Apr  5 13:43 ./__spark_libs__/parquet-format-2.3.0-incubating.jar
6292462 1204 -rwxr-xr-x   1 yarn     hadoop    1229125 Apr  5 13:43 ./__spark_libs__/xercesImpl-2.9.1.jar
6292679  212 -rwxr-xr-x   1 yarn     hadoop     213911 Apr  5 13:43 ./__spark_libs__/jline-2.12.1.jar
6292599   16 -rwxr-xr-x   1 yarn     hadoop      15010 Apr  5 13:43 ./__spark_libs__/xmlenc-0.52.jar
5243048 1996 -rwxr-xr-x   1 yarn     hadoop    2041628 Mar 28 23:40 ./__spark_libs__/jets3t-0.9.3.jar
5243035   64 -rwxr-xr-x   1 yarn     hadoop      63316 Mar 28 23:40 ./__spark_libs__/spark-repl_2.11-2.0.1.jar
6292700 2140 -rwxr-xr-x   1 yarn     hadoop    2189117 Apr  5 13:43 ./__spark_libs__/guava-14.0.1.jar
6292614  352 -rwxr-xr-x   1 yarn     hadoop     358390 Apr  5 13:43 ./__spark_libs__/kryo-shaded-3.0.3.jar
6292180  224 -rwxr-xr-x   1 yarn     hadoop     227712 Apr  5 13:43 ./__spark_libs__/libthrift-0.9.2.jar
6292666   44 -rwxr-xr-x   1 yarn     hadoop      45015 Apr  5 13:43 ./__spark_libs__/hadoop-mapreduce-client-shuffle-2.7.1.jar
5243009  144 -rwxr-xr-x   1 yarn     hadoop     144660 Mar 28 23:40 ./__spark_libs__/xbean-asm5-shaded-4.4.jar
6292287   24 -rwxr-xr-x   1 yarn     hadoop      23346 Apr  5 13:43 ./__spark_libs__/stax-api-1.0-2.jar
6292229   48 -rwxr-xr-x   1 yarn     hadoop      46983 Apr  5 13:43 ./__spark_libs__/jackson-annotations-2.6.5.jar
5243120  112 -rwxr-xr-x   1 yarn     hadoop     110600 Mar 28 23:40 ./__spark_libs__/bonecp-0.8.0.RELEASE.jar
6292663   28 -rwxr-xr-x   1 yarn     hadoop      27084 Apr  5 13:43 ./__spark_libs__/jackson-xc-1.9.13.jar
5243077  600 -rwxr-xr-x   1 yarn     hadoop     613299 Mar 28 23:40 ./__spark_libs__/janino-2.7.8.jar
6292268  280 -rwxr-xr-x   1 yarn     hadoop     285447 Apr  5 13:43 ./__spark_libs__/parquet-encoding-1.7.0.jar
6292221   44 -rwxr-xr-x   1 yarn     hadoop      44925 Apr  5 13:43 ./__spark_libs__/apacheds-i18n-2.0.0-M15.jar
6292264  416 -rwxr-xr-x   1 yarn     hadoop     423753 Apr  5 13:43 ./__spark_libs__/scala-parser-combinators_2.11-1.0.4.jar
5243834   72 -rwxr-xr-x   1 yarn     hadoop      72733 Apr  4 16:10 ./__spark_libs__/jersey-media-jaxb-2.22.2.jar
6292377    8 -rwxr-xr-x   1 yarn     hadoop       4596 Apr  5 13:43 ./__spark_libs__/jul-to-slf4j-1.7.16.jar
5243054  284 -rwxr-xr-x   1 yarn     hadoop     290506 Mar 28 23:40 ./__spark_libs__/univocity-parsers-2.1.1.jar
5242960   96 -rwxr-xr-x   1 yarn     hadoop      96221 Mar 28 23:40 ./__spark_libs__/commons-pool-1.5.4.jar
6292620   64 -rwxr-xr-x   1 yarn     hadoop      65261 Apr  5 13:43 ./__spark_libs__/oro-2.0.8.jar
6292548 2304 -rwxr-xr-x   1 yarn     hadoop    2355465 Apr  5 13:43 ./__spark_libs__/spark-network-common_2.11-2.0.1.jar
6292340  356 -rwxr-xr-x   1 yarn     hadoop     363908 Apr  5 13:43 ./__spark_libs__/hadoop-yarn-server-common-2.7.1.jar
6292324   72 -rwxr-xr-x   1 yarn     hadoop      70688 Apr  5 13:43 ./__spark_libs__/hadoop-auth-2.7.1.jar
6292275  684 -rwxr-xr-x   1 yarn     hadoop     698375 Apr  5 13:43 ./__spark_libs__/jersey-common-2.22.2.jar
5242950  188 -rwxr-xr-x   1 yarn     hadoop     188671 Mar 28 23:40 ./__spark_libs__/commons-beanutils-1.7.0.jar
6292794 2008 -rwxr-xr-x   1 yarn     hadoop    2054931 Apr  5 13:43 ./__spark_libs__/netty-all-4.0.29.Final.jar
5243015   40 -rwxr-xr-x   1 yarn     hadoop      39280 Mar 28 23:40 ./__spark_libs__/metrics-jvm-3.1.2.jar
6292713   68 -rwxr-xr-x   1 yarn     hadoop      69409 Apr  5 13:43 ./__spark_libs__/activation-1.1.1.jar
6292283   48 -rwxr-xr-x   1 yarn     hadoop      48720 Apr  5 13:43 ./__spark_libs__/snappy-0.2.jar
6292798  164 -rwxr-xr-x   1 yarn     hadoop     167421 Apr  5 13:43 ./__spark_libs__/jersey-client-2.22.2.jar
6292636   20 -rwxr-xr-x   1 yarn     hadoop      20235 Apr  5 13:43 ./__spark_libs__/osgi-resource-locator-1.0.1.jar
6292299    8 -rwxr-xr-x   1 yarn     hadoop       5711 Apr  5 13:43 ./__spark_libs__/minlog-1.3.0.jar
5243854  436 -rwxr-xr-x   1 yarn     hadoop     445288 Apr  4 16:10 ./__spark_libs__/antlr-2.7.7.jar
6292682 3352 -rwxr-xr-x   1 yarn     hadoop    3431544 Apr  5 13:43 ./__spark_libs__/hadoop-common-2.7.1.jar
5243126 1592 -rwxr-xr-x   1 yarn     hadoop    1627065 Mar 28 23:40 ./__spark_libs__/jersey-bundle-1.19.1.jar
5243051  160 -rwxr-xr-x   1 yarn     hadoop     160519 Mar 28 23:40 ./__spark_libs__/commons-dbcp-1.4.jar
6292529   80 -rwxr-xr-x   1 yarn     hadoop      79912 Apr  5 13:43 ./__spark_libs__/api-util-1.0.0-M20.jar
6292766   44 -rwxr-xr-x   1 yarn     hadoop      41123 Apr  5 13:43 ./__spark_libs__/commons-cli-1.2.jar
6292634  504 -rwxr-xr-x   1 yarn     hadoop     515604 Apr  5 13:43 ./__spark_libs__/jackson-module-scala_2.11-2.6.5.jar
5242985  428 -rwxr-xr-x   1 yarn     hadoop     436303 Mar 28 23:40 ./__spark_libs__/avro-1.7.7.jar
5242965   64 -rwxr-xr-x   1 yarn     hadoop      63777 Mar 28 23:40 ./__spark_libs__/validation-api-1.1.0.Final.jar
5243114 1256 -rwxr-xr-x   1 yarn     hadoop    1282424 Mar 28 23:40 ./__spark_libs__/ivy-2.4.0.jar
5242973   40 -rwxr-xr-x   1 yarn     hadoop      40341 Mar 28 23:40 ./__spark_libs__/json4s-jackson_2.11-3.2.11.jar
5245288  332 -rwxr-xr-x   1 yarn     hadoop     339666 Apr  4 16:10 ./__spark_libs__/datanucleus-api-jdo-3.2.6.jar
6292214 3440 -rwxr-xr-x   1 yarn     hadoop    3519262 Apr  5 13:43 ./__spark_libs__/calcite-core-1.2.0-incubating.jar
5242963   16 -rwxr-xr-x   1 yarn     hadoop      15305 Mar 28 23:40 ./__spark_libs__/spark-tags_2.11-2.0.1.jar
5243007   32 -rwxr-xr-x   1 yarn     hadoop      29555 Mar 28 23:40 ./__spark_libs__/paranamer-2.3.jar
6292755  400 -rwxr-xr-x   1 yarn     hadoop     409467 Apr  5 13:43 ./__spark_libs__/mx4j-3.0.2.jar
5243095   40 -rwxr-xr-x   1 yarn     hadoop      40509 Mar 28 23:40 ./__spark_libs__/slf4j-api-1.7.16.jar
5242975 2044 -rwxr-xr-x   1 yarn     hadoop    2090370 Mar 28 23:40 ./__spark_libs__/spark-streaming_2.11-2.0.1.jar
6292639   24 -rwxr-xr-x   1 yarn     hadoop      20852 Apr  5 13:43 ./__spark_libs__/metrics-graphite-3.1.2.jar
5243893  184 -rwxr-xr-x   1 yarn     hadoop     185140 Apr  4 16:10 ./__spark_libs__/commons-io-2.4.jar
6292365 1032 -rwxr-xr-x   1 yarn     hadoop    1056168 Apr  5 13:43 ./__spark_libs__/snappy-java-1.1.2.6.jar
6292592   36 -rwxr-xr-x   1 yarn     hadoop      33015 Apr  5 13:43 ./__spark_libs__/jsr305-1.3.9.jar
5243102  764 -rwxr-xr-x   1 yarn     hadoop     780664 Mar 28 23:40 ./__spark_libs__/jackson-mapper-asl-1.9.13.jar
5242941   48 -rwxr-xr-x   1 yarn     hadoop      45944 Mar 28 23:40 ./__spark_libs__/json-20090211.jar
6292205  776 -rwxr-xr-x   1 yarn     hadoop     792964 Apr  5 13:43 ./__spark_libs__/zookeeper-3.4.6.jar
6292355  232 -rwxr-xr-x   1 yarn     hadoop     236880 Apr  5 13:43 ./__spark_libs__/lz4-1.3.0.jar
6292650  896 -rwxr-xr-x   1 yarn     hadoop     917052 Apr  5 13:43 ./__spark_libs__/parquet-column-1.7.0.jar
5243825  256 -rwxr-xr-x   1 yarn     hadoop     258370 Apr  4 16:10 ./__spark_libs__/calcite-avatica-1.2.0-incubating.jar
5243039  932 -rwxr-xr-x   1 yarn     hadoop     951701 Mar 28 23:40 ./__spark_libs__/jersey-server-2.22.2.jar
6292727  140 -rwxr-xr-x   1 yarn     hadoop     142631 Apr  5 13:43 ./__spark_libs__/hadoop-yarn-client-2.7.1.jar
5242954    8 -rwxr-xr-x   1 yarn     hadoop       5950 Mar 28 23:40 ./__spark_libs__/javax.inject-2.4.0-b34.jar
6292715  420 -rwxr-xr-x   1 yarn     hadoop     427780 Apr  5 13:43 ./__spark_libs__/jodd-core-3.5.2.jar
6292777  296 -rwxr-xr-x   1 yarn     hadoop     302248 Apr  5 13:43 ./__spark_libs__/antlr4-runtime-4.5.3.jar
6292285  952 -rwxr-xr-x   1 yarn     hadoop     971310 Apr  5 13:43 ./__spark_libs__/jersey-guava-2.22.2.jar
6292278   20 -rwxr-xr-x   1 yarn     hadoop      16560 Apr  5 13:43 ./__spark_libs__/api-asn1-api-1.0.0-M20.jar
5243855  104 -rwxr-xr-x   1 yarn     hadoop     105134 Apr  4 16:10 ./__spark_libs__/jaxb-api-2.2.2.jar
6292627   20 -rwxr-xr-x   1 yarn     hadoop      18482 Apr  5 13:43 ./__spark_libs__/eigenbase-properties-1.1.5.jar
5243815  112 -rwxr-xr-x   1 yarn     hadoop     112558 Apr  4 16:10 ./__spark_libs__/metrics-core-3.1.2.jar
6292347  220 -rwxr-xr-x   1 yarn     hadoop     223573 Apr  5 13:43 ./__spark_libs__/chill_2.11-0.8.0.jar
6292694  440 -rwxr-xr-x   1 yarn     hadoop     448794 Apr  5 13:43 ./__spark_libs__/apache-log4j-extras-1.2.17.jar
6292226   56 -rwxr-xr-x   1 yarn     hadoop      55511 Apr  5 13:43 ./__spark_libs__/spark-network-shuffle_2.11-2.0.1.jar
5243087   92 -rwxr-xr-x   1 yarn     hadoop      93407 Mar 28 23:40 ./__spark_libs__/pyrolite-4.9.jar
6292762   28 -rwxr-xr-x   1 yarn     hadoop      26514 Apr  5 13:43 ./__spark_libs__/stax-api-1.0.1.jar
5243159  192 -rwxr-xr-x   1 yarn     hadoop     192993 Mar 28 23:40 ./__spark_libs__/avro-ipc-1.7.7.jar
6292632  320 -rwxr-xr-x   1 yarn     hadoop     326724 Apr  5 13:43 ./__spark_libs__/httpcore-4.4.4.jar
6292744  468 -rwxr-xr-x   1 yarn     hadoop     477970 Apr  5 13:43 ./__spark_libs__/lift-json_2.11-2.6.3.jar
6292796  508 -rwxr-xr-x   1 yarn     hadoop     516127 Apr  5 13:43 ./__spark_libs__/hadoop-mapreduce-client-app-2.7.1.jar
6292297  292 -rwxr-xr-x   1 yarn     hadoop     298829 Apr  5 13:43 ./__spark_libs__/commons-configuration-1.6.jar
6292257 5380 -rwxr-xr-x   1 yarn     hadoop    5505200 Apr  5 13:43 ./__spark_libs__/hive-metastore-1.2.1.spark2.jar
6292216  100 -rwxr-xr-x   1 yarn     hadoop     100680 Apr  5 13:43 ./__spark_libs__/hive-jdbc-1.2.1.spark2.jar
6292771   24 -rwxr-xr-x   1 yarn     hadoop      21243 Apr  5 13:43 ./__spark_libs__/parquet-generator-1.7.0.jar
15861144    4 drwx------   2 yarn     hadoop       4096 May 16 11:10 ./__spark_conf__
15861168    8 -r-x------   1 yarn     hadoop       5404 May 16 11:10 ./__spark_conf__/capacity-scheduler.xml
15861298    4 -r-x------   1 yarn     hadoop       1020 May 16 11:10 ./__spark_conf__/commons-logging.properties
15861284    4 -r-x------   1 yarn     hadoop       1019 May 16 11:10 ./__spark_conf__/container-executor.cfg
15861163    4 -r-x------   1 yarn     hadoop       1631 May 16 11:10 ./__spark_conf__/kms-log4j.properties
15861269    8 -r-x------   1 yarn     hadoop       6572 May 16 11:10 ./__spark_conf__/hdfs-site.xml
15861170    4 -r-x------   1 yarn     hadoop       3518 May 16 11:10 ./__spark_conf__/kms-acls.xml
15861310    4 -r-x------   1 yarn     hadoop       1000 May 16 11:10 ./__spark_conf__/ssl-server.xml
15861150   20 -r-x------   1 yarn     hadoop      16530 May 16 11:10 ./__spark_conf__/yarn-site.xml
15861299    4 -r-x------   1 yarn     hadoop       2490 May 16 11:10 ./__spark_conf__/hadoop-metrics.properties
15861148   12 -r-x------   1 yarn     hadoop       8699 May 16 11:10 ./__spark_conf__/log4j.properties
15861314    4 -r-x------   1 yarn     hadoop        945 May 16 11:10 ./__spark_conf__/taskcontroller.cfg
15861304    4 -r-x------   1 yarn     hadoop       2316 May 16 11:10 ./__spark_conf__/ssl-client.xml.example
15861311    4 -r-x------   1 yarn     hadoop       2268 May 16 11:10 ./__spark_conf__/ssl-server.xml.example
15861149    4 -r-x------   1 yarn     hadoop       2302 May 16 11:10 ./__spark_conf__/hadoop-metrics2.properties
15861303    4 -r-x------   1 yarn     hadoop       1602 May 16 11:10 ./__spark_conf__/health_check
15861307    4 -r-x------   1 yarn     hadoop        229 May 16 11:10 ./__spark_conf__/slaves
15861172    4 -r-x------   1 yarn     hadoop       2358 May 16 11:10 ./__spark_conf__/topology_script.py
15861159    0 -r-x------   1 yarn     hadoop          0 May 16 11:10 ./__spark_conf__/yarn.exclude
15861309    4 -r-x------   1 yarn     hadoop        951 May 16 11:10 ./__spark_conf__/mapred-env.cmd
15861147    8 -r-x------   1 yarn     hadoop       5467 May 16 11:10 ./__spark_conf__/hadoop-env.sh
15861301    4 -r-x------   1 yarn     hadoop        690 May 16 11:10 ./__spark_conf__/mapred-env.sh
15861305    4 -r-x------   1 yarn     hadoop       1527 May 16 11:10 ./__spark_conf__/kms-env.sh
15861313    8 -r-x------   1 yarn     hadoop       5511 May 16 11:10 ./__spark_conf__/kms-site.xml
15861145    8 -r-x------   1 yarn     hadoop       6836 May 16 11:10 ./__spark_conf__/mapred-site.xml
15861300    8 -r-x------   1 yarn     hadoop       4221 May 16 11:10 ./__spark_conf__/task-log4j.properties
15861164    4 -r-x------   1 yarn     hadoop       2250 May 16 11:10 ./__spark_conf__/yarn-env.cmd
15861171    4 -r-x------   1 yarn     hadoop        758 May 16 11:10 ./__spark_conf__/mapred-site.xml.template
15861151    4 -r-x------   1 yarn     hadoop       3979 May 16 11:10 ./__spark_conf__/hadoop-env.cmd
15861315   36 -r-x------   1 yarn     hadoop      34061 May 16 11:10 ./__spark_conf__/__spark_conf__.properties
15861308    4 -r-x------   1 yarn     hadoop        724 May 16 11:10 ./__spark_conf__/topology_mappings.data
15861162    8 -r-x------   1 yarn     hadoop       4197 May 16 11:10 ./__spark_conf__/core-site.xml
15861173    4 -r-x------   1 yarn     hadoop       1335 May 16 11:10 ./__spark_conf__/configuration.xsl
15861306    4 -r-x------   1 yarn     hadoop       1308 May 16 11:10 ./__spark_conf__/hadoop-policy.xml
15861312    8 -r-x------   1 yarn     hadoop       4113 May 16 11:10 ./__spark_conf__/mapred-queues.xml.template
15861177    8 -r-x------   1 yarn     hadoop       5006 May 16 11:10 ./__spark_conf__/yarn-env.sh
15861165    4 -r-x------   1 yarn     hadoop        884 May 16 11:10 ./__spark_conf__/ssl-client.xml
broken symlinks(find -L . -maxdepth 5 -type l -ls):
End of LogType:directory.info

LogType:launch_container.sh
Log Upload Time:Tue May 16 11:12:21 +0700 2017
LogLength:75361
Log Contents:
#!/bin/bash

export SPARK_YARN_STAGING_DIR="hdfs://c408.hadoop.gda.lo:8020/user/fairy/.sparkStaging/application_1494395298335_303847"
export HADOOP_CONF_DIR="/usr/hdp/current/hadoop-client/conf"
export MAX_APP_ATTEMPTS="2"
export JAVA_HOME="/usr/jdk64/jdk1.8.0_92/"
export APP_SUBMIT_TIME_ENV="1494907824987"
export NM_HOST="c433.hadoop.gda.lo"
export LOGNAME="fairy"
export JVM_PID="$$"
export PWD="/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_01_000001"
export LOCAL_DIRS="/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847"
export APPLICATION_WEB_PROXY_BASE="/proxy/application_1494395298335_303847"
export NM_HTTP_PORT="8042"
export LOG_DIRS="/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000001"
export NM_AUX_SERVICE_mapreduce_shuffle="AAA0+gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=
"
export NM_PORT="45454"
export USER="fairy"
export HADOOP_YARN_HOME="/usr/hdp/current/hadoop-yarn-nodemanager"
export CLASSPATH="$PWD:$PWD/__spark_conf__:$PWD/__spark_libs__/*:$HADOOP_CONF_DIR:/usr/hdp/current/hadoop-client/*:/usr/hdp/current/hadoop-client/lib/*:/usr/hdp/current/hadoop-hdfs-client/*:/usr/hdp/current/hadoop-hdfs-client/lib/*:/usr/hdp/current/hadoop-yarn-client/*:/usr/hdp/current/hadoop-yarn-client/lib/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.4.2.0-258/hadoop/lib/hadoop-lzo-0.6.0.2.4.2.0-258.jar:/etc/hadoop/conf/secure"
export SPARK_YARN_MODE="true"
export HADOOP_TOKEN_FILE_LOCATION="/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_01_000001/container_tokens"
export NM_AUX_SERVICE_spark_shuffle=""
export SPARK_USER="fairy"
export HOME="/home/"
export CONTAINER_ID="container_e81_1494395298335_303847_01_000001"
export MALLOC_ARENA_MAX="4"
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/57/jersey-server-2.22.2.jar" "__spark_libs__/jersey-server-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/44/spark-mllib-local_2.11-2.0.1.jar" "__spark_libs__/spark-mllib-local_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3700/hadoop-mapreduce-client-common-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/58/jackson-module-paranamer-2.6.5.jar" "__spark_libs__/jackson-module-paranamer-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3663/parquet-jackson-1.7.0.jar" "__spark_libs__/parquet-jackson-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3691/commons-cli-1.2.jar" "__spark_libs__/commons-cli-1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3693/stream-2.7.0.jar" "__spark_libs__/stream-2.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3675/hadoop-mapreduce-client-jobclient-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-jobclient-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3690/stax-api-1.0.1.jar" "__spark_libs__/stax-api-1.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/46/jersey-container-servlet-core-2.22.2.jar" "__spark_libs__/jersey-container-servlet-core-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3657/osgi-resource-locator-1.0.1.jar" "__spark_libs__/osgi-resource-locator-1.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3158/spire-macros_2.11-0.7.4.jar" "__spark_libs__/spire-macros_2.11-0.7.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3606/scala-parser-combinators_2.11-1.0.4.jar" "__spark_libs__/scala-parser-combinators_2.11-1.0.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3623/commons-lang3-3.3.2.jar" "__spark_libs__/commons-lang3-3.3.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3643/derby-10.12.1.1.jar" "__spark_libs__/derby-10.12.1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3144/hadoop-mapreduce-client-core-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-core-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/68/commons-digester-1.8.jar" "__spark_libs__/commons-digester-1.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/89/compress-lzf-1.0.3.jar" "__spark_libs__/compress-lzf-1.0.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3688/mx4j-3.0.2.jar" "__spark_libs__/mx4j-3.0.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3634/minlog-1.3.0.jar" "__spark_libs__/minlog-1.3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3618/log4j-1.2.17.jar" "__spark_libs__/log4j-1.2.17.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/62/univocity-parsers-2.1.1.jar" "__spark_libs__/univocity-parsers-2.1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3647/xmlenc-0.52.jar" "__spark_libs__/xmlenc-0.52.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/39/avro-1.7.7.jar" "__spark_libs__/avro-1.7.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/28/javax.servlet-api-3.1.0.jar" "__spark_libs__/javax.servlet-api-3.1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3610/api-asn1-api-1.0.0-M20.jar" "__spark_libs__/api-asn1-api-1.0.0-M20.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/84/bonecp-0.8.0.RELEASE.jar" "__spark_libs__/bonecp-0.8.0.RELEASE.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/48/commons-math3-3.4.1.jar" "__spark_libs__/commons-math3-3.4.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3593/apacheds-i18n-2.0.0-M15.jar" "__spark_libs__/apacheds-i18n-2.0.0-M15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3645/joda-time-2.9.3.jar" "__spark_libs__/joda-time-2.9.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3619/parquet-format-2.3.0-incubating.jar" "__spark_libs__/parquet-format-2.3.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3617/commons-configuration-1.6.jar" "__spark_libs__/commons-configuration-1.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3602/jsp-api-2.1.jar" "__spark_libs__/jsp-api-2.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/35/hadoop-hdfs-2.7.1.jar" "__spark_libs__/hadoop-hdfs-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3159/mesos-0.21.1-shaded-protobuf.jar" "__spark_libs__/mesos-0.21.1-shaded-protobuf.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3589/zookeeper-3.4.6.jar" "__spark_libs__/zookeeper-3.4.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3599/aopalliance-repackaged-2.4.0-b34.jar" "__spark_libs__/aopalliance-repackaged-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3590/super-csv-2.2.0.jar" "__spark_libs__/super-csv-2.2.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/45/json4s-core_2.11-3.2.11.jar" "__spark_libs__/json4s-core_2.11-3.2.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3600/commons-lang-2.6.jar" "__spark_libs__/commons-lang-2.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/76/slf4j-api-1.7.16.jar" "__spark_libs__/slf4j-api-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65/scalap-2.11.8.jar" "__spark_libs__/scalap-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3636/spark-core_2.11-2.0.1.jar" "__spark_libs__/spark-core_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3147/calcite-avatica-1.2.0-incubating.jar" "__spark_libs__/calcite-avatica-1.2.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3163/datanucleus-api-jdo-3.2.6.jar" "__spark_libs__/datanucleus-api-jdo-3.2.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3637/xercesImpl-2.9.1.jar" "__spark_libs__/xercesImpl-2.9.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/96/hk2-locator-2.4.0-b34.jar" "__spark_libs__/hk2-locator-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/59/curator-recipes-2.6.0.jar" "__spark_libs__/curator-recipes-2.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3668/spark-graphx_2.11-2.0.1.jar" "__spark_libs__/spark-graphx_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3156/jaxb-api-2.2.2.jar" "__spark_libs__/jaxb-api-2.2.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/75/commons-codec-1.10.jar" "__spark_libs__/commons-codec-1.10.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3591/calcite-core-1.2.0-incubating.jar" "__spark_libs__/calcite-core-1.2.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/26/scala-xml_2.11-1.0.2.jar" "__spark_libs__/scala-xml_2.11-1.0.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3155/commons-io-2.4.jar" "__spark_libs__/commons-io-2.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/32/spark-tags_2.11-2.0.1.jar" "__spark_libs__/spark-tags_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/33/objenesis-2.1.jar" "__spark_libs__/objenesis-2.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/79/breeze_2.11-0.11.2.jar" "__spark_libs__/breeze_2.11-0.11.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3639/spark-sql_2.11-2.0.1.jar" "__spark_libs__/spark-sql_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3597/hive-cli-1.2.1.spark2.jar" "__spark_libs__/hive-cli-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/82/ivy-2.4.0.jar" "__spark_libs__/ivy-2.4.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3667/hadoop-mapreduce-client-shuffle-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-shuffle-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3640/api-util-1.0.0-M20.jar" "__spark_libs__/api-util-1.0.0-M20.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3666/jackson-xc-1.9.13.jar" "__spark_libs__/jackson-xc-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3605/spark-unsafe_2.11-2.0.1.jar" "__spark_libs__/spark-unsafe_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3644/datanucleus-rdbms-3.2.9.jar" "__spark_libs__/datanucleus-rdbms-3.2.9.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3650/kryo-shaded-3.0.3.jar" "__spark_libs__/kryo-shaded-3.0.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3649/java-xmlbuilder-1.0.jar" "__spark_libs__/java-xmlbuilder-1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3628/apacheds-kerberos-codec-2.0.0-M15.jar" "__spark_libs__/apacheds-kerberos-codec-2.0.0-M15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3145/scala-library-2.11.8.jar" "__spark_libs__/scala-library-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/78/jackson-mapper-asl-1.9.13.jar" "__spark_libs__/jackson-mapper-asl-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3687/spark-hive_2.11-2.0.1.jar" "__spark_libs__/spark-hive_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/95/jetty-6.1.26.jar" "__spark_libs__/jetty-6.1.26.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3150/jersey-media-jaxb-2.22.2.jar" "__spark_libs__/jersey-media-jaxb-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3676/apache-log4j-extras-1.2.17.jar" "__spark_libs__/apache-log4j-extras-1.2.17.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3621/hive-jdbc-1.2.1.spark2.jar" "__spark_libs__/hive-jdbc-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/77/spark-yarn_2.11-2.0.1.jar" "__spark_libs__/spark-yarn_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3143/metrics-core-3.1.2.jar" "__spark_libs__/metrics-core-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3654/eigenbase-properties-1.1.5.jar" "__spark_libs__/eigenbase-properties-1.1.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3679/activation-1.1.1.jar" "__spark_libs__/activation-1.1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55/jackson-core-2.6.5.jar" "__spark_libs__/jackson-core-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3625/hadoop-yarn-server-common-2.7.1.jar" "__spark_libs__/hadoop-yarn-server-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3592/guice-servlet-3.0.jar" "__spark_libs__/guice-servlet-3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3665/jackson-jaxrs-1.9.13.jar" "__spark_libs__/jackson-jaxrs-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3596/jackson-annotations-2.6.5.jar" "__spark_libs__/jackson-annotations-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3622/hadoop-auth-2.7.1.jar" "__spark_libs__/hadoop-auth-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/81/hadoop-yarn-server-web-proxy-2.7.1.jar" "__spark_libs__/hadoop-yarn-server-web-proxy-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3620/jpam-1.1.jar" "__spark_libs__/jpam-1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3614/javax.ws.rs-api-2.0.1.jar" "__spark_libs__/javax.ws.rs-api-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3655/snappy-java-1.1.2.6.jar" "__spark_libs__/snappy-java-1.1.2.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3658/metrics-graphite-3.1.2.jar" "__spark_libs__/metrics-graphite-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3626/commons-collections-3.2.2.jar" "__spark_libs__/commons-collections-3.2.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3701/pmml-schema-1.2.15.jar" "__spark_libs__/pmml-schema-1.2.15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3157/bcprov-jdk15on-1.51.jar" "__spark_libs__/bcprov-jdk15on-1.51.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3160/netty-3.8.0.Final.jar" "__spark_libs__/netty-3.8.0.Final.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/40/jcl-over-slf4j-1.7.16.jar" "__spark_libs__/jcl-over-slf4j-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/63/hk2-api-2.4.0-b34.jar" "__spark_libs__/hk2-api-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3653/javassist-3.18.1-GA.jar" "__spark_libs__/javassist-3.18.1-GA.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3153/antlr-2.7.7.jar" "__spark_libs__/antlr-2.7.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/29/javax.inject-2.4.0-b34.jar" "__spark_libs__/javax.inject-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3646/jsr305-1.3.9.jar" "__spark_libs__/jsr305-1.3.9.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3686/hk2-utils-2.4.0-b34.jar" "__spark_libs__/hk2-utils-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3689/spark-hive-thriftserver_2.11-2.0.1.jar" "__spark_libs__/spark-hive-thriftserver_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3608/jtransforms-2.4.0.jar" "__spark_libs__/jtransforms-2.4.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3607/parquet-encoding-1.7.0.jar" "__spark_libs__/parquet-encoding-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69/jersey-container-servlet-2.22.2.jar" "__spark_libs__/jersey-container-servlet-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/usercache/fairy/filecache/11522/__spark_conf__.zip" "__spark_conf__"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/85/chill-java-0.8.0.jar" "__spark_libs__/chill-java-0.8.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3651/guice-3.0.jar" "__spark_libs__/guice-3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/71/breeze-macros_2.11-0.11.2.jar" "__spark_libs__/breeze-macros_2.11-0.11.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3616/py4j-0.10.3.jar" "__spark_libs__/py4j-0.10.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3685/lift-json_2.11-2.6.3.jar" "__spark_libs__/lift-json_2.11-2.6.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3146/commons-compress-1.4.1.jar" "__spark_libs__/commons-compress-1.4.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/73/datanucleus-core-3.2.10.jar" "__spark_libs__/datanucleus-core-3.2.10.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3586/commons-beanutils-core-1.8.0.jar" "__spark_libs__/commons-beanutils-core-1.8.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/60/jets3t-0.9.3.jar" "__spark_libs__/jets3t-0.9.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3680/jodd-core-3.5.2.jar" "__spark_libs__/jodd-core-3.5.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3154/antlr-runtime-3.4.jar" "__spark_libs__/antlr-runtime-3.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/51/paranamer-2.3.jar" "__spark_libs__/paranamer-2.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3638/commons-net-2.2.jar" "__spark_libs__/commons-net-2.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3632/jdo-api-3.0.1.jar" "__spark_libs__/jdo-api-3.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3151/spark-sketch_2.11-2.0.1.jar" "__spark_libs__/spark-sketch_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/92/hive-exec-1.2.1.spark2.jar" "__spark_libs__/hive-exec-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/36/spark-streaming_2.11-2.0.1.jar" "__spark_libs__/spark-streaming_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/67/opencsv-2.3.jar" "__spark_libs__/opencsv-2.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3630/spark-mllib_2.11-2.0.1.jar" "__spark_libs__/spark-mllib_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3682/jta-1.1.jar" "__spark_libs__/jta-1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3692/parquet-generator-1.7.0.jar" "__spark_libs__/parquet-generator-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/25/json-20090211.jar" "__spark_libs__/json-20090211.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3699/hadoop-mapreduce-client-app-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-app-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3697/netty-all-4.0.29.Final.jar" "__spark_libs__/netty-all-4.0.29.Final.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3662/parquet-column-1.7.0.jar" "__spark_libs__/parquet-column-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3609/jersey-common-2.22.2.jar" "__spark_libs__/jersey-common-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/41/json4s-jackson_2.11-3.2.11.jar" "__spark_libs__/json4s-jackson_2.11-3.2.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3672/hadoop-common-2.7.1.jar" "__spark_libs__/hadoop-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/54/gson-2.2.4.jar" "__spark_libs__/gson-2.2.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/83/slf4j-log4j12-1.7.16.jar" "__spark_libs__/slf4j-log4j12-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/86/jersey-bundle-1.19.1.jar" "__spark_libs__/jersey-bundle-1.19.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3588/jetty-util-6.1.26.jar" "__spark_libs__/jetty-util-6.1.26.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/27/hadoop-client-2.7.1.jar" "__spark_libs__/hadoop-client-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3624/parquet-common-1.7.0.jar" "__spark_libs__/parquet-common-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3631/commons-httpclient-3.1.jar" "__spark_libs__/commons-httpclient-3.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3595/spark-network-shuffle_2.11-2.0.1.jar" "__spark_libs__/spark-network-shuffle_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/74/core-1.1.2.jar" "__spark_libs__/core-1.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3642/spark-network-common_2.11-2.0.1.jar" "__spark_libs__/spark-network-common_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3695/libfb303-0.9.2.jar" "__spark_libs__/libfb303-0.9.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3656/httpcore-4.4.4.jar" "__spark_libs__/httpcore-4.4.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3696/spire_2.11-0.7.4.jar" "__spark_libs__/spire_2.11-0.7.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/52/mysql-connector-java-5.0.8-bin.jar" "__spark_libs__/mysql-connector-java-5.0.8-bin.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/38/scala-compiler-2.11.8.jar" "__spark_libs__/scala-compiler-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/53/JavaEWAH-0.3.2.jar" "__spark_libs__/JavaEWAH-0.3.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/56/spark-repl_2.11-2.0.1.jar" "__spark_libs__/spark-repl_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3601/stringtemplate-3.2.1.jar" "__spark_libs__/stringtemplate-3.2.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3615/hadoop-yarn-common-2.7.1.jar" "__spark_libs__/hadoop-yarn-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3603/hive-metastore-1.2.1.spark2.jar" "__spark_libs__/hive-metastore-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/30/htrace-core-3.1.0-incubating.jar" "__spark_libs__/htrace-core-3.1.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3149/spark-catalyst_2.11-2.0.1.jar" "__spark_libs__/spark-catalyst_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3648/commons-compiler-2.7.6.jar" "__spark_libs__/commons-compiler-2.7.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3629/lz4-1.3.0.jar" "__spark_libs__/lz4-1.3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3661/mail-1.4.7.jar" "__spark_libs__/mail-1.4.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3664/xz-1.0.jar" "__spark_libs__/xz-1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3627/chill_2.11-0.8.0.jar" "__spark_libs__/chill_2.11-0.8.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3669/json4s-ast_2.11-3.2.11.jar" "__spark_libs__/json4s-ast_2.11-3.2.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3678/jackson-databind-2.6.5.jar" "__spark_libs__/jackson-databind-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3587/avro-mapred-1.7.7-hadoop2.jar" "__spark_libs__/avro-mapred-1.7.7-hadoop2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3612/jersey-guava-2.22.2.jar" "__spark_libs__/jersey-guava-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3673/jackson-module-scala_2.11-2.6.5.jar" "__spark_libs__/jackson-module-scala_2.11-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/49/metrics-jvm-3.1.2.jar" "__spark_libs__/metrics-jvm-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3594/metrics-json-3.1.2.jar" "__spark_libs__/metrics-json-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3671/jline-2.12.1.jar" "__spark_libs__/jline-2.12.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3161/curator-client-2.6.0.jar" "__spark_libs__/curator-client-2.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3635/javax.annotation-api-1.2.jar" "__spark_libs__/javax.annotation-api-1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/47/xbean-asm5-shaded-4.4.jar" "__spark_libs__/xbean-asm5-shaded-4.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/66/jackson-core-asl-1.9.13.jar" "__spark_libs__/jackson-core-asl-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/87/javax.inject-1.jar" "__spark_libs__/javax.inject-1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3613/stax-api-1.0-2.jar" "__spark_libs__/stax-api-1.0-2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3162/hadoop-annotations-2.7.1.jar" "__spark_libs__/hadoop-annotations-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3674/arpack_combined_all-0.1.jar" "__spark_libs__/arpack_combined_all-0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3677/guava-14.0.1.jar" "__spark_libs__/guava-14.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/97/avro-ipc-1.7.7.jar" "__spark_libs__/avro-ipc-1.7.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3659/curator-framework-2.6.0.jar" "__spark_libs__/curator-framework-2.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91/leveldbjni-all-1.8.jar" "__spark_libs__/leveldbjni-all-1.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3694/antlr4-runtime-4.5.3.jar" "__spark_libs__/antlr4-runtime-4.5.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/50/parquet-hadoop-bundle-1.6.0.jar" "__spark_libs__/parquet-hadoop-bundle-1.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/70/janino-2.7.8.jar" "__spark_libs__/janino-2.7.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3670/hadoop-yarn-api-2.7.1.jar" "__spark_libs__/hadoop-yarn-api-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/94/base64-2.3.8.jar" "__spark_libs__/base64-2.3.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3148/scala-reflect-2.11.8.jar" "__spark_libs__/scala-reflect-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/usercache/fairy/filecache/11521/stats-etlr-1.0.jar" "__app__.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/61/commons-dbcp-1.4.jar" "__spark_libs__/commons-dbcp-1.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/43/commons-beanutils-1.7.0.jar" "__spark_libs__/commons-beanutils-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/80/spark-launcher_2.11-2.0.1.jar" "__spark_libs__/spark-launcher_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3633/jul-to-slf4j-1.7.16.jar" "__spark_libs__/jul-to-slf4j-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/72/pyrolite-4.9.jar" "__spark_libs__/pyrolite-4.9.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3641/hive-beeline-1.2.1.spark2.jar" "__spark_libs__/hive-beeline-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3684/RoaringBitmap-0.5.11.jar" "__spark_libs__/RoaringBitmap-0.5.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3585/libthrift-0.9.2.jar" "__spark_libs__/libthrift-0.9.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3698/jersey-client-2.22.2.jar" "__spark_libs__/jersey-client-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3660/commons-logging-1.1.3.jar" "__spark_libs__/commons-logging-1.1.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/98/calcite-linq4j-1.2.0-incubating.jar" "__spark_libs__/calcite-linq4j-1.2.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3652/oro-2.0.8.jar" "__spark_libs__/oro-2.0.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3683/javolution-5.5.1.jar" "__spark_libs__/javolution-5.5.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/99/httpclient-4.5.2.jar" "__spark_libs__/httpclient-4.5.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3611/snappy-0.2.jar" "__spark_libs__/snappy-0.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/93/aopalliance-1.0.jar" "__spark_libs__/aopalliance-1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3681/hadoop-yarn-client-2.7.1.jar" "__spark_libs__/hadoop-yarn-client-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/88/parquet-hadoop-1.7.0.jar" "__spark_libs__/parquet-hadoop-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/31/commons-pool-1.5.4.jar" "__spark_libs__/commons-pool-1.5.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/90/ST4-4.0.4.jar" "__spark_libs__/ST4-4.0.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3152/protobuf-java-2.5.0.jar" "__spark_libs__/protobuf-java-2.5.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/42/pmml-model-1.2.15.jar" "__spark_libs__/pmml-model-1.2.15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/34/validation-api-1.1.0.Final.jar" "__spark_libs__/validation-api-1.1.0.Final.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
# Creating copy of launch script
cp "launch_container.sh" "/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000001/launch_container.sh"
chmod 640 "/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000001/launch_container.sh"
# Determining directory contents
echo "ls -l:" 1>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000001/directory.info"
ls -l 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000001/directory.info"
echo "find -L . -maxdepth 5 -ls:" 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000001/directory.info"
find -L . -maxdepth 5 -ls 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000001/directory.info"
echo "broken symlinks(find -L . -maxdepth 5 -type l -ls):" 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000001/directory.info"
find -L . -maxdepth 5 -type l -ls 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000001/directory.info"
exec /bin/bash -c "$JAVA_HOME/bin/java -server -Xmx3072m -Djava.io.tmpdir=$PWD/tmp '-Dhdp.version=2.4.2.0-258' -Dspark.yarn.app.container.log.dir=/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000001 org.apache.spark.deploy.yarn.ApplicationMaster --class 'vng.ge.stats.report.job.Runner' --jar file:/home/fairy/quangctn/spark_submit/report/lib/stats-etlr-1.0.jar --arg 'game_code=cack' --arg 'log_date=2017-05-06' --arg 'calc_id=id' --arg 'source=sdk' --arg 'group_id=game' --arg 'job_name=cack' --arg 'report_number=1-2-3-4-5-6-7-8-9' --arg 'run_timing=a1,a3,a7,a14,a30,ac7,ac30' --arg 'logDir=/ge/warehouse' --properties-file $PWD/__spark_conf__/__spark_conf__.properties 1> /hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000001/stdout 2> /hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000001/stderr"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
End of LogType:launch_container.sh

LogType:stderr
Log Upload Time:Tue May 16 11:12:21 +0700 2017
LogLength:191607
Log Contents:
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/hadoop/yarn/local/filecache/83/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/hdp/2.4.2.0-258/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
17/05/16 11:10:26 INFO util.SignalUtils: Registered signal handler for TERM
17/05/16 11:10:26 INFO util.SignalUtils: Registered signal handler for HUP
17/05/16 11:10:26 INFO util.SignalUtils: Registered signal handler for INT
17/05/16 11:10:28 INFO yarn.ApplicationMaster: Preparing Local resources
17/05/16 11:10:29 INFO yarn.ApplicationMaster: Prepared Local resources Map(__spark_libs__/guava-14.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/guava-14.0.1.jar" } size: 2189117 timestamp: 1479114351167 type: FILE visibility: PUBLIC, __spark_libs__/base64-2.3.8.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/base64-2.3.8.jar" } size: 17008 timestamp: 1479114347360 type: FILE visibility: PUBLIC, __spark_libs__/xbean-asm5-shaded-4.4.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/xbean-asm5-shaded-4.4.jar" } size: 144660 timestamp: 1479114365576 type: FILE visibility: PUBLIC, __spark_libs__/hive-beeline-1.2.1.spark2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hive-beeline-1.2.1.spark2.jar" } size: 138464 timestamp: 1479114353007 type: FILE visibility: PUBLIC, __spark_libs__/hive-cli-1.2.1.spark2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hive-cli-1.2.1.spark2.jar" } size: 40817 timestamp: 1479114353066 type: FILE visibility: PUBLIC, __spark_libs__/parquet-column-1.7.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/parquet-column-1.7.0.jar" } size: 917052 timestamp: 1479114359648 type: FILE visibility: PUBLIC, __spark_libs__/commons-codec-1.10.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/commons-codec-1.10.jar" } size: 284184 timestamp: 1479114348618 type: FILE visibility: PUBLIC, __spark_libs__/spire_2.11-0.7.4.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/spire_2.11-0.7.4.jar" } size: 7276083 timestamp: 1479114364780 type: FILE visibility: PUBLIC, __spark_libs__/datanucleus-rdbms-3.2.9.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/datanucleus-rdbms-3.2.9.jar" } size: 1809447 timestamp: 1479114350477 type: FILE visibility: PUBLIC, __spark_libs__/javolution-5.5.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/javolution-5.5.1.jar" } size: 395195 timestamp: 1479114356133 type: FILE visibility: PUBLIC, __spark_libs__/spark-sketch_2.11-2.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/spark-sketch_2.11-2.0.1.jar" } size: 29540 timestamp: 1479114363697 type: FILE visibility: PUBLIC, __spark_libs__/parquet-format-2.3.0-incubating.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/parquet-format-2.3.0-incubating.jar" } size: 387188 timestamp: 1479114359848 type: FILE visibility: PUBLIC, __spark_libs__/hadoop-auth-2.7.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hadoop-auth-2.7.1.jar" } size: 70688 timestamp: 1479114351488 type: FILE visibility: PUBLIC, __spark_libs__/snappy-java-1.1.2.6.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/snappy-java-1.1.2.6.jar" } size: 1056168 timestamp: 1479114361925 type: FILE visibility: PUBLIC, __spark_libs__/ivy-2.4.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/ivy-2.4.0.jar" } size: 1282424 timestamp: 1479114354167 type: FILE visibility: PUBLIC, __spark_libs__/jersey-client-2.22.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jersey-client-2.22.2.jar" } size: 167421 timestamp: 1479114356604 type: FILE visibility: PUBLIC, __spark_libs__/pyrolite-4.9.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/pyrolite-4.9.jar" } size: 93407 timestamp: 1479114360574 type: FILE visibility: PUBLIC, __spark_libs__/jetty-util-6.1.26.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jetty-util-6.1.26.jar" } size: 177131 timestamp: 1479114357309 type: FILE visibility: PUBLIC, __spark_libs__/scala-xml_2.11-1.0.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/scala-xml_2.11-1.0.2.jar" } size: 648678 timestamp: 1479114361648 type: FILE visibility: PUBLIC, __app__.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/.sparkStaging/application_1494395298335_303847/stats-etlr-1.0.jar" } size: 1178454 timestamp: 1494907824716 type: FILE visibility: PRIVATE, __spark_libs__/spark-launcher_2.11-2.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/spark-launcher_2.11-2.0.1.jar" } size: 65653 timestamp: 1479114362864 type: FILE visibility: PUBLIC, __spark_libs__/commons-compiler-2.7.6.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/commons-compiler-2.7.6.jar" } size: 30595 timestamp: 1479114348818 type: FILE visibility: PUBLIC, __spark_libs__/commons-io-2.4.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/commons-io-2.4.jar" } size: 185140 timestamp: 1479114349176 type: FILE visibility: PUBLIC, __spark_libs__/objenesis-2.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/objenesis-2.1.jar" } size: 41755 timestamp: 1479114359355 type: FILE visibility: PUBLIC, __spark_libs__/jersey-media-jaxb-2.22.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jersey-media-jaxb-2.22.2.jar" } size: 72733 timestamp: 1479114356914 type: FILE visibility: PUBLIC, __spark_libs__/parquet-jackson-1.7.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/parquet-jackson-1.7.0.jar" } size: 1048110 timestamp: 1479114360177 type: FILE visibility: PUBLIC, __spark_libs__/mx4j-3.0.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/mx4j-3.0.2.jar" } size: 409467 timestamp: 1479114359115 type: FILE visibility: PUBLIC, __spark_libs__/javax.servlet-api-3.1.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/javax.servlet-api-3.1.0.jar" } size: 95806 timestamp: 1479114355903 type: FILE visibility: PUBLIC, __spark_libs__/spire-macros_2.11-0.7.4.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/spire-macros_2.11-0.7.4.jar" } size: 86811 timestamp: 1479114364854 type: FILE visibility: PUBLIC, __spark_libs__/apacheds-kerberos-codec-2.0.0-M15.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/apacheds-kerberos-codec-2.0.0-M15.jar" } size: 691479 timestamp: 1479114346581 type: FILE visibility: PUBLIC, __spark_libs__/scala-parser-combinators_2.11-1.0.4.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/scala-parser-combinators_2.11-1.0.4.jar" } size: 423753 timestamp: 1479114361442 type: FILE visibility: PUBLIC, __spark_libs__/hadoop-client-2.7.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hadoop-client-2.7.1.jar" } size: 2545 timestamp: 1479114351539 type: FILE visibility: PUBLIC, __spark_libs__/spark-repl_2.11-2.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/spark-repl_2.11-2.0.1.jar" } size: 63316 timestamp: 1479114363593 type: FILE visibility: PUBLIC, __spark_libs__/commons-logging-1.1.3.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/commons-logging-1.1.3.jar" } size: 62050 timestamp: 1479114349329 type: FILE visibility: PUBLIC, __spark_libs__/calcite-avatica-1.2.0-incubating.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/calcite-avatica-1.2.0-incubating.jar" } size: 258370 timestamp: 1479114347988 type: FILE visibility: PUBLIC, __spark_libs__/scala-library-2.11.8.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/scala-library-2.11.8.jar" } size: 5744974 timestamp: 1479114361256 type: FILE visibility: PUBLIC, __spark_libs__/stream-2.7.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/stream-2.7.0.jar" } size: 174351 timestamp: 1479114365186 type: FILE visibility: PUBLIC, __spark_libs__/javax.annotation-api-1.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/javax.annotation-api-1.2.jar" } size: 26366 timestamp: 1479114355501 type: FILE visibility: PUBLIC, __spark_libs__/parquet-common-1.7.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/parquet-common-1.7.0.jar" } size: 21575 timestamp: 1479114359702 type: FILE visibility: PUBLIC, __spark_libs__/opencsv-2.3.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/opencsv-2.3.jar" } size: 19827 timestamp: 1479114359401 type: FILE visibility: PUBLIC, __spark_libs__/commons-lang-2.6.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/commons-lang-2.6.jar" } size: 284220 timestamp: 1479114349235 type: FILE visibility: PUBLIC, __spark_libs__/spark-graphx_2.11-2.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/spark-graphx_2.11-2.0.1.jar" } size: 680106 timestamp: 1479114362466 type: FILE visibility: PUBLIC, __spark_libs__/guice-servlet-3.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/guice-servlet-3.0.jar" } size: 65012 timestamp: 1479114351350 type: FILE visibility: PUBLIC, __spark_libs__/metrics-graphite-3.1.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/metrics-graphite-3.1.2.jar" } size: 20852 timestamp: 1479114358871 type: FILE visibility: PUBLIC, __spark_libs__/hadoop-mapreduce-client-common-2.7.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hadoop-mapreduce-client-common-2.7.1.jar" } size: 753012 timestamp: 1479114352328 type: FILE visibility: PUBLIC, __spark_libs__/hadoop-mapreduce-client-shuffle-2.7.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hadoop-mapreduce-client-shuffle-2.7.1.jar" } size: 45015 timestamp: 1479114352553 type: FILE visibility: PUBLIC, __spark_libs__/spark-network-common_2.11-2.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/spark-network-common_2.11-2.0.1.jar" } size: 2355465 timestamp: 1479114363447 type: FILE visibility: PUBLIC, __spark_libs__/commons-math3-3.4.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/commons-math3-3.4.1.jar" } size: 2035066 timestamp: 1479114349475 type: FILE visibility: PUBLIC, __spark_libs__/jersey-bundle-1.19.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jersey-bundle-1.19.1.jar" } size: 1627065 timestamp: 1479114356469 type: FILE visibility: PUBLIC, __spark_libs__/hk2-locator-2.4.0-b34.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hk2-locator-2.4.0-b34.jar" } size: 181271 timestamp: 1479114353738 type: FILE visibility: PUBLIC, __spark_libs__/commons-collections-3.2.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/commons-collections-3.2.2.jar" } size: 588337 timestamp: 1479114348763 type: FILE visibility: PUBLIC, __spark_libs__/kryo-shaded-3.0.3.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/kryo-shaded-3.0.3.jar" } size: 358390 timestamp: 1479114358300 type: FILE visibility: PUBLIC, __spark_libs__/arpack_combined_all-0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/arpack_combined_all-0.1.jar" } size: 1194003 timestamp: 1479114347079 type: FILE visibility: PUBLIC, __spark_libs__/jackson-module-paranamer-2.6.5.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jackson-module-paranamer-2.6.5.jar" } size: 41263 timestamp: 1479114354895 type: FILE visibility: PUBLIC, __spark_libs__/curator-client-2.6.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/curator-client-2.6.0.jar" } size: 68866 timestamp: 1479114350136 type: FILE visibility: PUBLIC, __spark_libs__/hadoop-yarn-client-2.7.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hadoop-yarn-client-2.7.1.jar" } size: 142631 timestamp: 1479114352719 type: FILE visibility: PUBLIC, __spark_libs__/lz4-1.3.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/lz4-1.3.0.jar" } size: 236880 timestamp: 1479114358635 type: FILE visibility: PUBLIC, __spark_libs__/RoaringBitmap-0.5.11.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/RoaringBitmap-0.5.11.jar" } size: 201928 timestamp: 1479114360629 type: FILE visibility: PUBLIC, __spark_libs__/javax.inject-1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/javax.inject-1.jar" } size: 2497 timestamp: 1479114355639 type: FILE visibility: PUBLIC, __spark_libs__/spark-streaming_2.11-2.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/spark-streaming_2.11-2.0.1.jar" } size: 2090370 timestamp: 1479114364106 type: FILE visibility: PUBLIC, __spark_libs__/datanucleus-core-3.2.10.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/datanucleus-core-3.2.10.jar" } size: 1890075 timestamp: 1479114350395 type: FILE visibility: PUBLIC, __spark_libs__/commons-lang3-3.3.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/commons-lang3-3.3.2.jar" } size: 412739 timestamp: 1479114349296 type: FILE visibility: PUBLIC, __spark_libs__/parquet-hadoop-bundle-1.6.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/parquet-hadoop-bundle-1.6.0.jar" } size: 2796935 timestamp: 1479114360096 type: FILE visibility: PUBLIC, __spark_libs__/javax.ws.rs-api-2.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/javax.ws.rs-api-2.0.1.jar" } size: 115534 timestamp: 1479114355988 type: FILE visibility: PUBLIC, __spark_libs__/validation-api-1.1.0.Final.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/validation-api-1.1.0.Final.jar" } size: 63777 timestamp: 1479114365455 type: FILE visibility: PUBLIC, __spark_libs__/jersey-container-servlet-2.22.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jersey-container-servlet-2.22.2.jar" } size: 18098 timestamp: 1479114356728 type: FILE visibility: PUBLIC, __spark_libs__/parquet-encoding-1.7.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/parquet-encoding-1.7.0.jar" } size: 285447 timestamp: 1479114359763 type: FILE visibility: PUBLIC, __spark_libs__/derby-10.12.1.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/derby-10.12.1.1.jar" } size: 3224708 timestamp: 1479114350586 type: FILE visibility: PUBLIC, __spark_libs__/jpam-1.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jpam-1.1.jar" } size: 12131 timestamp: 1479114357692 type: FILE visibility: PUBLIC, __spark_libs__/oro-2.0.8.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/oro-2.0.8.jar" } size: 65261 timestamp: 1479114359455 type: FILE visibility: PUBLIC, __spark_libs__/jackson-databind-2.6.5.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jackson-databind-2.6.5.jar" } size: 1171380 timestamp: 1479114354688 type: FILE visibility: PUBLIC, __spark_libs__/parquet-hadoop-1.7.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/parquet-hadoop-1.7.0.jar" } size: 209622 timestamp: 1479114359964 type: FILE visibility: PUBLIC, __spark_libs__/java-xmlbuilder-1.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/java-xmlbuilder-1.0.jar" } size: 10023 timestamp: 1479114355815 type: FILE visibility: PUBLIC, __spark_libs__/netty-3.8.0.Final.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/netty-3.8.0.Final.jar" } size: 1230201 timestamp: 1479114359200 type: FILE visibility: PUBLIC, __spark_libs__/snappy-0.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/snappy-0.2.jar" } size: 48720 timestamp: 1479114361854 type: FILE visibility: PUBLIC, __spark_libs__/ST4-4.0.4.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/ST4-4.0.4.jar" } size: 236660 timestamp: 1479114364959 type: FILE visibility: PUBLIC, __spark_libs__/calcite-linq4j-1.2.0-incubating.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/calcite-linq4j-1.2.0-incubating.jar" } size: 442406 timestamp: 1479114348202 type: FILE visibility: PUBLIC, __spark_libs__/avro-1.7.7.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/avro-1.7.7.jar" } size: 436303 timestamp: 1479114347141 type: FILE visibility: PUBLIC, __spark_libs__/apache-log4j-extras-1.2.17.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/apache-log4j-extras-1.2.17.jar" } size: 448794 timestamp: 1479114346658 type: FILE visibility: PUBLIC, __spark_libs__/metrics-json-3.1.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/metrics-json-3.1.2.jar" } size: 15827 timestamp: 1479114358921 type: FILE visibility: PUBLIC, __spark_libs__/spark-core_2.11-2.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/spark-core_2.11-2.0.1.jar" } size: 11723537 timestamp: 1479114362385 type: FILE visibility: PUBLIC, __spark_libs__/json4s-jackson_2.11-3.2.11.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/json4s-jackson_2.11-3.2.11.jar" } size: 40341 timestamp: 1479114357958 type: FILE visibility: PUBLIC, __spark_libs__/avro-ipc-1.7.7.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/avro-ipc-1.7.7.jar" } size: 192993 timestamp: 1479114347272 type: FILE visibility: PUBLIC, __spark_libs__/commons-pool-1.5.4.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/commons-pool-1.5.4.jar" } size: 96221 timestamp: 1479114349599 type: FILE visibility: PUBLIC, __spark_libs__/hive-metastore-1.2.1.spark2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hive-metastore-1.2.1.spark2.jar" } size: 5505200 timestamp: 1479114353604 type: FILE visibility: PUBLIC, __spark_libs__/py4j-0.10.3.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/py4j-0.10.3.jar" } size: 114913 timestamp: 1479114360491 type: FILE visibility: PUBLIC, __spark_libs__/scala-reflect-2.11.8.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/scala-reflect-2.11.8.jar" } size: 4573750 timestamp: 1479114361581 type: FILE visibility: PUBLIC, __spark_libs__/super-csv-2.2.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/super-csv-2.2.0.jar" } size: 93210 timestamp: 1479114365297 type: FILE visibility: PUBLIC, __spark_libs__/protobuf-java-2.5.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/protobuf-java-2.5.0.jar" } size: 533455 timestamp: 1479114360414 type: FILE visibility: PUBLIC, __spark_libs__/antlr-2.7.7.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/antlr-2.7.7.jar" } size: 445288 timestamp: 1479114346112 type: FILE visibility: PUBLIC, __spark_libs__/hadoop-mapreduce-client-app-2.7.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hadoop-mapreduce-client-app-2.7.1.jar" } size: 516127 timestamp: 1479114352239 type: FILE visibility: PUBLIC, __spark_libs__/curator-recipes-2.6.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/curator-recipes-2.6.0.jar" } size: 248171 timestamp: 1479114350263 type: FILE visibility: PUBLIC, __spark_libs__/jsp-api-2.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jsp-api-2.1.jar" } size: 100636 timestamp: 1479114358008 type: FILE visibility: PUBLIC, __spark_libs__/breeze_2.11-0.11.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/breeze_2.11-0.11.2.jar" } size: 13448966 timestamp: 1479114347866 type: FILE visibility: PUBLIC, __spark_libs__/commons-beanutils-core-1.8.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/commons-beanutils-core-1.8.0.jar" } size: 206035 timestamp: 1479114348473 type: FILE visibility: PUBLIC, __spark_libs__/metrics-core-3.1.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/metrics-core-3.1.2.jar" } size: 112558 timestamp: 1479114358822 type: FILE visibility: PUBLIC, __spark_libs__/jackson-mapper-asl-1.9.13.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jackson-mapper-asl-1.9.13.jar" } size: 780664 timestamp: 1479114354829 type: FILE visibility: PUBLIC, __spark_libs__/antlr-runtime-3.4.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/antlr-runtime-3.4.jar" } size: 164368 timestamp: 1479114346266 type: FILE visibility: PUBLIC, __spark_libs__/hadoop-hdfs-2.7.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hadoop-hdfs-2.7.1.jar" } size: 8260573 timestamp: 1479114352007 type: FILE visibility: PUBLIC, __spark_libs__/jackson-annotations-2.6.5.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jackson-annotations-2.6.5.jar" } size: 46983 timestamp: 1479114354226 type: FILE visibility: PUBLIC, __spark_libs__/libthrift-0.9.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/libthrift-0.9.2.jar" } size: 227712 timestamp: 1479114358512 type: FILE visibility: PUBLIC, __spark_libs__/spark-mllib-local_2.11-2.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/spark-mllib-local_2.11-2.0.1.jar" } size: 177832 timestamp: 1479114363111 type: FILE visibility: PUBLIC, __spark_libs__/jackson-jaxrs-1.9.13.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jackson-jaxrs-1.9.13.jar" } size: 18336 timestamp: 1479114354733 type: FILE visibility: PUBLIC, __spark_libs__/jersey-guava-2.22.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jersey-guava-2.22.2.jar" } size: 971310 timestamp: 1479114356860 type: FILE visibility: PUBLIC, __spark_libs__/minlog-1.3.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/minlog-1.3.0.jar" } size: 5711 timestamp: 1479114359046 type: FILE visibility: PUBLIC, __spark_libs__/httpclient-4.5.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/httpclient-4.5.2.jar" } size: 736658 timestamp: 1479114354023 type: FILE visibility: PUBLIC, __spark_libs__/paranamer-2.3.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/paranamer-2.3.jar" } size: 29555 timestamp: 1479114359589 type: FILE visibility: PUBLIC, __spark_libs__/hive-exec-1.2.1.spark2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hive-exec-1.2.1.spark2.jar" } size: 11498852 timestamp: 1479114353361 type: FILE visibility: PUBLIC, __spark_libs__/mysql-connector-java-5.0.8-bin.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/mysql-connector-java-5.0.8-bin.jar" } size: 540852 timestamp: 1487839787847 type: FILE visibility: PUBLIC, __spark_libs__/libfb303-0.9.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/libfb303-0.9.2.jar" } size: 313686 timestamp: 1479114358457 type: FILE visibility: PUBLIC, __spark_libs__/hadoop-yarn-api-2.7.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hadoop-yarn-api-2.7.1.jar" } size: 2015514 timestamp: 1479114352670 type: FILE visibility: PUBLIC, __spark_libs__/parquet-generator-1.7.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/parquet-generator-1.7.0.jar" } size: 21243 timestamp: 1479114359903 type: FILE visibility: PUBLIC, __spark_libs__/hadoop-mapreduce-client-core-2.7.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hadoop-mapreduce-client-core-2.7.1.jar" } size: 1544875 timestamp: 1479114352416 type: FILE visibility: PUBLIC, __spark_libs__/jodd-core-3.5.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jodd-core-3.5.2.jar" } size: 427780 timestamp: 1479114357600 type: FILE visibility: PUBLIC, __spark_libs__/spark-hive-thriftserver_2.11-2.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/spark-hive-thriftserver_2.11-2.0.1.jar" } size: 1814309 timestamp: 1479114362781 type: FILE visibility: PUBLIC, __spark_libs__/jackson-xc-1.9.13.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jackson-xc-1.9.13.jar" } size: 27084 timestamp: 1479114355045 type: FILE visibility: PUBLIC, __spark_libs__/javassist-3.18.1-GA.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/javassist-3.18.1-GA.jar" } size: 714194 timestamp: 1479114355425 type: FILE visibility: PUBLIC, __spark_libs__/jersey-common-2.22.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jersey-common-2.22.2.jar" } size: 698375 timestamp: 1479114356678 type: FILE visibility: PUBLIC, __spark_libs__/avro-mapred-1.7.7-hadoop2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/avro-mapred-1.7.7-hadoop2.jar" } size: 180736 timestamp: 1479114347317 type: FILE visibility: PUBLIC, __spark_libs__/spark-catalyst_2.11-2.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/spark-catalyst_2.11-2.0.1.jar" } size: 6873892 timestamp: 1479114362096 type: FILE visibility: PUBLIC, __spark_libs__/log4j-1.2.17.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/log4j-1.2.17.jar" } size: 489884 timestamp: 1479114358563 type: FILE visibility: PUBLIC, __spark_libs__/spark-tags_2.11-2.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/spark-tags_2.11-2.0.1.jar" } size: 15305 timestamp: 1479114364213 type: FILE visibility: PUBLIC, __spark_libs__/metrics-jvm-3.1.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/metrics-jvm-3.1.2.jar" } size: 39280 timestamp: 1479114358993 type: FILE visibility: PUBLIC, __spark_libs__/spark-unsafe_2.11-2.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/spark-unsafe_2.11-2.0.1.jar" } size: 41070 timestamp: 1479114364270 type: FILE visibility: PUBLIC, __spark_libs__/api-asn1-api-1.0.0-M20.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/api-asn1-api-1.0.0-M20.jar" } size: 16560 timestamp: 1479114346748 type: FILE visibility: PUBLIC, __spark_libs__/htrace-core-3.1.0-incubating.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/htrace-core-3.1.0-incubating.jar" } size: 1475955 timestamp: 1479114353943 type: FILE visibility: PUBLIC, __spark_libs__/aopalliance-repackaged-2.4.0-b34.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/aopalliance-repackaged-2.4.0-b34.jar" } size: 14766 timestamp: 1479114346380 type: FILE visibility: PUBLIC, __spark_conf__ -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/.sparkStaging/application_1494395298335_303847/__spark_conf__.zip" } size: 148464 timestamp: 1494907824876 type: ARCHIVE visibility: PRIVATE, __spark_libs__/netty-all-4.0.29.Final.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/netty-all-4.0.29.Final.jar" } size: 2054931 timestamp: 1479114359303 type: FILE visibility: PUBLIC, __spark_libs__/scala-compiler-2.11.8.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/scala-compiler-2.11.8.jar" } size: 15487351 timestamp: 1479114361118 type: FILE visibility: PUBLIC, __spark_libs__/core-1.1.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/core-1.1.2.jar" } size: 164422 timestamp: 1479114350080 type: FILE visibility: PUBLIC, __spark_libs__/mail-1.4.7.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/mail-1.4.7.jar" } size: 521157 timestamp: 1479114358695 type: FILE visibility: PUBLIC, __spark_libs__/jul-to-slf4j-1.7.16.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jul-to-slf4j-1.7.16.jar" } size: 4596 timestamp: 1479114358231 type: FILE visibility: PUBLIC, __spark_libs__/xmlenc-0.52.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/xmlenc-0.52.jar" } size: 15010 timestamp: 1479114366089 type: FILE visibility: PUBLIC, __spark_libs__/guice-3.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/guice-3.0.jar" } size: 710492 timestamp: 1479114351281 type: FILE visibility: PUBLIC, __spark_libs__/calcite-core-1.2.0-incubating.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/calcite-core-1.2.0-incubating.jar" } size: 3519262 timestamp: 1479114348109 type: FILE visibility: PUBLIC, __spark_libs__/xercesImpl-2.9.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/xercesImpl-2.9.1.jar" } size: 1229125 timestamp: 1479114366034 type: FILE visibility: PUBLIC, __spark_libs__/jets3t-0.9.3.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jets3t-0.9.3.jar" } size: 2041628 timestamp: 1479114357121 type: FILE visibility: PUBLIC, __spark_libs__/httpcore-4.4.4.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/httpcore-4.4.4.jar" } size: 326724 timestamp: 1479114354102 type: FILE visibility: PUBLIC, __spark_libs__/jline-2.12.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jline-2.12.1.jar" } size: 213911 timestamp: 1479114357395 type: FILE visibility: PUBLIC, __spark_libs__/json4s-ast_2.11-3.2.11.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/json4s-ast_2.11-3.2.11.jar" } size: 82421 timestamp: 1479114357826 type: FILE visibility: PUBLIC, __spark_libs__/slf4j-log4j12-1.7.16.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/slf4j-log4j12-1.7.16.jar" } size: 9939 timestamp: 1479114361780 type: FILE visibility: PUBLIC, __spark_libs__/jackson-core-2.6.5.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jackson-core-2.6.5.jar" } size: 258876 timestamp: 1479114354282 type: FILE visibility: PUBLIC, __spark_libs__/hadoop-common-2.7.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hadoop-common-2.7.1.jar" } size: 3431544 timestamp: 1479114351722 type: FILE visibility: PUBLIC, __spark_libs__/jaxb-api-2.2.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jaxb-api-2.2.2.jar" } size: 105134 timestamp: 1479114356207 type: FILE visibility: PUBLIC, __spark_libs__/pmml-model-1.2.15.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/pmml-model-1.2.15.jar" } size: 654216 timestamp: 1479114360268 type: FILE visibility: PUBLIC, __spark_libs__/jcl-over-slf4j-1.7.16.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jcl-over-slf4j-1.7.16.jar" } size: 16430 timestamp: 1479114356280 type: FILE visibility: PUBLIC, __spark_libs__/datanucleus-api-jdo-3.2.6.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/datanucleus-api-jdo-3.2.6.jar" } size: 339666 timestamp: 1479114350317 type: FILE visibility: PUBLIC, __spark_libs__/jdo-api-3.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jdo-api-3.0.1.jar" } size: 201124 timestamp: 1479114356360 type: FILE visibility: PUBLIC, __spark_libs__/curator-framework-2.6.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/curator-framework-2.6.0.jar" } size: 185245 timestamp: 1479114350187 type: FILE visibility: PUBLIC, __spark_libs__/jackson-module-scala_2.11-2.6.5.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jackson-module-scala_2.11-2.6.5.jar" } size: 515604 timestamp: 1479114355002 type: FILE visibility: PUBLIC, __spark_libs__/commons-beanutils-1.7.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/commons-beanutils-1.7.0.jar" } size: 188671 timestamp: 1479114348411 type: FILE visibility: PUBLIC, __spark_libs__/jta-1.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jta-1.1.jar" } size: 15071 timestamp: 1479114358099 type: FILE visibility: PUBLIC, __spark_libs__/aopalliance-1.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/aopalliance-1.0.jar" } size: 4467 timestamp: 1479114346315 type: FILE visibility: PUBLIC, __spark_libs__/bcprov-jdk15on-1.51.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/bcprov-jdk15on-1.51.jar" } size: 2842667 timestamp: 1479114347496 type: FILE visibility: PUBLIC, __spark_libs__/slf4j-api-1.7.16.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/slf4j-api-1.7.16.jar" } size: 40509 timestamp: 1479114361709 type: FILE visibility: PUBLIC, __spark_libs__/commons-dbcp-1.4.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/commons-dbcp-1.4.jar" } size: 160519 timestamp: 1479114348991 type: FILE visibility: PUBLIC, __spark_libs__/commons-digester-1.8.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/commons-digester-1.8.jar" } size: 143602 timestamp: 1479114349054 type: FILE visibility: PUBLIC, __spark_libs__/activation-1.1.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/activation-1.1.1.jar" } size: 69409 timestamp: 1479114345985 type: FILE visibility: PUBLIC, __spark_libs__/scalap-2.11.8.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/scalap-2.11.8.jar" } size: 802818 timestamp: 1479114361370 type: FILE visibility: PUBLIC, __spark_libs__/jersey-server-2.22.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jersey-server-2.22.2.jar" } size: 951701 timestamp: 1479114357031 type: FILE visibility: PUBLIC, __spark_libs__/janino-2.7.8.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/janino-2.7.8.jar" } size: 613299 timestamp: 1479114355178 type: FILE visibility: PUBLIC, __spark_libs__/jersey-container-servlet-core-2.22.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jersey-container-servlet-core-2.22.2.jar" } size: 66270 timestamp: 1479114356792 type: FILE visibility: PUBLIC, __spark_libs__/apacheds-i18n-2.0.0-M15.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/apacheds-i18n-2.0.0-M15.jar" } size: 44925 timestamp: 1479114346454 type: FILE visibility: PUBLIC, __spark_libs__/breeze-macros_2.11-0.11.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/breeze-macros_2.11-0.11.2.jar" } size: 135552 timestamp: 1479114347922 type: FILE visibility: PUBLIC, __spark_libs__/javax.inject-2.4.0-b34.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/javax.inject-2.4.0-b34.jar" } size: 5950 timestamp: 1479114355715 type: FILE visibility: PUBLIC, __spark_libs__/univocity-parsers-2.1.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/univocity-parsers-2.1.1.jar" } size: 290506 timestamp: 1479114365337 type: FILE visibility: PUBLIC, __spark_libs__/spark-yarn_2.11-2.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/spark-yarn_2.11-2.0.1.jar" } size: 669589 timestamp: 1479114364455 type: FILE visibility: PUBLIC, __spark_libs__/spark-hive_2.11-2.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/spark-hive_2.11-2.0.1.jar" } size: 1044511 timestamp: 1479114362531 type: FILE visibility: PUBLIC, __spark_libs__/chill-java-0.8.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/chill-java-0.8.0.jar" } size: 50619 timestamp: 1479114348335 type: FILE visibility: PUBLIC, __spark_libs__/jetty-6.1.26.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jetty-6.1.26.jar" } size: 539912 timestamp: 1479114357208 type: FILE visibility: PUBLIC, __spark_libs__/osgi-resource-locator-1.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/osgi-resource-locator-1.0.1.jar" } size: 20235 timestamp: 1479114359529 type: FILE visibility: PUBLIC, __spark_libs__/commons-httpclient-3.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/commons-httpclient-3.1.jar" } size: 305001 timestamp: 1479114349123 type: FILE visibility: PUBLIC, __spark_libs__/commons-compress-1.4.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/commons-compress-1.4.1.jar" } size: 241367 timestamp: 1479114348885 type: FILE visibility: PUBLIC, __spark_libs__/compress-lzf-1.0.3.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/compress-lzf-1.0.3.jar" } size: 79845 timestamp: 1479114349705 type: FILE visibility: PUBLIC, __spark_libs__/gson-2.2.4.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/gson-2.2.4.jar" } size: 190432 timestamp: 1479114351077 type: FILE visibility: PUBLIC, __spark_libs__/spark-mllib_2.11-2.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/spark-mllib_2.11-2.0.1.jar" } size: 5847591 timestamp: 1479114363045 type: FILE visibility: PUBLIC, __spark_libs__/commons-net-2.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/commons-net-2.2.jar" } size: 212453 timestamp: 1479114349538 type: FILE visibility: PUBLIC, __spark_libs__/stax-api-1.0-2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/stax-api-1.0-2.jar" } size: 23346 timestamp: 1479114365093 type: FILE visibility: PUBLIC, __spark_libs__/hadoop-annotations-2.7.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hadoop-annotations-2.7.1.jar" } size: 17385 timestamp: 1479114351396 type: FILE visibility: PUBLIC, __spark_libs__/mesos-0.21.1-shaded-protobuf.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/mesos-0.21.1-shaded-protobuf.jar" } size: 1277883 timestamp: 1479114358770 type: FILE visibility: PUBLIC, __spark_libs__/eigenbase-properties-1.1.5.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/eigenbase-properties-1.1.5.jar" } size: 18482 timestamp: 1479114350658 type: FILE visibility: PUBLIC, __spark_libs__/jtransforms-2.4.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jtransforms-2.4.0.jar" } size: 764569 timestamp: 1479114358157 type: FILE visibility: PUBLIC, __spark_libs__/chill_2.11-0.8.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/chill_2.11-0.8.0.jar" } size: 223573 timestamp: 1479114348273 type: FILE visibility: PUBLIC, __spark_libs__/api-util-1.0.0-M20.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/api-util-1.0.0-M20.jar" } size: 79912 timestamp: 1479114346966 type: FILE visibility: PUBLIC, __spark_libs__/hk2-api-2.4.0-b34.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hk2-api-2.4.0-b34.jar" } size: 178947 timestamp: 1479114353676 type: FILE visibility: PUBLIC, __spark_libs__/hk2-utils-2.4.0-b34.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hk2-utils-2.4.0-b34.jar" } size: 118973 timestamp: 1479114353838 type: FILE visibility: PUBLIC, __spark_libs__/stringtemplate-3.2.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/stringtemplate-3.2.1.jar" } size: 148627 timestamp: 1479114365251 type: FILE visibility: PUBLIC, __spark_libs__/antlr4-runtime-4.5.3.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/antlr4-runtime-4.5.3.jar" } size: 302248 timestamp: 1479114346180 type: FILE visibility: PUBLIC, __spark_libs__/json-20090211.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/json-20090211.jar" } size: 45944 timestamp: 1479114357767 type: FILE visibility: PUBLIC, __spark_libs__/stax-api-1.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/stax-api-1.0.1.jar" } size: 26514 timestamp: 1479114365021 type: FILE visibility: PUBLIC, __spark_libs__/JavaEWAH-0.3.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/JavaEWAH-0.3.2.jar" } size: 16993 timestamp: 1479114355289 type: FILE visibility: PUBLIC, __spark_libs__/bonecp-0.8.0.RELEASE.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/bonecp-0.8.0.RELEASE.jar" } size: 110600 timestamp: 1479114347572 type: FILE visibility: PUBLIC, __spark_libs__/json4s-core_2.11-3.2.11.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/json4s-core_2.11-3.2.11.jar" } size: 589462 timestamp: 1479114357905 type: FILE visibility: PUBLIC, __spark_libs__/jackson-core-asl-1.9.13.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jackson-core-asl-1.9.13.jar" } size: 232248 timestamp: 1479114354339 type: FILE visibility: PUBLIC, __spark_libs__/hadoop-yarn-server-web-proxy-2.7.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hadoop-yarn-server-web-proxy-2.7.1.jar" } size: 32145 timestamp: 1479114352953 type: FILE visibility: PUBLIC, __spark_libs__/spark-network-shuffle_2.11-2.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/spark-network-shuffle_2.11-2.0.1.jar" } size: 55511 timestamp: 1479114363500 type: FILE visibility: PUBLIC, __spark_libs__/zookeeper-3.4.6.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/zookeeper-3.4.6.jar" } size: 792964 timestamp: 1479114366194 type: FILE visibility: PUBLIC, __spark_libs__/pmml-schema-1.2.15.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/pmml-schema-1.2.15.jar" } size: 5310 timestamp: 1479114360327 type: FILE visibility: PUBLIC, __spark_libs__/hadoop-yarn-common-2.7.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hadoop-yarn-common-2.7.1.jar" } size: 1654097 timestamp: 1479114352811 type: FILE visibility: PUBLIC, __spark_libs__/xz-1.0.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/xz-1.0.jar" } size: 94672 timestamp: 1479114366136 type: FILE visibility: PUBLIC, __spark_libs__/commons-cli-1.2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/commons-cli-1.2.jar" } size: 41123 timestamp: 1479114348550 type: FILE visibility: PUBLIC, __spark_libs__/spark-sql_2.11-2.0.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/spark-sql_2.11-2.0.1.jar" } size: 6290315 timestamp: 1479114363954 type: FILE visibility: PUBLIC, __spark_libs__/hadoop-mapreduce-client-jobclient-2.7.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hadoop-mapreduce-client-jobclient-2.7.1.jar" } size: 38134 timestamp: 1479114352478 type: FILE visibility: PUBLIC, __spark_libs__/joda-time-2.9.3.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/joda-time-2.9.3.jar" } size: 627814 timestamp: 1479114357501 type: FILE visibility: PUBLIC, __spark_libs__/hadoop-yarn-server-common-2.7.1.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hadoop-yarn-server-common-2.7.1.jar" } size: 363908 timestamp: 1479114352894 type: FILE visibility: PUBLIC, __spark_libs__/commons-configuration-1.6.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/commons-configuration-1.6.jar" } size: 298829 timestamp: 1479114348940 type: FILE visibility: PUBLIC, __spark_libs__/lift-json_2.11-2.6.3.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/lift-json_2.11-2.6.3.jar" } size: 477970 timestamp: 1487839577149 type: FILE visibility: PUBLIC, __spark_libs__/leveldbjni-all-1.8.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/leveldbjni-all-1.8.jar" } size: 1045744 timestamp: 1479114358383 type: FILE visibility: PUBLIC, __spark_libs__/hive-jdbc-1.2.1.spark2.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/hive-jdbc-1.2.1.spark2.jar" } size: 100680 timestamp: 1479114353442 type: FILE visibility: PUBLIC, __spark_libs__/jsr305-1.3.9.jar -> resource { scheme: "hdfs" host: "c408.hadoop.gda.lo" port: 8020 file: "/user/fairy/libs/spark2/jsr305-1.3.9.jar" } size: 33015 timestamp: 1479114358053 type: FILE visibility: PUBLIC)
17/05/16 11:10:29 INFO yarn.ApplicationMaster: ApplicationAttemptId: appattempt_1494395298335_303847_000001
17/05/16 11:10:29 INFO spark.SecurityManager: Changing view acls to: yarn,fairy
17/05/16 11:10:29 INFO spark.SecurityManager: Changing modify acls to: yarn,fairy
17/05/16 11:10:29 INFO spark.SecurityManager: Changing view acls groups to: 
17/05/16 11:10:29 INFO spark.SecurityManager: Changing modify acls groups to: 
17/05/16 11:10:29 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(yarn, fairy); groups with view permissions: Set(); users  with modify permissions: Set(yarn, fairy); groups with modify permissions: Set()
17/05/16 11:10:29 INFO yarn.ApplicationMaster: Starting the user application in a separate Thread
17/05/16 11:10:29 INFO yarn.ApplicationMaster: Waiting for spark context initialization
17/05/16 11:10:29 INFO yarn.ApplicationMaster: Waiting for spark context initialization ... 
17/05/16 11:10:29 INFO spark.SparkContext: Running Spark version 2.0.1
17/05/16 11:10:29 INFO spark.SecurityManager: Changing view acls to: yarn,fairy
17/05/16 11:10:29 INFO spark.SecurityManager: Changing modify acls to: yarn,fairy
17/05/16 11:10:29 INFO spark.SecurityManager: Changing view acls groups to: 
17/05/16 11:10:29 INFO spark.SecurityManager: Changing modify acls groups to: 
17/05/16 11:10:29 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(yarn, fairy); groups with view permissions: Set(); users  with modify permissions: Set(yarn, fairy); groups with modify permissions: Set()
17/05/16 11:10:29 INFO util.Utils: Successfully started service 'sparkDriver' on port 19210.
17/05/16 11:10:29 INFO spark.SparkEnv: Registering MapOutputTracker
17/05/16 11:10:29 INFO spark.SparkEnv: Registering BlockManagerMaster
17/05/16 11:10:29 INFO storage.DiskBlockManager: Created local directory at /hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/blockmgr-827f7935-adb2-4391-9d30-10f3c1f8b164
17/05/16 11:10:29 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MB
17/05/16 11:10:30 INFO spark.SparkEnv: Registering OutputCommitCoordinator
17/05/16 11:10:30 INFO util.log: Logging initialized @4126ms
17/05/16 11:10:30 INFO ui.JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
17/05/16 11:10:30 INFO server.Server: jetty-9.2.z-SNAPSHOT
17/05/16 11:10:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@44667231{/jobs,null,AVAILABLE}
17/05/16 11:10:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1e3432da{/jobs/json,null,AVAILABLE}
17/05/16 11:10:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4c8a2aa9{/jobs/job,null,AVAILABLE}
17/05/16 11:10:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64461174{/jobs/job/json,null,AVAILABLE}
17/05/16 11:10:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@13c60037{/stages,null,AVAILABLE}
17/05/16 11:10:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2c5b5277{/stages/json,null,AVAILABLE}
17/05/16 11:10:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3119fa37{/stages/stage,null,AVAILABLE}
17/05/16 11:10:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@650080d1{/stages/stage/json,null,AVAILABLE}
17/05/16 11:10:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4664e3fe{/stages/pool,null,AVAILABLE}
17/05/16 11:10:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@30dac9af{/stages/pool/json,null,AVAILABLE}
17/05/16 11:10:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4eec45c1{/storage,null,AVAILABLE}
17/05/16 11:10:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f0b52c5{/storage/json,null,AVAILABLE}
17/05/16 11:10:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4e113fc9{/storage/rdd,null,AVAILABLE}
17/05/16 11:10:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@28601015{/storage/rdd/json,null,AVAILABLE}
17/05/16 11:10:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@40e8a95{/environment,null,AVAILABLE}
17/05/16 11:10:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@71e66dab{/environment/json,null,AVAILABLE}
17/05/16 11:10:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@36cefb29{/executors,null,AVAILABLE}
17/05/16 11:10:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1e86b55d{/executors/json,null,AVAILABLE}
17/05/16 11:10:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6eacce3a{/executors/threadDump,null,AVAILABLE}
17/05/16 11:10:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2c657d71{/executors/threadDump/json,null,AVAILABLE}
17/05/16 11:10:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6db3dc67{/static,null,AVAILABLE}
17/05/16 11:10:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@46b4b0d1{/,null,AVAILABLE}
17/05/16 11:10:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4e616b23{/api,null,AVAILABLE}
17/05/16 11:10:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@49e8096b{/stages/stage/kill,null,AVAILABLE}
17/05/16 11:10:30 INFO server.ServerConnector: Started ServerConnector@74e7c4f5{HTTP/1.1}{0.0.0.0:6491}
17/05/16 11:10:30 INFO server.Server: Started @4386ms
17/05/16 11:10:30 INFO util.Utils: Successfully started service 'SparkUI' on port 6491.
17/05/16 11:10:30 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.60.43.33:6491
17/05/16 11:10:30 INFO cluster.YarnClusterScheduler: Created YarnClusterScheduler
17/05/16 11:10:30 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1494395298335_303847 and attemptId Some(appattempt_1494395298335_303847_000001)
17/05/16 11:10:30 INFO util.Utils: Using initial executors = 4, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances
17/05/16 11:10:30 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 5141.
17/05/16 11:10:30 INFO netty.NettyBlockTransferService: Server created on 10.60.43.33:5141
17/05/16 11:10:30 INFO storage.BlockManager: external shuffle service port = 7337
17/05/16 11:10:30 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.60.43.33, 5141)
17/05/16 11:10:30 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.60.43.33:5141 with 1458.6 MB RAM, BlockManagerId(driver, 10.60.43.33, 5141)
17/05/16 11:10:30 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.60.43.33, 5141)
17/05/16 11:10:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@633c4bff{/metrics/json,null,AVAILABLE}
17/05/16 11:10:31 INFO scheduler.EventLoggingListener: Logging events to hdfs:///spark-history/application_1494395298335_303847_1
17/05/16 11:10:31 INFO util.Utils: Using initial executors = 4, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances
17/05/16 11:10:31 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!
17/05/16 11:10:31 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark://YarnAM@10.60.43.33:19210)
17/05/16 11:10:31 INFO client.RMProxy: Connecting to ResourceManager at c428.hadoop.gda.lo/10.60.43.28:8030
17/05/16 11:10:31 INFO yarn.YarnRMClient: Registering the ApplicationMaster
17/05/16 11:10:31 INFO util.Utils: Using initial executors = 4, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances
17/05/16 11:10:31 INFO yarn.YarnAllocator: Will request 4 executor containers, each with 1 cores and 3456 MB memory including 384 MB overhead
17/05/16 11:10:31 INFO yarn.YarnAllocator: Canceled 0 container requests (locality no longer needed)
17/05/16 11:10:31 INFO yarn.YarnAllocator: Submitted container request (host: Any, capability: <memory:3456, vCores:1>)
17/05/16 11:10:31 INFO yarn.YarnAllocator: Submitted container request (host: Any, capability: <memory:3456, vCores:1>)
17/05/16 11:10:31 INFO yarn.YarnAllocator: Submitted container request (host: Any, capability: <memory:3456, vCores:1>)
17/05/16 11:10:31 INFO yarn.YarnAllocator: Submitted container request (host: Any, capability: <memory:3456, vCores:1>)
17/05/16 11:10:31 INFO yarn.ApplicationMaster: Started progress reporter thread with (heartbeat : 5000, initial allocation : 200) intervals
17/05/16 11:10:31 INFO impl.AMRMClientImpl: Received new token for : c432.hadoop.gda.lo:45454
17/05/16 11:10:31 INFO impl.AMRMClientImpl: Received new token for : c433.hadoop.gda.lo:45454
17/05/16 11:10:31 INFO impl.AMRMClientImpl: Received new token for : c411.hadoop.gda.lo:45454
17/05/16 11:10:31 INFO impl.AMRMClientImpl: Received new token for : c414.hadoop.gda.lo:45454
17/05/16 11:10:31 INFO yarn.YarnAllocator: Launching container container_e81_1494395298335_303847_01_000002 for on host c432.hadoop.gda.lo
17/05/16 11:10:31 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@10.60.43.33:19210,  executorHostname: c432.hadoop.gda.lo
17/05/16 11:10:31 INFO yarn.YarnAllocator: Launching container container_e81_1494395298335_303847_01_000003 for on host c433.hadoop.gda.lo
17/05/16 11:10:31 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@10.60.43.33:19210,  executorHostname: c433.hadoop.gda.lo
17/05/16 11:10:31 INFO yarn.YarnAllocator: Launching container container_e81_1494395298335_303847_01_000004 for on host c411.hadoop.gda.lo
17/05/16 11:10:31 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@10.60.43.33:19210,  executorHostname: c411.hadoop.gda.lo
17/05/16 11:10:31 INFO yarn.YarnAllocator: Launching container container_e81_1494395298335_303847_01_000005 for on host c414.hadoop.gda.lo
17/05/16 11:10:31 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@10.60.43.33:19210,  executorHostname: c414.hadoop.gda.lo
17/05/16 11:10:31 INFO yarn.YarnAllocator: Received 4 containers from YARN, launching executors on 4 of them.
17/05/16 11:10:31 INFO yarn.ExecutorRunnable: Starting Executor Container
17/05/16 11:10:31 INFO yarn.ExecutorRunnable: Starting Executor Container
17/05/16 11:10:31 INFO yarn.ExecutorRunnable: Starting Executor Container
17/05/16 11:10:31 INFO yarn.ExecutorRunnable: Starting Executor Container
17/05/16 11:10:31 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
17/05/16 11:10:31 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
17/05/16 11:10:31 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
17/05/16 11:10:31 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
17/05/16 11:10:31 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
17/05/16 11:10:31 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
17/05/16 11:10:31 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
17/05/16 11:10:31 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
17/05/16 11:10:32 INFO yarn.ExecutorRunnable: 
===============================================================================
YARN executor launch context:
  env:
    CLASSPATH -> {{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark_libs__/*<CPS>$HADOOP_CONF_DIR<CPS>/usr/hdp/current/hadoop-client/*<CPS>/usr/hdp/current/hadoop-client/lib/*<CPS>/usr/hdp/current/hadoop-hdfs-client/*<CPS>/usr/hdp/current/hadoop-hdfs-client/lib/*<CPS>/usr/hdp/current/hadoop-yarn-client/*<CPS>/usr/hdp/current/hadoop-yarn-client/lib/*<CPS>$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.4.2.0-258/hadoop/lib/hadoop-lzo-0.6.0.2.4.2.0-258.jar:/etc/hadoop/conf/secure
    SPARK_LOG_URL_STDERR -> http://c432.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_01_000002/fairy/stderr?start=-4096
    SPARK_YARN_STAGING_DIR -> hdfs://c408.hadoop.gda.lo:8020/user/fairy/.sparkStaging/application_1494395298335_303847
    SPARK_USER -> fairy
    SPARK_YARN_MODE -> true
    SPARK_LOG_URL_STDOUT -> http://c432.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_01_000002/fairy/stdout?start=-4096

  command:
    {{JAVA_HOME}}/bin/java -server -Xmx3072m '-Dhdp.version=2.4.2.0-258' -Djava.io.tmpdir={{PWD}}/tmp '-Dspark.history.ui.port=18080' '-Dspark.ui.port=0' '-Dspark.driver.port=19210' -Dspark.yarn.app.container.log.dir=<LOG_DIR> -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.60.43.33:19210 --executor-id 1 --hostname c432.hadoop.gda.lo --cores 1 --app-id application_1494395298335_303847 --user-class-path file:$PWD/__app__.jar 1> <LOG_DIR>/stdout 2> <LOG_DIR>/stderr
===============================================================================
      
17/05/16 11:10:32 INFO yarn.ExecutorRunnable: 
===============================================================================
YARN executor launch context:
  env:
    CLASSPATH -> {{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark_libs__/*<CPS>$HADOOP_CONF_DIR<CPS>/usr/hdp/current/hadoop-client/*<CPS>/usr/hdp/current/hadoop-client/lib/*<CPS>/usr/hdp/current/hadoop-hdfs-client/*<CPS>/usr/hdp/current/hadoop-hdfs-client/lib/*<CPS>/usr/hdp/current/hadoop-yarn-client/*<CPS>/usr/hdp/current/hadoop-yarn-client/lib/*<CPS>$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.4.2.0-258/hadoop/lib/hadoop-lzo-0.6.0.2.4.2.0-258.jar:/etc/hadoop/conf/secure
    SPARK_LOG_URL_STDERR -> http://c433.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_01_000003/fairy/stderr?start=-4096
    SPARK_YARN_STAGING_DIR -> hdfs://c408.hadoop.gda.lo:8020/user/fairy/.sparkStaging/application_1494395298335_303847
    SPARK_USER -> fairy
    SPARK_YARN_MODE -> true
    SPARK_LOG_URL_STDOUT -> http://c433.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_01_000003/fairy/stdout?start=-4096

  command:
    {{JAVA_HOME}}/bin/java -server -Xmx3072m '-Dhdp.version=2.4.2.0-258' -Djava.io.tmpdir={{PWD}}/tmp '-Dspark.history.ui.port=18080' '-Dspark.ui.port=0' '-Dspark.driver.port=19210' -Dspark.yarn.app.container.log.dir=<LOG_DIR> -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.60.43.33:19210 --executor-id 2 --hostname c433.hadoop.gda.lo --cores 1 --app-id application_1494395298335_303847 --user-class-path file:$PWD/__app__.jar 1> <LOG_DIR>/stdout 2> <LOG_DIR>/stderr
===============================================================================
      
17/05/16 11:10:32 INFO yarn.ExecutorRunnable: 
===============================================================================
YARN executor launch context:
  env:
    CLASSPATH -> {{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark_libs__/*<CPS>$HADOOP_CONF_DIR<CPS>/usr/hdp/current/hadoop-client/*<CPS>/usr/hdp/current/hadoop-client/lib/*<CPS>/usr/hdp/current/hadoop-hdfs-client/*<CPS>/usr/hdp/current/hadoop-hdfs-client/lib/*<CPS>/usr/hdp/current/hadoop-yarn-client/*<CPS>/usr/hdp/current/hadoop-yarn-client/lib/*<CPS>$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.4.2.0-258/hadoop/lib/hadoop-lzo-0.6.0.2.4.2.0-258.jar:/etc/hadoop/conf/secure
    SPARK_LOG_URL_STDERR -> http://c411.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_01_000004/fairy/stderr?start=-4096
    SPARK_YARN_STAGING_DIR -> hdfs://c408.hadoop.gda.lo:8020/user/fairy/.sparkStaging/application_1494395298335_303847
    SPARK_USER -> fairy
    SPARK_YARN_MODE -> true
    SPARK_LOG_URL_STDOUT -> http://c411.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_01_000004/fairy/stdout?start=-4096

  command:
    {{JAVA_HOME}}/bin/java -server -Xmx3072m '-Dhdp.version=2.4.2.0-258' -Djava.io.tmpdir={{PWD}}/tmp '-Dspark.history.ui.port=18080' '-Dspark.ui.port=0' '-Dspark.driver.port=19210' -Dspark.yarn.app.container.log.dir=<LOG_DIR> -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.60.43.33:19210 --executor-id 3 --hostname c411.hadoop.gda.lo --cores 1 --app-id application_1494395298335_303847 --user-class-path file:$PWD/__app__.jar 1> <LOG_DIR>/stdout 2> <LOG_DIR>/stderr
===============================================================================
      
17/05/16 11:10:32 INFO yarn.ExecutorRunnable: 
===============================================================================
YARN executor launch context:
  env:
    CLASSPATH -> {{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark_libs__/*<CPS>$HADOOP_CONF_DIR<CPS>/usr/hdp/current/hadoop-client/*<CPS>/usr/hdp/current/hadoop-client/lib/*<CPS>/usr/hdp/current/hadoop-hdfs-client/*<CPS>/usr/hdp/current/hadoop-hdfs-client/lib/*<CPS>/usr/hdp/current/hadoop-yarn-client/*<CPS>/usr/hdp/current/hadoop-yarn-client/lib/*<CPS>$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.4.2.0-258/hadoop/lib/hadoop-lzo-0.6.0.2.4.2.0-258.jar:/etc/hadoop/conf/secure
    SPARK_LOG_URL_STDERR -> http://c414.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_01_000005/fairy/stderr?start=-4096
    SPARK_YARN_STAGING_DIR -> hdfs://c408.hadoop.gda.lo:8020/user/fairy/.sparkStaging/application_1494395298335_303847
    SPARK_USER -> fairy
    SPARK_YARN_MODE -> true
    SPARK_LOG_URL_STDOUT -> http://c414.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_01_000005/fairy/stdout?start=-4096

  command:
    {{JAVA_HOME}}/bin/java -server -Xmx3072m '-Dhdp.version=2.4.2.0-258' -Djava.io.tmpdir={{PWD}}/tmp '-Dspark.history.ui.port=18080' '-Dspark.ui.port=0' '-Dspark.driver.port=19210' -Dspark.yarn.app.container.log.dir=<LOG_DIR> -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.60.43.33:19210 --executor-id 4 --hostname c414.hadoop.gda.lo --cores 1 --app-id application_1494395298335_303847 --user-class-path file:$PWD/__app__.jar 1> <LOG_DIR>/stdout 2> <LOG_DIR>/stderr
===============================================================================
      
17/05/16 11:10:32 INFO impl.ContainerManagementProtocolProxy: Opening proxy : c411.hadoop.gda.lo:45454
17/05/16 11:10:32 INFO impl.ContainerManagementProtocolProxy: Opening proxy : c414.hadoop.gda.lo:45454
17/05/16 11:10:32 INFO impl.ContainerManagementProtocolProxy: Opening proxy : c433.hadoop.gda.lo:45454
17/05/16 11:10:32 INFO impl.ContainerManagementProtocolProxy: Opening proxy : c432.hadoop.gda.lo:45454
17/05/16 11:10:35 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(null) (10.60.43.11:49042) with ID 3
17/05/16 11:10:36 INFO spark.ExecutorAllocationManager: New executor 3 has registered (new total is 1)
17/05/16 11:10:36 INFO storage.BlockManagerMasterEndpoint: Registering block manager c411.hadoop.gda.lo:27572 with 1458.6 MB RAM, BlockManagerId(3, c411.hadoop.gda.lo, 27572)
17/05/16 11:10:36 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(null) (10.60.43.33:8328) with ID 2
17/05/16 11:10:36 INFO spark.ExecutorAllocationManager: New executor 2 has registered (new total is 2)
17/05/16 11:10:36 INFO storage.BlockManagerMasterEndpoint: Registering block manager c433.hadoop.gda.lo:28145 with 1458.6 MB RAM, BlockManagerId(2, c433.hadoop.gda.lo, 28145)
17/05/16 11:10:37 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(null) (10.60.43.32:46409) with ID 1
17/05/16 11:10:37 INFO spark.ExecutorAllocationManager: New executor 1 has registered (new total is 3)
17/05/16 11:10:38 INFO storage.BlockManagerMasterEndpoint: Registering block manager c432.hadoop.gda.lo:14031 with 1458.6 MB RAM, BlockManagerId(1, c432.hadoop.gda.lo, 14031)
17/05/16 11:10:38 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(null) (10.60.43.14:57606) with ID 4
17/05/16 11:10:38 INFO spark.ExecutorAllocationManager: New executor 4 has registered (new total is 4)
17/05/16 11:10:38 INFO cluster.YarnClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
17/05/16 11:10:38 INFO cluster.YarnClusterScheduler: YarnClusterScheduler.postStartHook done
17/05/16 11:10:38 WARN spark.SparkContext: Use an existing SparkContext, some configuration may not take effect.
17/05/16 11:10:38 INFO storage.BlockManagerMasterEndpoint: Registering block manager c414.hadoop.gda.lo:34712 with 1458.6 MB RAM, BlockManagerId(4, c414.hadoop.gda.lo, 34712)
17/05/16 11:10:38 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4c168298{/SQL,null,AVAILABLE}
17/05/16 11:10:38 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@22088c59{/SQL/json,null,AVAILABLE}
17/05/16 11:10:38 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@114c398e{/SQL/execution,null,AVAILABLE}
17/05/16 11:10:38 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5cc07f9d{/SQL/execution/json,null,AVAILABLE}
17/05/16 11:10:38 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@788c9c6c{/static/sql,null,AVAILABLE}
17/05/16 11:10:38 INFO hive.HiveSharedState: Warehouse path is '/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_01_000001/spark-warehouse'.
17/05/16 11:10:40 INFO hive.HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17/05/16 11:10:41 INFO metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/05/16 11:10:41 INFO metastore.ObjectStore: ObjectStore, initialize called
17/05/16 11:10:41 INFO DataNucleus.Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/05/16 11:10:41 INFO DataNucleus.Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/05/16 11:10:44 INFO metastore.ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/05/16 11:10:45 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/05/16 11:10:45 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/05/16 11:10:46 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/05/16 11:10:46 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/05/16 11:10:47 INFO metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/05/16 11:10:47 INFO metastore.ObjectStore: Initialized ObjectStore
17/05/16 11:10:47 WARN metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/05/16 11:10:47 WARN metastore.ObjectStore: Failed to get database default, returning NoSuchObjectException
17/05/16 11:10:47 INFO metastore.HiveMetaStore: Added admin role in metastore
17/05/16 11:10:47 INFO metastore.HiveMetaStore: Added public role in metastore
17/05/16 11:10:47 INFO metastore.HiveMetaStore: No user is added in admin role, since config is empty
17/05/16 11:10:47 INFO metastore.HiveMetaStore: 0: get_all_databases
17/05/16 11:10:47 INFO HiveMetaStore.audit: ugi=fairy	ip=unknown-ip-addr	cmd=get_all_databases	
17/05/16 11:10:47 INFO metastore.HiveMetaStore: 0: get_functions: db=default pat=*
17/05/16 11:10:47 INFO HiveMetaStore.audit: ugi=fairy	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/05/16 11:10:47 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/05/16 11:10:48 INFO session.SessionState: Created local directory: /hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_01_000001/tmp/yarn
17/05/16 11:10:48 INFO session.SessionState: Created local directory: /hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_01_000001/tmp/a9db39f9-7417-4ff1-9f9a-918ae715e7bd_resources
17/05/16 11:10:48 INFO session.SessionState: Created HDFS directory: /tmp/hive/fairy/a9db39f9-7417-4ff1-9f9a-918ae715e7bd
17/05/16 11:10:48 INFO session.SessionState: Created local directory: /hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_01_000001/tmp/yarn/a9db39f9-7417-4ff1-9f9a-918ae715e7bd
17/05/16 11:10:48 INFO session.SessionState: Created HDFS directory: /tmp/hive/fairy/a9db39f9-7417-4ff1-9f9a-918ae715e7bd/_tmp_space.db
17/05/16 11:10:48 INFO client.HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is /hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_01_000001/spark-warehouse
17/05/16 11:10:48 INFO session.SessionState: Created local directory: /hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_01_000001/tmp/7c3e5a96-fff6-435e-b346-1e159c8466f0_resources
17/05/16 11:10:48 INFO session.SessionState: Created HDFS directory: /tmp/hive/fairy/7c3e5a96-fff6-435e-b346-1e159c8466f0
17/05/16 11:10:48 INFO session.SessionState: Created local directory: /hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_01_000001/tmp/yarn/7c3e5a96-fff6-435e-b346-1e159c8466f0
17/05/16 11:10:48 INFO session.SessionState: Created HDFS directory: /tmp/hive/fairy/7c3e5a96-fff6-435e-b346-1e159c8466f0/_tmp_space.db
17/05/16 11:10:48 INFO client.HiveClientImpl: Warehouse location for Hive client (version 1.2.1) is /hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_01_000001/spark-warehouse
17/05/16 11:10:48 INFO metastore.HiveMetaStore: 0: create_database: Database(name:default, description:default database, locationUri:hdfs://c408.hadoop.gda.lo:8020/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_01_000001/spark-warehouse, parameters:{})
17/05/16 11:10:48 INFO HiveMetaStore.audit: ugi=fairy	ip=unknown-ip-addr	cmd=create_database: Database(name:default, description:default database, locationUri:hdfs://c408.hadoop.gda.lo:8020/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_01_000001/spark-warehouse, parameters:{})	
17/05/16 11:10:48 ERROR metastore.RetryingHMSHandler: AlreadyExistsException(message:Database default already exists)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:891)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at com.sun.proxy.$Proxy30.create_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:644)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
	at com.sun.proxy.$Proxy31.createDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Hive.java:306)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply$mcV$sp(HiveClientImpl.scala:309)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:309)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:309)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:280)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:227)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:226)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:269)
	at org.apache.spark.sql.hive.client.HiveClientImpl.createDatabase(HiveClientImpl.scala:308)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply$mcV$sp(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:99)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:72)
	at org.apache.spark.sql.hive.HiveExternalCatalog.createDatabase(HiveExternalCatalog.scala:98)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:147)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.<init>(SessionCatalog.scala:89)
	at org.apache.spark.sql.hive.HiveSessionCatalog.<init>(HiveSessionCatalog.scala:51)
	at org.apache.spark.sql.hive.HiveSessionState.catalog$lzycompute(HiveSessionState.scala:49)
	at org.apache.spark.sql.hive.HiveSessionState.catalog(HiveSessionState.scala:48)
	at org.apache.spark.sql.hive.HiveSessionState$$anon$1.<init>(HiveSessionState.scala:63)
	at org.apache.spark.sql.hive.HiveSessionState.analyzer$lzycompute(HiveSessionState.scala:63)
	at org.apache.spark.sql.hive.HiveSessionState.analyzer(HiveSessionState.scala:62)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:542)
	at org.apache.spark.sql.SparkSession.createDataFrame(SparkSession.scala:302)
	at vng.ge.stats.report.base.DataReader.loadFile(DataReader.scala:179)
	at vng.ge.stats.report.loader.GameDataLoader.run(GameDataLoader.scala:32)
	at vng.ge.stats.report.base.DataPool.report(DataPool.scala:44)
	at vng.ge.stats.report.job.Runner$$anonfun$run$2$$anonfun$apply$1.apply$mcV$sp(Runner.scala:76)
	at scala.util.control.Breaks.breakable(Breaks.scala:38)
	at vng.ge.stats.report.job.Runner$$anonfun$run$2.apply(Runner.scala:56)
	at vng.ge.stats.report.job.Runner$$anonfun$run$2.apply(Runner.scala:44)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at vng.ge.stats.report.job.Runner$.run(Runner.scala:44)
	at vng.ge.stats.report.job.Runner$.main(Runner.scala:19)
	at vng.ge.stats.report.job.Runner.main(Runner.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:627)

17/05/16 11:10:50 INFO spark.ContextCleaner: Cleaned accumulator 0
17/05/16 11:10:50 INFO spark.ContextCleaner: Cleaned accumulator 1
17/05/16 11:10:50 INFO codegen.CodeGenerator: Code generated in 587.979627 ms
17/05/16 11:10:50 INFO codegen.CodeGenerator: Code generated in 79.416693 ms
17/05/16 11:10:50 INFO codegen.CodeGenerator: Code generated in 91.325561 ms
17/05/16 11:10:50 INFO spark.SparkContext: Starting job: first at CcuReport.scala:39
17/05/16 11:10:50 INFO scheduler.DAGScheduler: Registering RDD 14 (first at CcuReport.scala:39)
17/05/16 11:10:50 INFO scheduler.DAGScheduler: Registering RDD 17 (first at CcuReport.scala:39)
17/05/16 11:10:50 INFO scheduler.DAGScheduler: Got job 0 (first at CcuReport.scala:39) with 1 output partitions
17/05/16 11:10:50 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (first at CcuReport.scala:39)
17/05/16 11:10:50 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
17/05/16 11:10:50 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 1)
17/05/16 11:10:50 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[17] at first at CcuReport.scala:39), which has no missing parents
17/05/16 11:10:51 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 30.3 KB, free 1458.6 MB)
17/05/16 11:10:51 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.2 KB, free 1458.6 MB)
17/05/16 11:10:51 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.60.43.33:5141 (size: 13.2 KB, free: 1458.6 MB)
17/05/16 11:10:51 INFO spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
17/05/16 11:10:51 INFO scheduler.DAGScheduler: Submitting 200 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[17] at first at CcuReport.scala:39)
17/05/16 11:10:51 INFO cluster.YarnClusterScheduler: Adding task set 1.0 with 200 tasks
17/05/16 11:10:51 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 0, c414.hadoop.gda.lo, partition 0, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:51 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 1, c411.hadoop.gda.lo, partition 1, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:51 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 1.0 (TID 2, c433.hadoop.gda.lo, partition 2, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:51 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 1.0 (TID 3, c432.hadoop.gda.lo, partition 3, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:51 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 0 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:51 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 1 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:51 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 2 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:51 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 3 on executor id: 1 hostname: c432.hadoop.gda.lo.
17/05/16 11:10:51 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on c414.hadoop.gda.lo:34712 (size: 13.2 KB, free: 1458.6 MB)
17/05/16 11:10:51 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on c411.hadoop.gda.lo:27572 (size: 13.2 KB, free: 1458.6 MB)
17/05/16 11:10:51 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on c433.hadoop.gda.lo:28145 (size: 13.2 KB, free: 1458.6 MB)
17/05/16 11:10:51 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on c432.hadoop.gda.lo:14031 (size: 13.2 KB, free: 1458.6 MB)
17/05/16 11:10:51 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.60.43.14:57606
17/05/16 11:10:51 INFO spark.MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 83 bytes
17/05/16 11:10:51 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.60.43.11:49042
17/05/16 11:10:51 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.60.43.33:8328
17/05/16 11:10:52 INFO yarn.YarnAllocator: Driver requested a total number of 5 executor(s).
17/05/16 11:10:52 INFO spark.ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 5)
17/05/16 11:10:52 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.60.43.32:46409
17/05/16 11:10:52 INFO yarn.YarnAllocator: Will request 1 executor containers, each with 1 cores and 3456 MB memory including 384 MB overhead
17/05/16 11:10:52 INFO yarn.YarnAllocator: Canceled 0 container requests (locality no longer needed)
17/05/16 11:10:52 INFO yarn.YarnAllocator: Submitted container request (host: Any, capability: <memory:3456, vCores:1>)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 1.0 (TID 4, c411.hadoop.gda.lo, partition 4, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 4 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 1.0 (TID 5, c414.hadoop.gda.lo, partition 5, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 5 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:52 INFO impl.AMRMClientImpl: Received new token for : c427.hadoop.gda.lo:45454
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 1) in 1238 ms on c411.hadoop.gda.lo (1/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 0) in 1277 ms on c414.hadoop.gda.lo (2/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 1.0 (TID 6, c411.hadoop.gda.lo, partition 6, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 6 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 1.0 (TID 4) in 41 ms on c411.hadoop.gda.lo (3/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 1.0 (TID 7, c414.hadoop.gda.lo, partition 7, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 7 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 1.0 (TID 5) in 39 ms on c414.hadoop.gda.lo (4/200)
17/05/16 11:10:52 INFO yarn.YarnAllocator: Launching container container_e81_1494395298335_303847_01_000006 for on host c427.hadoop.gda.lo
17/05/16 11:10:52 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@10.60.43.33:19210,  executorHostname: c427.hadoop.gda.lo
17/05/16 11:10:52 INFO yarn.YarnAllocator: Received 1 containers from YARN, launching executors on 1 of them.
17/05/16 11:10:52 INFO yarn.ExecutorRunnable: Starting Executor Container
17/05/16 11:10:52 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
17/05/16 11:10:52 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
17/05/16 11:10:52 INFO yarn.ExecutorRunnable: 
===============================================================================
YARN executor launch context:
  env:
    CLASSPATH -> {{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark_libs__/*<CPS>$HADOOP_CONF_DIR<CPS>/usr/hdp/current/hadoop-client/*<CPS>/usr/hdp/current/hadoop-client/lib/*<CPS>/usr/hdp/current/hadoop-hdfs-client/*<CPS>/usr/hdp/current/hadoop-hdfs-client/lib/*<CPS>/usr/hdp/current/hadoop-yarn-client/*<CPS>/usr/hdp/current/hadoop-yarn-client/lib/*<CPS>$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.4.2.0-258/hadoop/lib/hadoop-lzo-0.6.0.2.4.2.0-258.jar:/etc/hadoop/conf/secure
    SPARK_LOG_URL_STDERR -> http://c427.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_01_000006/fairy/stderr?start=-4096
    SPARK_YARN_STAGING_DIR -> hdfs://c408.hadoop.gda.lo:8020/user/fairy/.sparkStaging/application_1494395298335_303847
    SPARK_USER -> fairy
    SPARK_YARN_MODE -> true
    SPARK_LOG_URL_STDOUT -> http://c427.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_01_000006/fairy/stdout?start=-4096

  command:
    {{JAVA_HOME}}/bin/java -server -Xmx3072m '-Dhdp.version=2.4.2.0-258' -Djava.io.tmpdir={{PWD}}/tmp '-Dspark.history.ui.port=18080' '-Dspark.ui.port=0' '-Dspark.driver.port=19210' -Dspark.yarn.app.container.log.dir=<LOG_DIR> -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.60.43.33:19210 --executor-id 5 --hostname c427.hadoop.gda.lo --cores 1 --app-id application_1494395298335_303847 --user-class-path file:$PWD/__app__.jar 1> <LOG_DIR>/stdout 2> <LOG_DIR>/stderr
===============================================================================
      
17/05/16 11:10:52 INFO impl.ContainerManagementProtocolProxy: Opening proxy : c427.hadoop.gda.lo:45454
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 1.0 (TID 8, c414.hadoop.gda.lo, partition 8, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 8 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 1.0 (TID 7) in 26 ms on c414.hadoop.gda.lo (5/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 1.0 (TID 9, c411.hadoop.gda.lo, partition 9, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 9 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 1.0 (TID 6) in 33 ms on c411.hadoop.gda.lo (6/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 1.0 (TID 10, c414.hadoop.gda.lo, partition 10, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 10 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 1.0 (TID 8) in 24 ms on c414.hadoop.gda.lo (7/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 1.0 (TID 11, c411.hadoop.gda.lo, partition 11, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 11 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 1.0 (TID 9) in 25 ms on c411.hadoop.gda.lo (8/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 1.0 (TID 12, c414.hadoop.gda.lo, partition 12, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 12 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 1.0 (TID 10) in 23 ms on c414.hadoop.gda.lo (9/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 1.0 (TID 13, c411.hadoop.gda.lo, partition 13, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 13 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 1.0 (TID 11) in 30 ms on c411.hadoop.gda.lo (10/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 1.0 (TID 14, c414.hadoop.gda.lo, partition 14, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 14 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 1.0 (TID 12) in 20 ms on c414.hadoop.gda.lo (11/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 1.0 (TID 15, c433.hadoop.gda.lo, partition 15, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 15 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 1.0 (TID 2) in 1363 ms on c433.hadoop.gda.lo (12/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 16.0 in stage 1.0 (TID 16, c414.hadoop.gda.lo, partition 16, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 16 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 1.0 (TID 14) in 18 ms on c414.hadoop.gda.lo (13/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 17.0 in stage 1.0 (TID 17, c411.hadoop.gda.lo, partition 17, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 17 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 1.0 (TID 13) in 26 ms on c411.hadoop.gda.lo (14/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 18.0 in stage 1.0 (TID 18, c411.hadoop.gda.lo, partition 18, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 18 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 17.0 in stage 1.0 (TID 17) in 25 ms on c411.hadoop.gda.lo (15/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 19.0 in stage 1.0 (TID 19, c414.hadoop.gda.lo, partition 19, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 19 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 16.0 in stage 1.0 (TID 16) in 32 ms on c414.hadoop.gda.lo (16/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 20.0 in stage 1.0 (TID 20, c433.hadoop.gda.lo, partition 20, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 20 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 1.0 (TID 15) in 49 ms on c433.hadoop.gda.lo (17/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 21.0 in stage 1.0 (TID 21, c411.hadoop.gda.lo, partition 21, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 21 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 18.0 in stage 1.0 (TID 18) in 28 ms on c411.hadoop.gda.lo (18/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 22.0 in stage 1.0 (TID 22, c414.hadoop.gda.lo, partition 22, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 22 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 19.0 in stage 1.0 (TID 19) in 25 ms on c414.hadoop.gda.lo (19/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 23.0 in stage 1.0 (TID 23, c411.hadoop.gda.lo, partition 23, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 23 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 21.0 in stage 1.0 (TID 21) in 22 ms on c411.hadoop.gda.lo (20/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 24.0 in stage 1.0 (TID 24, c433.hadoop.gda.lo, partition 24, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 24 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 20.0 in stage 1.0 (TID 20) in 34 ms on c433.hadoop.gda.lo (21/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 25.0 in stage 1.0 (TID 25, c414.hadoop.gda.lo, partition 25, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 25 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 22.0 in stage 1.0 (TID 22) in 23 ms on c414.hadoop.gda.lo (22/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 26.0 in stage 1.0 (TID 26, c411.hadoop.gda.lo, partition 26, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 23.0 in stage 1.0 (TID 23) in 24 ms on c411.hadoop.gda.lo (23/200)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 26 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 27.0 in stage 1.0 (TID 27, c414.hadoop.gda.lo, partition 27, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 27 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 25.0 in stage 1.0 (TID 25) in 21 ms on c414.hadoop.gda.lo (24/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 28.0 in stage 1.0 (TID 28, c433.hadoop.gda.lo, partition 28, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 28 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 24.0 in stage 1.0 (TID 24) in 28 ms on c433.hadoop.gda.lo (25/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 29.0 in stage 1.0 (TID 29, c414.hadoop.gda.lo, partition 29, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 29 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 27.0 in stage 1.0 (TID 27) in 20 ms on c414.hadoop.gda.lo (26/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 30.0 in stage 1.0 (TID 30, c411.hadoop.gda.lo, partition 30, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 30 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 26.0 in stage 1.0 (TID 26) in 28 ms on c411.hadoop.gda.lo (27/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 31.0 in stage 1.0 (TID 31, c433.hadoop.gda.lo, partition 31, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 31 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 28.0 in stage 1.0 (TID 28) in 30 ms on c433.hadoop.gda.lo (28/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 32.0 in stage 1.0 (TID 32, c414.hadoop.gda.lo, partition 32, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 32 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 29.0 in stage 1.0 (TID 29) in 19 ms on c414.hadoop.gda.lo (29/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 33.0 in stage 1.0 (TID 33, c411.hadoop.gda.lo, partition 33, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 33 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 30.0 in stage 1.0 (TID 30) in 23 ms on c411.hadoop.gda.lo (30/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 34.0 in stage 1.0 (TID 34, c433.hadoop.gda.lo, partition 34, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 34 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 31.0 in stage 1.0 (TID 31) in 26 ms on c433.hadoop.gda.lo (31/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 35.0 in stage 1.0 (TID 35, c411.hadoop.gda.lo, partition 35, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 35 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 33.0 in stage 1.0 (TID 33) in 19 ms on c411.hadoop.gda.lo (32/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 36.0 in stage 1.0 (TID 36, c414.hadoop.gda.lo, partition 36, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 36 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 32.0 in stage 1.0 (TID 32) in 29 ms on c414.hadoop.gda.lo (33/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 37.0 in stage 1.0 (TID 37, c411.hadoop.gda.lo, partition 37, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 37 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 35.0 in stage 1.0 (TID 35) in 18 ms on c411.hadoop.gda.lo (34/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 38.0 in stage 1.0 (TID 38, c433.hadoop.gda.lo, partition 38, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 38 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 34.0 in stage 1.0 (TID 34) in 25 ms on c433.hadoop.gda.lo (35/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 39.0 in stage 1.0 (TID 39, c414.hadoop.gda.lo, partition 39, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 39 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 36.0 in stage 1.0 (TID 36) in 21 ms on c414.hadoop.gda.lo (36/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 40.0 in stage 1.0 (TID 40, c411.hadoop.gda.lo, partition 40, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 40 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 37.0 in stage 1.0 (TID 37) in 19 ms on c411.hadoop.gda.lo (37/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 41.0 in stage 1.0 (TID 41, c414.hadoop.gda.lo, partition 41, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 41 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 39.0 in stage 1.0 (TID 39) in 22 ms on c414.hadoop.gda.lo (38/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 42.0 in stage 1.0 (TID 42, c433.hadoop.gda.lo, partition 42, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 42 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 38.0 in stage 1.0 (TID 38) in 29 ms on c433.hadoop.gda.lo (39/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 43.0 in stage 1.0 (TID 43, c411.hadoop.gda.lo, partition 43, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 43 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 40.0 in stage 1.0 (TID 40) in 21 ms on c411.hadoop.gda.lo (40/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 44.0 in stage 1.0 (TID 44, c414.hadoop.gda.lo, partition 44, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 44 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 41.0 in stage 1.0 (TID 41) in 19 ms on c414.hadoop.gda.lo (41/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 45.0 in stage 1.0 (TID 45, c433.hadoop.gda.lo, partition 45, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 45 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 42.0 in stage 1.0 (TID 42) in 28 ms on c433.hadoop.gda.lo (42/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 46.0 in stage 1.0 (TID 46, c414.hadoop.gda.lo, partition 46, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 46 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 44.0 in stage 1.0 (TID 44) in 18 ms on c414.hadoop.gda.lo (43/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 47.0 in stage 1.0 (TID 47, c411.hadoop.gda.lo, partition 47, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 47 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 43.0 in stage 1.0 (TID 43) in 32 ms on c411.hadoop.gda.lo (44/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 48.0 in stage 1.0 (TID 48, c414.hadoop.gda.lo, partition 48, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 48 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 46.0 in stage 1.0 (TID 46) in 15 ms on c414.hadoop.gda.lo (45/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 49.0 in stage 1.0 (TID 49, c433.hadoop.gda.lo, partition 49, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 49 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 45.0 in stage 1.0 (TID 45) in 24 ms on c433.hadoop.gda.lo (46/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 50.0 in stage 1.0 (TID 50, c414.hadoop.gda.lo, partition 50, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 50 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 48.0 in stage 1.0 (TID 48) in 17 ms on c414.hadoop.gda.lo (47/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 51.0 in stage 1.0 (TID 51, c411.hadoop.gda.lo, partition 51, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 51 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 47.0 in stage 1.0 (TID 47) in 25 ms on c411.hadoop.gda.lo (48/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 52.0 in stage 1.0 (TID 52, c433.hadoop.gda.lo, partition 52, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 52 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 49.0 in stage 1.0 (TID 49) in 22 ms on c433.hadoop.gda.lo (49/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 53.0 in stage 1.0 (TID 53, c414.hadoop.gda.lo, partition 53, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 53 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 50.0 in stage 1.0 (TID 50) in 21 ms on c414.hadoop.gda.lo (50/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 54.0 in stage 1.0 (TID 54, c411.hadoop.gda.lo, partition 54, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 54 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 51.0 in stage 1.0 (TID 51) in 23 ms on c411.hadoop.gda.lo (51/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 55.0 in stage 1.0 (TID 55, c433.hadoop.gda.lo, partition 55, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 55 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 52.0 in stage 1.0 (TID 52) in 24 ms on c433.hadoop.gda.lo (52/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 56.0 in stage 1.0 (TID 56, c414.hadoop.gda.lo, partition 56, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 56 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 57.0 in stage 1.0 (TID 57, c411.hadoop.gda.lo, partition 57, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 57 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 54.0 in stage 1.0 (TID 54) in 21 ms on c411.hadoop.gda.lo (53/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 53.0 in stage 1.0 (TID 53) in 26 ms on c414.hadoop.gda.lo (54/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 58.0 in stage 1.0 (TID 58, c414.hadoop.gda.lo, partition 58, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 58 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 56.0 in stage 1.0 (TID 56) in 16 ms on c414.hadoop.gda.lo (55/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 59.0 in stage 1.0 (TID 59, c433.hadoop.gda.lo, partition 59, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 59 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 55.0 in stage 1.0 (TID 55) in 28 ms on c433.hadoop.gda.lo (56/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 60.0 in stage 1.0 (TID 60, c411.hadoop.gda.lo, partition 60, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 60 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 57.0 in stage 1.0 (TID 57) in 21 ms on c411.hadoop.gda.lo (57/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 61.0 in stage 1.0 (TID 61, c414.hadoop.gda.lo, partition 61, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 61 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 58.0 in stage 1.0 (TID 58) in 18 ms on c414.hadoop.gda.lo (58/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 62.0 in stage 1.0 (TID 62, c411.hadoop.gda.lo, partition 62, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 62 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 60.0 in stage 1.0 (TID 60) in 21 ms on c411.hadoop.gda.lo (59/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 63.0 in stage 1.0 (TID 63, c433.hadoop.gda.lo, partition 63, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 63 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 59.0 in stage 1.0 (TID 59) in 27 ms on c433.hadoop.gda.lo (60/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 64.0 in stage 1.0 (TID 64, c414.hadoop.gda.lo, partition 64, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 64 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 61.0 in stage 1.0 (TID 61) in 18 ms on c414.hadoop.gda.lo (61/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 65.0 in stage 1.0 (TID 65, c411.hadoop.gda.lo, partition 65, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 65 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 62.0 in stage 1.0 (TID 62) in 21 ms on c411.hadoop.gda.lo (62/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 66.0 in stage 1.0 (TID 66, c414.hadoop.gda.lo, partition 66, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 66 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 64.0 in stage 1.0 (TID 64) in 18 ms on c414.hadoop.gda.lo (63/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 67.0 in stage 1.0 (TID 67, c433.hadoop.gda.lo, partition 67, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 67 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 63.0 in stage 1.0 (TID 63) in 26 ms on c433.hadoop.gda.lo (64/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 68.0 in stage 1.0 (TID 68, c414.hadoop.gda.lo, partition 68, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 68 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 66.0 in stage 1.0 (TID 66) in 16 ms on c414.hadoop.gda.lo (65/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 69.0 in stage 1.0 (TID 69, c411.hadoop.gda.lo, partition 69, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 69 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 65.0 in stage 1.0 (TID 65) in 23 ms on c411.hadoop.gda.lo (66/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 70.0 in stage 1.0 (TID 70, c433.hadoop.gda.lo, partition 70, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 70 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 67.0 in stage 1.0 (TID 67) in 27 ms on c433.hadoop.gda.lo (67/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 71.0 in stage 1.0 (TID 71, c414.hadoop.gda.lo, partition 71, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 71 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 68.0 in stage 1.0 (TID 68) in 16 ms on c414.hadoop.gda.lo (68/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 72.0 in stage 1.0 (TID 72, c411.hadoop.gda.lo, partition 72, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 72 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 69.0 in stage 1.0 (TID 69) in 23 ms on c411.hadoop.gda.lo (69/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 73.0 in stage 1.0 (TID 73, c414.hadoop.gda.lo, partition 73, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 73 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 71.0 in stage 1.0 (TID 71) in 18 ms on c414.hadoop.gda.lo (70/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 74.0 in stage 1.0 (TID 74, c433.hadoop.gda.lo, partition 74, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 74 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 70.0 in stage 1.0 (TID 70) in 29 ms on c433.hadoop.gda.lo (71/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 75.0 in stage 1.0 (TID 75, c414.hadoop.gda.lo, partition 75, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 75 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 73.0 in stage 1.0 (TID 73) in 14 ms on c414.hadoop.gda.lo (72/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 76.0 in stage 1.0 (TID 76, c411.hadoop.gda.lo, partition 76, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 76 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 72.0 in stage 1.0 (TID 72) in 24 ms on c411.hadoop.gda.lo (73/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 77.0 in stage 1.0 (TID 77, c414.hadoop.gda.lo, partition 77, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 77 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 75.0 in stage 1.0 (TID 75) in 15 ms on c414.hadoop.gda.lo (74/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 78.0 in stage 1.0 (TID 78, c433.hadoop.gda.lo, partition 78, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 78 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 74.0 in stage 1.0 (TID 74) in 23 ms on c433.hadoop.gda.lo (75/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 79.0 in stage 1.0 (TID 79, c411.hadoop.gda.lo, partition 79, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 79 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 76.0 in stage 1.0 (TID 76) in 26 ms on c411.hadoop.gda.lo (76/200)
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Starting task 80.0 in stage 1.0 (TID 80, c414.hadoop.gda.lo, partition 80, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 80 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:52 INFO scheduler.TaskSetManager: Finished task 77.0 in stage 1.0 (TID 77) in 16 ms on c414.hadoop.gda.lo (77/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 81.0 in stage 1.0 (TID 81, c433.hadoop.gda.lo, partition 81, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 81 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 78.0 in stage 1.0 (TID 78) in 26 ms on c433.hadoop.gda.lo (78/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 82.0 in stage 1.0 (TID 82, c411.hadoop.gda.lo, partition 82, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 82 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 79.0 in stage 1.0 (TID 79) in 21 ms on c411.hadoop.gda.lo (79/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 83.0 in stage 1.0 (TID 83, c414.hadoop.gda.lo, partition 83, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 83 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 80.0 in stage 1.0 (TID 80) in 22 ms on c414.hadoop.gda.lo (80/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 84.0 in stage 1.0 (TID 84, c433.hadoop.gda.lo, partition 84, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 84 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 81.0 in stage 1.0 (TID 81) in 22 ms on c433.hadoop.gda.lo (81/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 85.0 in stage 1.0 (TID 85, c414.hadoop.gda.lo, partition 85, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 85 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 86.0 in stage 1.0 (TID 86, c411.hadoop.gda.lo, partition 86, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 86 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 83.0 in stage 1.0 (TID 83) in 20 ms on c414.hadoop.gda.lo (82/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 82.0 in stage 1.0 (TID 82) in 22 ms on c411.hadoop.gda.lo (83/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 87.0 in stage 1.0 (TID 87, c414.hadoop.gda.lo, partition 87, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 87 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 85.0 in stage 1.0 (TID 85) in 15 ms on c414.hadoop.gda.lo (84/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 88.0 in stage 1.0 (TID 88, c411.hadoop.gda.lo, partition 88, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 88 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 86.0 in stage 1.0 (TID 86) in 16 ms on c411.hadoop.gda.lo (85/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 89.0 in stage 1.0 (TID 89, c433.hadoop.gda.lo, partition 89, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 89 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 84.0 in stage 1.0 (TID 84) in 24 ms on c433.hadoop.gda.lo (86/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 90.0 in stage 1.0 (TID 90, c411.hadoop.gda.lo, partition 90, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 90 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 88.0 in stage 1.0 (TID 88) in 13 ms on c411.hadoop.gda.lo (87/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 91.0 in stage 1.0 (TID 91, c414.hadoop.gda.lo, partition 91, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 91 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 87.0 in stage 1.0 (TID 87) in 17 ms on c414.hadoop.gda.lo (88/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 92.0 in stage 1.0 (TID 92, c433.hadoop.gda.lo, partition 92, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 92 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 89.0 in stage 1.0 (TID 89) in 22 ms on c433.hadoop.gda.lo (89/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 93.0 in stage 1.0 (TID 93, c414.hadoop.gda.lo, partition 93, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 93 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 91.0 in stage 1.0 (TID 91) in 15 ms on c414.hadoop.gda.lo (90/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 94.0 in stage 1.0 (TID 94, c411.hadoop.gda.lo, partition 94, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 94 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 90.0 in stage 1.0 (TID 90) in 21 ms on c411.hadoop.gda.lo (91/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 95.0 in stage 1.0 (TID 95, c414.hadoop.gda.lo, partition 95, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 95 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 93.0 in stage 1.0 (TID 93) in 14 ms on c414.hadoop.gda.lo (92/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 96.0 in stage 1.0 (TID 96, c433.hadoop.gda.lo, partition 96, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 96 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 92.0 in stage 1.0 (TID 92) in 21 ms on c433.hadoop.gda.lo (93/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 97.0 in stage 1.0 (TID 97, c411.hadoop.gda.lo, partition 97, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 97 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 94.0 in stage 1.0 (TID 94) in 15 ms on c411.hadoop.gda.lo (94/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 98.0 in stage 1.0 (TID 98, c414.hadoop.gda.lo, partition 98, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 98 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 95.0 in stage 1.0 (TID 95) in 14 ms on c414.hadoop.gda.lo (95/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 99.0 in stage 1.0 (TID 99, c432.hadoop.gda.lo, partition 99, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 99 on executor id: 1 hostname: c432.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 1.0 (TID 3) in 1939 ms on c432.hadoop.gda.lo (96/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 100.0 in stage 1.0 (TID 100, c411.hadoop.gda.lo, partition 100, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 100 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:53 INFO yarn.YarnAllocator: Driver requested a total number of 7 executor(s).
17/05/16 11:10:53 INFO yarn.YarnAllocator: Will request 2 executor containers, each with 1 cores and 3456 MB memory including 384 MB overhead
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 97.0 in stage 1.0 (TID 97) in 17 ms on c411.hadoop.gda.lo (97/200)
17/05/16 11:10:53 INFO spark.ExecutorAllocationManager: Requesting 2 new executors because tasks are backlogged (new desired total will be 7)
17/05/16 11:10:53 INFO yarn.YarnAllocator: Canceled 0 container requests (locality no longer needed)
17/05/16 11:10:53 INFO yarn.YarnAllocator: Submitted container request (host: Any, capability: <memory:3456, vCores:1>)
17/05/16 11:10:53 INFO yarn.YarnAllocator: Submitted container request (host: Any, capability: <memory:3456, vCores:1>)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 101.0 in stage 1.0 (TID 101, c414.hadoop.gda.lo, partition 101, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 101 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 98.0 in stage 1.0 (TID 98) in 18 ms on c414.hadoop.gda.lo (98/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 102.0 in stage 1.0 (TID 102, c433.hadoop.gda.lo, partition 102, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 102 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 96.0 in stage 1.0 (TID 96) in 28 ms on c433.hadoop.gda.lo (99/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 103.0 in stage 1.0 (TID 103, c411.hadoop.gda.lo, partition 103, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 103 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 100.0 in stage 1.0 (TID 100) in 15 ms on c411.hadoop.gda.lo (100/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 104.0 in stage 1.0 (TID 104, c414.hadoop.gda.lo, partition 104, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 104 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 101.0 in stage 1.0 (TID 101) in 15 ms on c414.hadoop.gda.lo (101/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 105.0 in stage 1.0 (TID 105, c411.hadoop.gda.lo, partition 105, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 105 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 103.0 in stage 1.0 (TID 103) in 17 ms on c411.hadoop.gda.lo (102/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 106.0 in stage 1.0 (TID 106, c414.hadoop.gda.lo, partition 106, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 106 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 104.0 in stage 1.0 (TID 104) in 15 ms on c414.hadoop.gda.lo (103/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 107.0 in stage 1.0 (TID 107, c433.hadoop.gda.lo, partition 107, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 107 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 102.0 in stage 1.0 (TID 102) in 27 ms on c433.hadoop.gda.lo (104/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 108.0 in stage 1.0 (TID 108, c411.hadoop.gda.lo, partition 108, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 108 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 105.0 in stage 1.0 (TID 105) in 22 ms on c411.hadoop.gda.lo (105/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 109.0 in stage 1.0 (TID 109, c414.hadoop.gda.lo, partition 109, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 109 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 106.0 in stage 1.0 (TID 106) in 22 ms on c414.hadoop.gda.lo (106/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 110.0 in stage 1.0 (TID 110, c432.hadoop.gda.lo, partition 110, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 110 on executor id: 1 hostname: c432.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 99.0 in stage 1.0 (TID 99) in 65 ms on c432.hadoop.gda.lo (107/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 111.0 in stage 1.0 (TID 111, c433.hadoop.gda.lo, partition 111, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 111 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 107.0 in stage 1.0 (TID 107) in 25 ms on c433.hadoop.gda.lo (108/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 112.0 in stage 1.0 (TID 112, c414.hadoop.gda.lo, partition 112, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 112 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 109.0 in stage 1.0 (TID 109) in 14 ms on c414.hadoop.gda.lo (109/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 113.0 in stage 1.0 (TID 113, c411.hadoop.gda.lo, partition 113, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 113 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 108.0 in stage 1.0 (TID 108) in 23 ms on c411.hadoop.gda.lo (110/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 114.0 in stage 1.0 (TID 114, c414.hadoop.gda.lo, partition 114, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 114 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 112.0 in stage 1.0 (TID 112) in 13 ms on c414.hadoop.gda.lo (111/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 115.0 in stage 1.0 (TID 115, c433.hadoop.gda.lo, partition 115, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 115 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 111.0 in stage 1.0 (TID 111) in 25 ms on c433.hadoop.gda.lo (112/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 116.0 in stage 1.0 (TID 116, c414.hadoop.gda.lo, partition 116, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 116 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 114.0 in stage 1.0 (TID 114) in 12 ms on c414.hadoop.gda.lo (113/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 117.0 in stage 1.0 (TID 117, c411.hadoop.gda.lo, partition 117, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 117 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 113.0 in stage 1.0 (TID 113) in 21 ms on c411.hadoop.gda.lo (114/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 118.0 in stage 1.0 (TID 118, c432.hadoop.gda.lo, partition 118, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 118 on executor id: 1 hostname: c432.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 110.0 in stage 1.0 (TID 110) in 41 ms on c432.hadoop.gda.lo (115/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 119.0 in stage 1.0 (TID 119, c414.hadoop.gda.lo, partition 119, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 119 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 116.0 in stage 1.0 (TID 116) in 13 ms on c414.hadoop.gda.lo (116/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 120.0 in stage 1.0 (TID 120, c411.hadoop.gda.lo, partition 120, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 120 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 117.0 in stage 1.0 (TID 117) in 18 ms on c411.hadoop.gda.lo (117/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 121.0 in stage 1.0 (TID 121, c433.hadoop.gda.lo, partition 121, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 121 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 115.0 in stage 1.0 (TID 115) in 29 ms on c433.hadoop.gda.lo (118/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 122.0 in stage 1.0 (TID 122, c414.hadoop.gda.lo, partition 122, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 122 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 119.0 in stage 1.0 (TID 119) in 14 ms on c414.hadoop.gda.lo (119/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 123.0 in stage 1.0 (TID 123, c414.hadoop.gda.lo, partition 123, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 123 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 122.0 in stage 1.0 (TID 122) in 14 ms on c414.hadoop.gda.lo (120/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 124.0 in stage 1.0 (TID 124, c411.hadoop.gda.lo, partition 124, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 124 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 120.0 in stage 1.0 (TID 120) in 24 ms on c411.hadoop.gda.lo (121/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 125.0 in stage 1.0 (TID 125, c432.hadoop.gda.lo, partition 125, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 125 on executor id: 1 hostname: c432.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 118.0 in stage 1.0 (TID 118) in 43 ms on c432.hadoop.gda.lo (122/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 126.0 in stage 1.0 (TID 126, c433.hadoop.gda.lo, partition 126, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 126 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 121.0 in stage 1.0 (TID 121) in 31 ms on c433.hadoop.gda.lo (123/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 127.0 in stage 1.0 (TID 127, c414.hadoop.gda.lo, partition 127, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 127 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 123.0 in stage 1.0 (TID 123) in 16 ms on c414.hadoop.gda.lo (124/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 128.0 in stage 1.0 (TID 128, c414.hadoop.gda.lo, partition 128, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 128 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 127.0 in stage 1.0 (TID 127) in 18 ms on c414.hadoop.gda.lo (125/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 129.0 in stage 1.0 (TID 129, c411.hadoop.gda.lo, partition 129, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 129 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 124.0 in stage 1.0 (TID 124) in 32 ms on c411.hadoop.gda.lo (126/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 130.0 in stage 1.0 (TID 130, c433.hadoop.gda.lo, partition 130, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 130 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 126.0 in stage 1.0 (TID 126) in 30 ms on c433.hadoop.gda.lo (127/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 131.0 in stage 1.0 (TID 131, c414.hadoop.gda.lo, partition 131, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 131 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 128.0 in stage 1.0 (TID 128) in 14 ms on c414.hadoop.gda.lo (128/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 132.0 in stage 1.0 (TID 132, c432.hadoop.gda.lo, partition 132, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 132 on executor id: 1 hostname: c432.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 125.0 in stage 1.0 (TID 125) in 43 ms on c432.hadoop.gda.lo (129/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 133.0 in stage 1.0 (TID 133, c414.hadoop.gda.lo, partition 133, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 133 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 131.0 in stage 1.0 (TID 131) in 10 ms on c414.hadoop.gda.lo (130/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 134.0 in stage 1.0 (TID 134, c433.hadoop.gda.lo, partition 134, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 134 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 130.0 in stage 1.0 (TID 130) in 23 ms on c433.hadoop.gda.lo (131/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 135.0 in stage 1.0 (TID 135, c414.hadoop.gda.lo, partition 135, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 135 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 133.0 in stage 1.0 (TID 133) in 14 ms on c414.hadoop.gda.lo (132/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 136.0 in stage 1.0 (TID 136, c411.hadoop.gda.lo, partition 136, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 136 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 129.0 in stage 1.0 (TID 129) in 37 ms on c411.hadoop.gda.lo (133/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 137.0 in stage 1.0 (TID 137, c414.hadoop.gda.lo, partition 137, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 137 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 135.0 in stage 1.0 (TID 135) in 11 ms on c414.hadoop.gda.lo (134/200)
17/05/16 11:10:53 INFO impl.AMRMClientImpl: Received new token for : c402.hadoop.gda.lo:45454
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 138.0 in stage 1.0 (TID 138, c433.hadoop.gda.lo, partition 138, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 138 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 134.0 in stage 1.0 (TID 134) in 24 ms on c433.hadoop.gda.lo (135/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 139.0 in stage 1.0 (TID 139, c432.hadoop.gda.lo, partition 139, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 139 on executor id: 1 hostname: c432.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 132.0 in stage 1.0 (TID 132) in 37 ms on c432.hadoop.gda.lo (136/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 140.0 in stage 1.0 (TID 140, c414.hadoop.gda.lo, partition 140, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 140 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 137.0 in stage 1.0 (TID 137) in 13 ms on c414.hadoop.gda.lo (137/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 141.0 in stage 1.0 (TID 141, c411.hadoop.gda.lo, partition 141, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 141 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 136.0 in stage 1.0 (TID 136) in 30 ms on c411.hadoop.gda.lo (138/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 142.0 in stage 1.0 (TID 142, c414.hadoop.gda.lo, partition 142, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 142 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 140.0 in stage 1.0 (TID 140) in 11 ms on c414.hadoop.gda.lo (139/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 143.0 in stage 1.0 (TID 143, c433.hadoop.gda.lo, partition 143, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 143 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 138.0 in stage 1.0 (TID 138) in 20 ms on c433.hadoop.gda.lo (140/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 144.0 in stage 1.0 (TID 144, c411.hadoop.gda.lo, partition 144, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 144 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 141.0 in stage 1.0 (TID 141) in 17 ms on c411.hadoop.gda.lo (141/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 145.0 in stage 1.0 (TID 145, c432.hadoop.gda.lo, partition 145, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 145 on executor id: 1 hostname: c432.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 139.0 in stage 1.0 (TID 139) in 29 ms on c432.hadoop.gda.lo (142/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 146.0 in stage 1.0 (TID 146, c414.hadoop.gda.lo, partition 146, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 146 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 142.0 in stage 1.0 (TID 142) in 22 ms on c414.hadoop.gda.lo (143/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 147.0 in stage 1.0 (TID 147, c433.hadoop.gda.lo, partition 147, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 147 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:53 INFO yarn.YarnAllocator: Launching container container_e81_1494395298335_303847_01_000007 for on host c402.hadoop.gda.lo
17/05/16 11:10:53 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@10.60.43.33:19210,  executorHostname: c402.hadoop.gda.lo
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 143.0 in stage 1.0 (TID 143) in 21 ms on c433.hadoop.gda.lo (144/200)
17/05/16 11:10:53 INFO yarn.YarnAllocator: Launching container container_e81_1494395298335_303847_01_000008 for on host c427.hadoop.gda.lo
17/05/16 11:10:53 INFO yarn.YarnAllocator: Launching ExecutorRunnable. driverUrl: spark://CoarseGrainedScheduler@10.60.43.33:19210,  executorHostname: c427.hadoop.gda.lo
17/05/16 11:10:53 INFO yarn.YarnAllocator: Received 2 containers from YARN, launching executors on 2 of them.
17/05/16 11:10:53 INFO yarn.ExecutorRunnable: Starting Executor Container
17/05/16 11:10:53 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
17/05/16 11:10:53 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
17/05/16 11:10:53 INFO yarn.ExecutorRunnable: Starting Executor Container
17/05/16 11:10:53 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 148.0 in stage 1.0 (TID 148, c411.hadoop.gda.lo, partition 148, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO yarn.ExecutorRunnable: Setting up ContainerLaunchContext
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 148 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 144.0 in stage 1.0 (TID 144) in 20 ms on c411.hadoop.gda.lo (145/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 149.0 in stage 1.0 (TID 149, c414.hadoop.gda.lo, partition 149, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 149 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 146.0 in stage 1.0 (TID 146) in 15 ms on c414.hadoop.gda.lo (146/200)
17/05/16 11:10:53 INFO yarn.ExecutorRunnable: 
===============================================================================
YARN executor launch context:
  env:
    CLASSPATH -> {{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark_libs__/*<CPS>$HADOOP_CONF_DIR<CPS>/usr/hdp/current/hadoop-client/*<CPS>/usr/hdp/current/hadoop-client/lib/*<CPS>/usr/hdp/current/hadoop-hdfs-client/*<CPS>/usr/hdp/current/hadoop-hdfs-client/lib/*<CPS>/usr/hdp/current/hadoop-yarn-client/*<CPS>/usr/hdp/current/hadoop-yarn-client/lib/*<CPS>$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.4.2.0-258/hadoop/lib/hadoop-lzo-0.6.0.2.4.2.0-258.jar:/etc/hadoop/conf/secure
    SPARK_LOG_URL_STDERR -> http://c427.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_01_000008/fairy/stderr?start=-4096
    SPARK_YARN_STAGING_DIR -> hdfs://c408.hadoop.gda.lo:8020/user/fairy/.sparkStaging/application_1494395298335_303847
    SPARK_USER -> fairy
    SPARK_YARN_MODE -> true
    SPARK_LOG_URL_STDOUT -> http://c427.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_01_000008/fairy/stdout?start=-4096

  command:
    {{JAVA_HOME}}/bin/java -server -Xmx3072m '-Dhdp.version=2.4.2.0-258' -Djava.io.tmpdir={{PWD}}/tmp '-Dspark.history.ui.port=18080' '-Dspark.ui.port=0' '-Dspark.driver.port=19210' -Dspark.yarn.app.container.log.dir=<LOG_DIR> -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.60.43.33:19210 --executor-id 7 --hostname c427.hadoop.gda.lo --cores 1 --app-id application_1494395298335_303847 --user-class-path file:$PWD/__app__.jar 1> <LOG_DIR>/stdout 2> <LOG_DIR>/stderr
===============================================================================
      
17/05/16 11:10:53 INFO yarn.ExecutorRunnable: 
===============================================================================
YARN executor launch context:
  env:
    CLASSPATH -> {{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark_libs__/*<CPS>$HADOOP_CONF_DIR<CPS>/usr/hdp/current/hadoop-client/*<CPS>/usr/hdp/current/hadoop-client/lib/*<CPS>/usr/hdp/current/hadoop-hdfs-client/*<CPS>/usr/hdp/current/hadoop-hdfs-client/lib/*<CPS>/usr/hdp/current/hadoop-yarn-client/*<CPS>/usr/hdp/current/hadoop-yarn-client/lib/*<CPS>$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.4.2.0-258/hadoop/lib/hadoop-lzo-0.6.0.2.4.2.0-258.jar:/etc/hadoop/conf/secure
    SPARK_LOG_URL_STDERR -> http://c402.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_01_000007/fairy/stderr?start=-4096
    SPARK_YARN_STAGING_DIR -> hdfs://c408.hadoop.gda.lo:8020/user/fairy/.sparkStaging/application_1494395298335_303847
    SPARK_USER -> fairy
    SPARK_YARN_MODE -> true
    SPARK_LOG_URL_STDOUT -> http://c402.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_01_000007/fairy/stdout?start=-4096

  command:
    {{JAVA_HOME}}/bin/java -server -Xmx3072m '-Dhdp.version=2.4.2.0-258' -Djava.io.tmpdir={{PWD}}/tmp '-Dspark.history.ui.port=18080' '-Dspark.ui.port=0' '-Dspark.driver.port=19210' -Dspark.yarn.app.container.log.dir=<LOG_DIR> -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.60.43.33:19210 --executor-id 6 --hostname c402.hadoop.gda.lo --cores 1 --app-id application_1494395298335_303847 --user-class-path file:$PWD/__app__.jar 1> <LOG_DIR>/stdout 2> <LOG_DIR>/stderr
===============================================================================
      
17/05/16 11:10:53 INFO impl.ContainerManagementProtocolProxy: Opening proxy : c427.hadoop.gda.lo:45454
17/05/16 11:10:53 INFO impl.ContainerManagementProtocolProxy: Opening proxy : c402.hadoop.gda.lo:45454
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 150.0 in stage 1.0 (TID 150, c432.hadoop.gda.lo, partition 150, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 150 on executor id: 1 hostname: c432.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 145.0 in stage 1.0 (TID 145) in 27 ms on c432.hadoop.gda.lo (147/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 151.0 in stage 1.0 (TID 151, c433.hadoop.gda.lo, partition 151, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 151 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 147.0 in stage 1.0 (TID 147) in 22 ms on c433.hadoop.gda.lo (148/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 152.0 in stage 1.0 (TID 152, c414.hadoop.gda.lo, partition 152, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 152 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 149.0 in stage 1.0 (TID 149) in 13 ms on c414.hadoop.gda.lo (149/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 153.0 in stage 1.0 (TID 153, c411.hadoop.gda.lo, partition 153, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 153 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 148.0 in stage 1.0 (TID 148) in 17 ms on c411.hadoop.gda.lo (150/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 154.0 in stage 1.0 (TID 154, c414.hadoop.gda.lo, partition 154, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 154 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 152.0 in stage 1.0 (TID 152) in 14 ms on c414.hadoop.gda.lo (151/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 155.0 in stage 1.0 (TID 155, c433.hadoop.gda.lo, partition 155, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 155 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 151.0 in stage 1.0 (TID 151) in 20 ms on c433.hadoop.gda.lo (152/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 156.0 in stage 1.0 (TID 156, c432.hadoop.gda.lo, partition 156, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 156 on executor id: 1 hostname: c432.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 150.0 in stage 1.0 (TID 150) in 24 ms on c432.hadoop.gda.lo (153/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 157.0 in stage 1.0 (TID 157, c411.hadoop.gda.lo, partition 157, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 157 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 153.0 in stage 1.0 (TID 153) in 19 ms on c411.hadoop.gda.lo (154/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 158.0 in stage 1.0 (TID 158, c414.hadoop.gda.lo, partition 158, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 158 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 154.0 in stage 1.0 (TID 154) in 14 ms on c414.hadoop.gda.lo (155/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 159.0 in stage 1.0 (TID 159, c433.hadoop.gda.lo, partition 159, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 159 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 155.0 in stage 1.0 (TID 155) in 21 ms on c433.hadoop.gda.lo (156/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 160.0 in stage 1.0 (TID 160, c414.hadoop.gda.lo, partition 160, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 160 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 158.0 in stage 1.0 (TID 158) in 15 ms on c414.hadoop.gda.lo (157/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 161.0 in stage 1.0 (TID 161, c432.hadoop.gda.lo, partition 161, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 161 on executor id: 1 hostname: c432.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 156.0 in stage 1.0 (TID 156) in 27 ms on c432.hadoop.gda.lo (158/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 162.0 in stage 1.0 (TID 162, c411.hadoop.gda.lo, partition 162, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 162 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 157.0 in stage 1.0 (TID 157) in 35 ms on c411.hadoop.gda.lo (159/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 163.0 in stage 1.0 (TID 163, c433.hadoop.gda.lo, partition 163, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 163 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 159.0 in stage 1.0 (TID 159) in 22 ms on c433.hadoop.gda.lo (160/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 164.0 in stage 1.0 (TID 164, c414.hadoop.gda.lo, partition 164, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 164 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 160.0 in stage 1.0 (TID 160) in 18 ms on c414.hadoop.gda.lo (161/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 165.0 in stage 1.0 (TID 165, c432.hadoop.gda.lo, partition 165, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 165 on executor id: 1 hostname: c432.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 161.0 in stage 1.0 (TID 161) in 28 ms on c432.hadoop.gda.lo (162/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 166.0 in stage 1.0 (TID 166, c414.hadoop.gda.lo, partition 166, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 166 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 164.0 in stage 1.0 (TID 164) in 17 ms on c414.hadoop.gda.lo (163/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 167.0 in stage 1.0 (TID 167, c411.hadoop.gda.lo, partition 167, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 167 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 162.0 in stage 1.0 (TID 162) in 22 ms on c411.hadoop.gda.lo (164/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 168.0 in stage 1.0 (TID 168, c433.hadoop.gda.lo, partition 168, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 168 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 163.0 in stage 1.0 (TID 163) in 21 ms on c433.hadoop.gda.lo (165/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 169.0 in stage 1.0 (TID 169, c414.hadoop.gda.lo, partition 169, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 169 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 166.0 in stage 1.0 (TID 166) in 21 ms on c414.hadoop.gda.lo (166/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 170.0 in stage 1.0 (TID 170, c411.hadoop.gda.lo, partition 170, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 170 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 167.0 in stage 1.0 (TID 167) in 20 ms on c411.hadoop.gda.lo (167/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 171.0 in stage 1.0 (TID 171, c433.hadoop.gda.lo, partition 171, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 171 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 168.0 in stage 1.0 (TID 168) in 20 ms on c433.hadoop.gda.lo (168/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 172.0 in stage 1.0 (TID 172, c432.hadoop.gda.lo, partition 172, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 172 on executor id: 1 hostname: c432.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 165.0 in stage 1.0 (TID 165) in 31 ms on c432.hadoop.gda.lo (169/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 173.0 in stage 1.0 (TID 173, c414.hadoop.gda.lo, partition 173, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 173 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 169.0 in stage 1.0 (TID 169) in 13 ms on c414.hadoop.gda.lo (170/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 174.0 in stage 1.0 (TID 174, c414.hadoop.gda.lo, partition 174, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 174 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 173.0 in stage 1.0 (TID 173) in 11 ms on c414.hadoop.gda.lo (171/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 175.0 in stage 1.0 (TID 175, c411.hadoop.gda.lo, partition 175, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 175 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 170.0 in stage 1.0 (TID 170) in 22 ms on c411.hadoop.gda.lo (172/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 176.0 in stage 1.0 (TID 176, c433.hadoop.gda.lo, partition 176, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 176 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 171.0 in stage 1.0 (TID 171) in 22 ms on c433.hadoop.gda.lo (173/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 177.0 in stage 1.0 (TID 177, c414.hadoop.gda.lo, partition 177, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 177 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 174.0 in stage 1.0 (TID 174) in 14 ms on c414.hadoop.gda.lo (174/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 178.0 in stage 1.0 (TID 178, c433.hadoop.gda.lo, partition 178, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 178 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 176.0 in stage 1.0 (TID 176) in 19 ms on c433.hadoop.gda.lo (175/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 179.0 in stage 1.0 (TID 179, c411.hadoop.gda.lo, partition 179, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 179 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 175.0 in stage 1.0 (TID 175) in 22 ms on c411.hadoop.gda.lo (176/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 180.0 in stage 1.0 (TID 180, c414.hadoop.gda.lo, partition 180, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 180 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 177.0 in stage 1.0 (TID 177) in 15 ms on c414.hadoop.gda.lo (177/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 181.0 in stage 1.0 (TID 181, c414.hadoop.gda.lo, partition 181, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 181 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 180.0 in stage 1.0 (TID 180) in 14 ms on c414.hadoop.gda.lo (178/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 182.0 in stage 1.0 (TID 182, c433.hadoop.gda.lo, partition 182, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 182 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 178.0 in stage 1.0 (TID 178) in 22 ms on c433.hadoop.gda.lo (179/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 183.0 in stage 1.0 (TID 183, c411.hadoop.gda.lo, partition 183, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 183 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 179.0 in stage 1.0 (TID 179) in 31 ms on c411.hadoop.gda.lo (180/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 184.0 in stage 1.0 (TID 184, c414.hadoop.gda.lo, partition 184, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 184 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 181.0 in stage 1.0 (TID 181) in 15 ms on c414.hadoop.gda.lo (181/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 185.0 in stage 1.0 (TID 185, c414.hadoop.gda.lo, partition 185, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 185 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 184.0 in stage 1.0 (TID 184) in 16 ms on c414.hadoop.gda.lo (182/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 186.0 in stage 1.0 (TID 186, c411.hadoop.gda.lo, partition 186, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 186 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 183.0 in stage 1.0 (TID 183) in 20 ms on c411.hadoop.gda.lo (183/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 187.0 in stage 1.0 (TID 187, c414.hadoop.gda.lo, partition 187, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 187 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 185.0 in stage 1.0 (TID 185) in 17 ms on c414.hadoop.gda.lo (184/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 188.0 in stage 1.0 (TID 188, c411.hadoop.gda.lo, partition 188, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 188 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 186.0 in stage 1.0 (TID 186) in 17 ms on c411.hadoop.gda.lo (185/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 189.0 in stage 1.0 (TID 189, c433.hadoop.gda.lo, partition 189, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 189 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 182.0 in stage 1.0 (TID 182) in 49 ms on c433.hadoop.gda.lo (186/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 190.0 in stage 1.0 (TID 190, c414.hadoop.gda.lo, partition 190, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 190 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 187.0 in stage 1.0 (TID 187) in 13 ms on c414.hadoop.gda.lo (187/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 191.0 in stage 1.0 (TID 191, c411.hadoop.gda.lo, partition 191, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 191 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 188.0 in stage 1.0 (TID 188) in 20 ms on c411.hadoop.gda.lo (188/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 192.0 in stage 1.0 (TID 192, c433.hadoop.gda.lo, partition 192, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 192 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 189.0 in stage 1.0 (TID 189) in 18 ms on c433.hadoop.gda.lo (189/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 193.0 in stage 1.0 (TID 193, c414.hadoop.gda.lo, partition 193, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 193 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 190.0 in stage 1.0 (TID 190) in 13 ms on c414.hadoop.gda.lo (190/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 194.0 in stage 1.0 (TID 194, c414.hadoop.gda.lo, partition 194, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 194 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 193.0 in stage 1.0 (TID 193) in 14 ms on c414.hadoop.gda.lo (191/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 195.0 in stage 1.0 (TID 195, c432.hadoop.gda.lo, partition 195, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 195 on executor id: 1 hostname: c432.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 172.0 in stage 1.0 (TID 172) in 139 ms on c432.hadoop.gda.lo (192/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 196.0 in stage 1.0 (TID 196, c411.hadoop.gda.lo, partition 196, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 196 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 191.0 in stage 1.0 (TID 191) in 19 ms on c411.hadoop.gda.lo (193/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 197.0 in stage 1.0 (TID 197, c433.hadoop.gda.lo, partition 197, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 197 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 192.0 in stage 1.0 (TID 192) in 22 ms on c433.hadoop.gda.lo (194/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 198.0 in stage 1.0 (TID 198, c414.hadoop.gda.lo, partition 198, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 198 on executor id: 4 hostname: c414.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 194.0 in stage 1.0 (TID 194) in 14 ms on c414.hadoop.gda.lo (195/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 199.0 in stage 1.0 (TID 199, c411.hadoop.gda.lo, partition 199, PROCESS_LOCAL, 5221 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 199 on executor id: 3 hostname: c411.hadoop.gda.lo.
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 196.0 in stage 1.0 (TID 196) in 17 ms on c411.hadoop.gda.lo (196/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 195.0 in stage 1.0 (TID 195) in 27 ms on c432.hadoop.gda.lo (197/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 197.0 in stage 1.0 (TID 197) in 21 ms on c433.hadoop.gda.lo (198/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 198.0 in stage 1.0 (TID 198) in 16 ms on c414.hadoop.gda.lo (199/200)
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 199.0 in stage 1.0 (TID 199) in 22 ms on c411.hadoop.gda.lo (200/200)
17/05/16 11:10:53 INFO scheduler.DAGScheduler: ShuffleMapStage 1 (first at CcuReport.scala:39) finished in 2.542 s
17/05/16 11:10:53 INFO cluster.YarnClusterScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
17/05/16 11:10:53 INFO scheduler.DAGScheduler: looking for newly runnable stages
17/05/16 11:10:53 INFO scheduler.DAGScheduler: running: Set()
17/05/16 11:10:53 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 2)
17/05/16 11:10:53 INFO scheduler.DAGScheduler: failed: Set()
17/05/16 11:10:53 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[20] at first at CcuReport.scala:39), which has no missing parents
17/05/16 11:10:53 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.8 KB, free 1458.5 MB)
17/05/16 11:10:53 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.4 KB, free 1458.5 MB)
17/05/16 11:10:53 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.60.43.33:5141 (size: 4.4 KB, free: 1458.6 MB)
17/05/16 11:10:53 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1012
17/05/16 11:10:53 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[20] at first at CcuReport.scala:39)
17/05/16 11:10:53 INFO cluster.YarnClusterScheduler: Adding task set 2.0 with 1 tasks
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 200, c433.hadoop.gda.lo, partition 0, NODE_LOCAL, 5232 bytes)
17/05/16 11:10:53 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Launching task 200 on executor id: 2 hostname: c433.hadoop.gda.lo.
17/05/16 11:10:53 INFO yarn.YarnAllocator: Driver requested a total number of 1 executor(s).
17/05/16 11:10:53 INFO yarn.YarnAllocator: Canceling requests for 0 executor container(s) to have a new desired total 1 executors.
17/05/16 11:10:53 WARN yarn.YarnAllocator: Expected to find pending requests, but found none.
17/05/16 11:10:53 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on c433.hadoop.gda.lo:28145 (size: 4.4 KB, free: 1458.6 MB)
17/05/16 11:10:53 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.60.43.33:8328
17/05/16 11:10:53 INFO spark.MapOutputTrackerMaster: Size of output statuses for shuffle 1 is 340 bytes
17/05/16 11:10:53 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 200) in 289 ms on c433.hadoop.gda.lo (1/1)
17/05/16 11:10:53 INFO cluster.YarnClusterScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
17/05/16 11:10:53 INFO scheduler.DAGScheduler: ResultStage 2 (first at CcuReport.scala:39) finished in 0.291 s
17/05/16 11:10:53 INFO scheduler.DAGScheduler: Job 0 finished: first at CcuReport.scala:39, took 3.057101 s
17/05/16 11:10:54 INFO yarn.YarnAllocator: Driver requested a total number of 0 executor(s).
17/05/16 11:10:54 INFO yarn.YarnAllocator: Canceling requests for 0 executor container(s) to have a new desired total 0 executors.
17/05/16 11:10:54 WARN yarn.YarnAllocator: Expected to find pending requests, but found none.
17/05/16 11:10:54 INFO codegen.CodeGenerator: Code generated in 14.757095 ms
17/05/16 11:10:54 ERROR yarn.ApplicationMaster: User class threw exception: java.lang.NullPointerException: Value at index 0 is null
java.lang.NullPointerException: Value at index 0 is null
	at org.apache.spark.sql.Row$class.getAnyValAs(Row.scala:466)
	at org.apache.spark.sql.Row$class.getDouble(Row.scala:242)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getDouble(rows.scala:192)
	at vng.ge.stats.report.report.game.CcuReport.write(CcuReport.scala:39)
	at vng.ge.stats.report.base.TReport.run(TReport.scala:38)
	at vng.ge.stats.report.loader.GameDataLoader.run(GameDataLoader.scala:57)
	at vng.ge.stats.report.base.DataPool.report(DataPool.scala:44)
	at vng.ge.stats.report.job.Runner$$anonfun$run$2$$anonfun$apply$1.apply$mcV$sp(Runner.scala:76)
	at scala.util.control.Breaks.breakable(Breaks.scala:38)
	at vng.ge.stats.report.job.Runner$$anonfun$run$2.apply(Runner.scala:56)
	at vng.ge.stats.report.job.Runner$$anonfun$run$2.apply(Runner.scala:44)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at vng.ge.stats.report.job.Runner$.run(Runner.scala:44)
	at vng.ge.stats.report.job.Runner$.main(Runner.scala:19)
	at vng.ge.stats.report.job.Runner.main(Runner.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:627)
17/05/16 11:10:54 INFO yarn.ApplicationMaster: Final app status: FAILED, exitCode: 15, (reason: User class threw exception: java.lang.NullPointerException: Value at index 0 is null)
17/05/16 11:10:54 INFO spark.SparkContext: Invoking stop() from shutdown hook
17/05/16 11:10:54 INFO server.ServerConnector: Stopped ServerConnector@74e7c4f5{HTTP/1.1}{0.0.0.0:0}
17/05/16 11:10:54 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@49e8096b{/stages/stage/kill,null,UNAVAILABLE}
17/05/16 11:10:54 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@4e616b23{/api,null,UNAVAILABLE}
17/05/16 11:10:54 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@46b4b0d1{/,null,UNAVAILABLE}
17/05/16 11:10:54 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@6db3dc67{/static,null,UNAVAILABLE}
17/05/16 11:10:54 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@2c657d71{/executors/threadDump/json,null,UNAVAILABLE}
17/05/16 11:10:54 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@6eacce3a{/executors/threadDump,null,UNAVAILABLE}
17/05/16 11:10:54 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@1e86b55d{/executors/json,null,UNAVAILABLE}
17/05/16 11:10:54 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@36cefb29{/executors,null,UNAVAILABLE}
17/05/16 11:10:54 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@71e66dab{/environment/json,null,UNAVAILABLE}
17/05/16 11:10:54 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@40e8a95{/environment,null,UNAVAILABLE}
17/05/16 11:10:54 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@28601015{/storage/rdd/json,null,UNAVAILABLE}
17/05/16 11:10:54 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@4e113fc9{/storage/rdd,null,UNAVAILABLE}
17/05/16 11:10:54 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@2f0b52c5{/storage/json,null,UNAVAILABLE}
17/05/16 11:10:54 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@4eec45c1{/storage,null,UNAVAILABLE}
17/05/16 11:10:54 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@30dac9af{/stages/pool/json,null,UNAVAILABLE}
17/05/16 11:10:54 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@4664e3fe{/stages/pool,null,UNAVAILABLE}
17/05/16 11:10:54 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@650080d1{/stages/stage/json,null,UNAVAILABLE}
17/05/16 11:10:54 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@3119fa37{/stages/stage,null,UNAVAILABLE}
17/05/16 11:10:54 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@2c5b5277{/stages/json,null,UNAVAILABLE}
17/05/16 11:10:54 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@13c60037{/stages,null,UNAVAILABLE}
17/05/16 11:10:54 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@64461174{/jobs/job/json,null,UNAVAILABLE}
17/05/16 11:10:54 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@4c8a2aa9{/jobs/job,null,UNAVAILABLE}
17/05/16 11:10:54 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@1e3432da{/jobs/json,null,UNAVAILABLE}
17/05/16 11:10:54 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@44667231{/jobs,null,UNAVAILABLE}
17/05/16 11:10:54 INFO ui.SparkUI: Stopped Spark web UI at http://10.60.43.33:6491
17/05/16 11:10:54 INFO cluster.YarnClusterSchedulerBackend: Shutting down all executors
17/05/16 11:10:54 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
17/05/16 11:10:54 INFO cluster.SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
17/05/16 11:10:54 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/05/16 11:10:54 INFO memory.MemoryStore: MemoryStore cleared
17/05/16 11:10:54 INFO storage.BlockManager: BlockManager stopped
17/05/16 11:10:54 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
17/05/16 11:10:54 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/05/16 11:10:54 INFO spark.SparkContext: Successfully stopped SparkContext
17/05/16 11:10:54 INFO util.ShutdownHookManager: Shutdown hook called
17/05/16 11:10:54 INFO util.ShutdownHookManager: Deleting directory /hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/spark-66ad7b9d-f17a-4a5f-bcbc-38b6134666d8
End of LogType:stderr

LogType:stdout
Log Upload Time:Tue May 16 11:12:21 +0700 2017
LogLength:2995
Log Contents:
STATS INFO: 2017-05-16 11:10:38 \_ Parameters: Map(logDir -> /ge/warehouse, group_id -> game, source -> sdk, game_code -> cack, calc_id -> id, job_name -> cack, report_number -> 1-2-3-4-5-6-7-8-9, log_date -> 2017-05-06, run_timing -> a1,a3,a7,a14,a30,ac7,ac30)
STATS INFO: 2017-05-16 11:10:38 ---------------------------------------------------------------------------------------------
STATS INFO: 2017-05-16 11:10:38 \_ Timing: a1
STATS INFO: 2017-05-16 11:10:38 \_ ===============
STATS INFO: 2017-05-16 11:10:38 |	\_ Reading /ge/fairy/warehouse/cack/ub/sdk_data/ccu_2/2017-05-06
STATS WARN: 2017-05-16 11:10:38 |		\_ Loading file failed: Path does not exist: hdfs://c408.hadoop.gda.lo:8020/ge/fairy/warehouse/cack/ub/sdk_data/ccu_2/2017-05-06;
STATS WARN: 2017-05-16 11:10:38 |		\_ /ge/fairy/warehouse/cack/ub/sdk_data/ccu_2/2017-05-06 is not found => using default schema!
STATS INFO: 2017-05-16 11:10:49 |	\_ Reading /ge/fairy/warehouse/cack/ub/sdk_data/activity_2
STATS WARN: 2017-05-16 11:10:49 |		\_ Loading file failed: Path does not exist: hdfs://c408.hadoop.gda.lo:8020/ge/fairy/warehouse/cack/ub/sdk_data/activity_2/{2017-05-06};
STATS WARN: 2017-05-16 11:10:49 |		\_ /ge/fairy/warehouse/cack/ub/sdk_data/activity_2/{2017-05-06} is not found => using default schema!
STATS INFO: 2017-05-16 11:10:49 |	\_ Reading /ge/fairy/warehouse/cack/ub/sdk_data/accregister_2
STATS WARN: 2017-05-16 11:10:49 |		\_ Loading file failed: Path does not exist: hdfs://c408.hadoop.gda.lo:8020/ge/fairy/warehouse/cack/ub/sdk_data/accregister_2/{2017-05-06};
STATS WARN: 2017-05-16 11:10:49 |		\_ /ge/fairy/warehouse/cack/ub/sdk_data/accregister_2/{2017-05-06} is not found => using default schema!
STATS INFO: 2017-05-16 11:10:49 |	\_ Reading /ge/fairy/warehouse/cack/ub/sdk_data/payment_2
STATS WARN: 2017-05-16 11:10:49 |		\_ Loading file failed: Path does not exist: hdfs://c408.hadoop.gda.lo:8020/ge/fairy/warehouse/cack/ub/sdk_data/payment_2/{2017-05-06};
STATS WARN: 2017-05-16 11:10:49 |		\_ /ge/fairy/warehouse/cack/ub/sdk_data/payment_2/{2017-05-06} is not found => using default schema!
STATS INFO: 2017-05-16 11:10:49 |	\_ Reading /ge/fairy/warehouse/cack/ub/sdk_data/first_charge_2
STATS WARN: 2017-05-16 11:10:49 |		\_ Loading file failed: Path does not exist: hdfs://c408.hadoop.gda.lo:8020/ge/fairy/warehouse/cack/ub/sdk_data/first_charge_2/{2017-05-06};
STATS WARN: 2017-05-16 11:10:49 |		\_ /ge/fairy/warehouse/cack/ub/sdk_data/first_charge_2/{2017-05-06} is not found => using default schema!
STATS INFO: 2017-05-16 11:10:49 |	\_ Reading /ge/fairy/warehouse/cack/ub/sdk_data/total_login_acc_2/2017-05-06
STATS WARN: 2017-05-16 11:10:49 |		\_ Loading file failed: Path does not exist: hdfs://c408.hadoop.gda.lo:8020/ge/fairy/warehouse/cack/ub/sdk_data/total_login_acc_2/2017-05-06;
STATS WARN: 2017-05-16 11:10:49 |		\_ /ge/fairy/warehouse/cack/ub/sdk_data/total_login_acc_2/2017-05-06 is not found => using default schema!
STATS INFO: 2017-05-16 11:10:49 Skip game retention report!
End of LogType:stdout



Container: container_e81_1494395298335_303847_01_000003 on c433.hadoop.gda.lo_45454
=====================================================================================
LogType:directory.info
Log Upload Time:Tue May 16 11:12:21 +0700 2017
LogLength:28674
Log Contents:
ls -l:
total 112
lrwxrwxrwx 1 yarn hadoop    69 May 16 11:10 __app__.jar -> /hadoop/yarn/local/usercache/fairy/filecache/11521/stats-etlr-1.0.jar
-rw-r--r-- 1 yarn hadoop    87 May 16 11:10 container_tokens
-rwx------ 1 yarn hadoop   676 May 16 11:10 default_container_executor_session.sh
-rwx------ 1 yarn hadoop   730 May 16 11:10 default_container_executor.sh
-rwx------ 1 yarn hadoop 75408 May 16 11:10 launch_container.sh
lrwxrwxrwx 1 yarn hadoop    69 May 16 11:10 __spark_conf__ -> /hadoop/yarn/local/usercache/fairy/filecache/11522/__spark_conf__.zip
drwxr-xr-x 2 yarn hadoop 12288 May 16 11:10 __spark_libs__
drwx--x--- 2 yarn hadoop  4096 May 16 11:10 tmp
find -L . -maxdepth 5 -ls:
15861637    4 drwx--x---   4 yarn     hadoop       4096 May 16 11:10 .
15861118 1152 -r-x------   1 yarn     hadoop    1178454 May 16 11:10 ./__app__.jar
15861644    4 -rw-r--r--   1 yarn     hadoop         16 May 16 11:10 ./.default_container_executor_session.sh.crc
15861642    4 -rw-r--r--   1 yarn     hadoop        600 May 16 11:10 ./.launch_container.sh.crc
15861646    4 -rw-r--r--   1 yarn     hadoop         16 May 16 11:10 ./.default_container_executor.sh.crc
15861638    4 drwx--x---   2 yarn     hadoop       4096 May 16 11:10 ./tmp
15861643    4 -rwx------   1 yarn     hadoop        676 May 16 11:10 ./default_container_executor_session.sh
15861645    4 -rwx------   1 yarn     hadoop        730 May 16 11:10 ./default_container_executor.sh
15861640    4 -rw-r--r--   1 yarn     hadoop         12 May 16 11:10 ./.container_tokens.crc
15861639    4 -rw-r--r--   1 yarn     hadoop         87 May 16 11:10 ./container_tokens
15861641   76 -rwx------   1 yarn     hadoop      75408 May 16 11:10 ./launch_container.sh
15861647   12 drwxr-xr-x   2 yarn     hadoop      12288 May 16 11:10 ./__spark_libs__
5243081  136 -rwxr-xr-x   1 yarn     hadoop     135552 Mar 28 23:40 ./__spark_libs__/breeze-macros_2.11-0.11.2.jar
5243085 1848 -rwxr-xr-x   1 yarn     hadoop    1890075 Mar 28 23:40 ./__spark_libs__/datanucleus-core-3.2.10.jar
5243820 4468 -rwxr-xr-x   1 yarn     hadoop    4573750 Apr  4 16:10 ./__spark_libs__/scala-reflect-2.11.8.jar
6292483  136 -rwxr-xr-x   1 yarn     hadoop     138464 Apr  5 13:43 ./__spark_libs__/hive-beeline-1.2.1.spark2.jar
5243093  280 -rwxr-xr-x   1 yarn     hadoop     284184 Mar 28 23:40 ./__spark_libs__/commons-codec-1.10.jar
5243129    4 -rwxr-xr-x   1 yarn     hadoop       2497 Mar 28 23:40 ./__spark_libs__/javax.inject-1.jar
6292774  172 -rwxr-xr-x   1 yarn     hadoop     174351 Apr  5 13:43 ./__spark_libs__/stream-2.7.0.jar
6292609   12 -rwxr-xr-x   1 yarn     hadoop      10023 Apr  5 13:43 ./__spark_libs__/java-xmlbuilder-1.0.jar
6292272  748 -rwxr-xr-x   1 yarn     hadoop     764569 Apr  5 13:43 ./__spark_libs__/jtransforms-2.4.0.jar
5243017 2732 -rwxr-xr-x   1 yarn     hadoop    2796935 Mar 28 23:40 ./__spark_libs__/parquet-hadoop-bundle-1.6.0.jar
6292648   40 -rwxr-xr-x   1 yarn     hadoop      38134 Apr  5 13:43 ./__spark_libs__/hadoop-mapreduce-client-jobclient-2.7.1.jar
6292301  480 -rwxr-xr-x   1 yarn     hadoop     489884 Apr  5 13:43 ./__spark_libs__/log4j-1.2.17.jar
5242956  164 -rwxr-xr-x   1 yarn     hadoop     164422 Mar 28 23:40 ./__spark_libs__/core-1.1.2.jar
6292331   24 -rwxr-xr-x   1 yarn     hadoop      21575 Apr  5 13:43 ./__spark_libs__/parquet-common-1.7.0.jar
5243822  236 -rwxr-xr-x   1 yarn     hadoop     241367 Apr  4 16:10 ./__spark_libs__/commons-compress-1.4.1.jar
5243012 1988 -rwxr-xr-x   1 yarn     hadoop    2035066 Mar 28 23:40 ./__spark_libs__/commons-math3-3.4.1.jar
6292544   32 -rwxr-xr-x   1 yarn     hadoop      30595 Apr  5 13:43 ./__spark_libs__/commons-compiler-2.7.6.jar
5243135   80 -rwxr-xr-x   1 yarn     hadoop      79845 Mar 28 23:40 ./__spark_libs__/compress-lzf-1.0.3.jar
5243840  524 -rwxr-xr-x   1 yarn     hadoop     533455 Apr  4 16:10 ./__spark_libs__/protobuf-java-2.5.0.jar
6292196  176 -rwxr-xr-x   1 yarn     hadoop     177131 Apr  5 13:43 ./__spark_libs__/jetty-util-6.1.26.jar
5243027   20 -rwxr-xr-x   1 yarn     hadoop      16993 Mar 28 23:40 ./__spark_libs__/JavaEWAH-0.3.2.jar
6292184  180 -rwxr-xr-x   1 yarn     hadoop     180736 Apr  5 13:43 ./__spark_libs__/avro-mapred-1.7.7-hadoop2.jar
5242976 8068 -rwxr-xr-x   1 yarn     hadoop    8260573 Mar 28 23:40 ./__spark_libs__/hadoop-hdfs-2.7.1.jar
5243144 11232 -rwxr-xr-x   1 yarn     hadoop   11498852 Mar 28 23:40 ./__spark_libs__/hive-exec-1.2.1.spark2.jar
5243831 6716 -rwxr-xr-x   1 yarn     hadoop    6873892 Apr  4 16:10 ./__spark_libs__/spark-catalyst_2.11-2.0.1.jar
5242958 1444 -rwxr-xr-x   1 yarn     hadoop    1475955 Mar 28 23:40 ./__spark_libs__/htrace-core-3.1.0-incubating.jar
6292618  700 -rwxr-xr-x   1 yarn     hadoop     714194 Apr  5 13:43 ./__spark_libs__/javassist-3.18.1-GA.jar
6292739  200 -rwxr-xr-x   1 yarn     hadoop     201928 Apr  5 13:43 ./__spark_libs__/RoaringBitmap-0.5.11.jar
5243111   32 -rwxr-xr-x   1 yarn     hadoop      32145 Mar 28 23:40 ./__spark_libs__/hadoop-yarn-server-web-proxy-2.7.1.jar
5243057  176 -rwxr-xr-x   1 yarn     hadoop     178947 Mar 28 23:40 ./__spark_libs__/hk2-api-2.4.0-b34.jar
6292507  208 -rwxr-xr-x   1 yarn     hadoop     212453 Apr  5 13:43 ./__spark_libs__/commons-net-2.2.jar
5243000  176 -rwxr-xr-x   1 yarn     hadoop     177832 Mar 28 23:40 ./__spark_libs__/spark-mllib-local_2.11-2.0.1.jar
6292732   16 -rwxr-xr-x   1 yarn     hadoop      15071 Apr  5 13:43 ./__spark_libs__/jta-1.1.jar
5243147    8 -rwxr-xr-x   1 yarn     hadoop       4467 Mar 28 23:40 ./__spark_libs__/aopalliance-1.0.jar
5243066  228 -rwxr-xr-x   1 yarn     hadoop     232248 Mar 28 23:40 ./__spark_libs__/jackson-core-asl-1.9.13.jar
5242994  640 -rwxr-xr-x   1 yarn     hadoop     654216 Mar 28 23:40 ./__spark_libs__/pmml-model-1.2.15.jar
6292677 7108 -rwxr-xr-x   1 yarn     hadoop    7276083 Apr  5 13:43 ./__spark_libs__/spire_2.11-0.7.4.jar
5243852 2780 -rwxr-xr-x   1 yarn     hadoop    2842667 Apr  4 16:10 ./__spark_libs__/bcprov-jdk15on-1.51.jar
6292250  148 -rwxr-xr-x   1 yarn     hadoop     148627 Apr  5 13:43 ./__spark_libs__/stringtemplate-3.2.1.jar
6292379 11452 -rwxr-xr-x   1 yarn     hadoop   11723537 Apr  5 13:43 ./__spark_libs__/spark-core_2.11-2.0.1.jar
5243004   68 -rwxr-xr-x   1 yarn     hadoop      66270 Mar 28 23:40 ./__spark_libs__/jersey-container-servlet-core-2.22.2.jar
6292401   28 -rwxr-xr-x   1 yarn     hadoop      26366 Apr  5 13:43 ./__spark_libs__/javax.annotation-api-1.2.jar
6292290  116 -rwxr-xr-x   1 yarn     hadoop     115534 Apr  5 13:43 ./__spark_libs__/javax.ws.rs-api-2.0.1.jar
6292706 1144 -rwxr-xr-x   1 yarn     hadoop    1171380 Apr  5 13:43 ./__spark_libs__/jackson-databind-2.6.5.jar
5242952   96 -rwxr-xr-x   1 yarn     hadoop      95806 Mar 28 23:40 ./__spark_libs__/javax.servlet-api-3.1.0.jar
6292747  120 -rwxr-xr-x   1 yarn     hadoop     118973 Apr  5 13:43 ./__spark_libs__/hk2-utils-2.4.0-b34.jar
6292212   92 -rwxr-xr-x   1 yarn     hadoop      93210 Apr  5 13:43 ./__spark_libs__/super-csv-2.2.0.jar
6292653 1024 -rwxr-xr-x   1 yarn     hadoop    1048110 Apr  5 13:43 ./__spark_libs__/parquet-jackson-1.7.0.jar
6292218   64 -rwxr-xr-x   1 yarn     hadoop      65012 Apr  5 13:43 ./__spark_libs__/guice-servlet-3.0.jar
6292280 1616 -rwxr-xr-x   1 yarn     hadoop    1654097 Apr  5 13:43 ./__spark_libs__/hadoop-yarn-common-2.7.1.jar
5245267 1248 -rwxr-xr-x   1 yarn     hadoop    1277883 Apr  4 16:10 ./__spark_libs__/mesos-0.21.1-shaded-protobuf.jar
5243024  532 -rwxr-xr-x   1 yarn     hadoop     540852 Mar 28 23:40 ./__spark_libs__/mysql-connector-java-5.0.8-bin.jar
6292753 1772 -rwxr-xr-x   1 yarn     hadoop    1814309 Apr  5 13:43 ./__spark_libs__/spark-hive-thriftserver_2.11-2.0.1.jar
6292349  676 -rwxr-xr-x   1 yarn     hadoop     691479 Apr  5 13:43 ./__spark_libs__/apacheds-kerberos-codec-2.0.0-M15.jar
5242967   44 -rwxr-xr-x   1 yarn     hadoop      41755 Mar 28 23:40 ./__spark_libs__/objenesis-2.1.jar
5243099  656 -rwxr-xr-x   1 yarn     hadoop     669589 Mar 28 23:40 ./__spark_libs__/spark-yarn_2.11-2.0.1.jar
6292327  404 -rwxr-xr-x   1 yarn     hadoop     412739 Apr  5 13:43 ./__spark_libs__/commons-lang3-3.3.2.jar
6292750 1024 -rwxr-xr-x   1 yarn     hadoop    1044511 Apr  5 13:43 ./__spark_libs__/spark-hive_2.11-2.0.1.jar
6292658   96 -rwxr-xr-x   1 yarn     hadoop      94672 Apr  5 13:43 ./__spark_libs__/xz-1.0.jar
5243816 1512 -rwxr-xr-x   1 yarn     hadoop    1544875 Apr  4 16:10 ./__spark_libs__/hadoop-mapreduce-client-core-2.7.1.jar
5242988   20 -rwxr-xr-x   1 yarn     hadoop      16430 Mar 28 23:40 ./__spark_libs__/jcl-over-slf4j-1.7.16.jar
5243139 1024 -rwxr-xr-x   1 yarn     hadoop    1045744 Mar 28 23:40 ./__spark_libs__/leveldbjni-all-1.8.jar
5245277   20 -rwxr-xr-x   1 yarn     hadoop      17385 Apr  4 16:10 ./__spark_libs__/hadoop-annotations-2.7.1.jar
6292262   44 -rwxr-xr-x   1 yarn     hadoop      41070 Apr  5 13:43 ./__spark_libs__/spark-unsafe_2.11-2.0.1.jar
5242998  256 -rwxr-xr-x   1 yarn     hadoop     258876 Mar 28 23:40 ./__spark_libs__/jackson-core-2.6.5.jar
5243076   20 -rwxr-xr-x   1 yarn     hadoop      18098 Mar 28 23:40 ./__spark_libs__/jersey-container-servlet-2.22.2.jar
6292194  204 -rwxr-xr-x   1 yarn     hadoop     206035 Apr  5 13:43 ./__spark_libs__/commons-beanutils-core-1.8.0.jar
6292736  388 -rwxr-xr-x   1 yarn     hadoop     395195 Apr  5 13:43 ./__spark_libs__/javolution-5.5.1.jar
6292672   84 -rwxr-xr-x   1 yarn     hadoop      82421 Apr  5 13:43 ./__spark_libs__/json4s-ast_2.11-3.2.11.jar
6292644   64 -rwxr-xr-x   1 yarn     hadoop      62050 Apr  5 13:43 ./__spark_libs__/commons-logging-1.1.3.jar
5243072  144 -rwxr-xr-x   1 yarn     hadoop     143602 Mar 28 23:40 ./__spark_libs__/commons-digester-1.8.jar
5243061  788 -rwxr-xr-x   1 yarn     hadoop     802818 Mar 28 23:40 ./__spark_libs__/scalap-2.11.8.jar
6292804  736 -rwxr-xr-x   1 yarn     hadoop     753012 Apr  5 13:43 ./__spark_libs__/hadoop-mapreduce-client-common-2.7.1.jar
6292303   12 -rwxr-xr-x   1 yarn     hadoop      12131 Apr  5 13:43 ./__spark_libs__/jpam-1.1.jar
5243123   52 -rwxr-xr-x   1 yarn     hadoop      50619 Mar 28 23:40 ./__spark_libs__/chill-java-0.8.0.jar
5245269 1204 -rwxr-xr-x   1 yarn     hadoop    1230201 Apr  4 16:10 ./__spark_libs__/netty-3.8.0.Final.jar
5245264   88 -rwxr-xr-x   1 yarn     hadoop      86811 Apr  4 16:10 ./__spark_libs__/spire-macros_2.11-0.7.4.jar
5243042   44 -rwxr-xr-x   1 yarn     hadoop      41263 Mar 28 23:40 ./__spark_libs__/jackson-module-paranamer-2.6.5.jar
5243155  180 -rwxr-xr-x   1 yarn     hadoop     181271 Mar 28 23:40 ./__spark_libs__/hk2-locator-2.4.0-b34.jar
6292556 3152 -rwxr-xr-x   1 yarn     hadoop    3224708 Apr  5 13:43 ./__spark_libs__/derby-10.12.1.1.jar
6292236   40 -rwxr-xr-x   1 yarn     hadoop      40817 Apr  5 13:43 ./__spark_libs__/hive-cli-1.2.1.spark2.jar
6292559 1768 -rwxr-xr-x   1 yarn     hadoop    1809447 Apr  5 13:43 ./__spark_libs__/datanucleus-rdbms-3.2.9.jar
5243836   32 -rwxr-xr-x   1 yarn     hadoop      29540 Apr  4 16:10 ./__spark_libs__/spark-sketch_2.11-2.0.1.jar
6292623  184 -rwxr-xr-x   1 yarn     hadoop     185245 Apr  5 13:43 ./__spark_libs__/curator-framework-2.6.0.jar
6292780  308 -rwxr-xr-x   1 yarn     hadoop     313686 Apr  5 13:43 ./__spark_libs__/libfb303-0.9.2.jar
6292329  576 -rwxr-xr-x   1 yarn     hadoop     588337 Apr  5 13:43 ./__spark_libs__/commons-collections-3.2.2.jar
6292689 1168 -rwxr-xr-x   1 yarn     hadoop    1194003 Apr  5 13:43 ./__spark_libs__/arpack_combined_all-0.1.jar
5243106 13136 -rwxr-xr-x   1 yarn     hadoop   13448966 Mar 28 23:40 ./__spark_libs__/breeze_2.11-0.11.2.jar
5243132  208 -rwxr-xr-x   1 yarn     hadoop     209622 Mar 28 23:40 ./__spark_libs__/parquet-hadoop-1.7.0.jar
6292178   16 -rwxr-xr-x   1 yarn     hadoop      15827 Apr  5 13:43 ./__spark_libs__/metrics-json-3.1.2.jar
6292669  668 -rwxr-xr-x   1 yarn     hadoop     680106 Apr  5 13:43 ./__spark_libs__/spark-graphx_2.11-2.0.1.jar
5243150   20 -rwxr-xr-x   1 yarn     hadoop      17008 Mar 28 23:40 ./__spark_libs__/base64-2.3.8.jar
5242982 15128 -rwxr-xr-x   1 yarn     hadoop   15487351 Mar 28 23:40 ./__spark_libs__/scala-compiler-2.11.8.jar
6292660   20 -rwxr-xr-x   1 yarn     hadoop      18336 Apr  5 13:43 ./__spark_libs__/jackson-jaxrs-1.9.13.jar
6292246  280 -rwxr-xr-x   1 yarn     hadoop     284220 Apr  5 13:43 ./__spark_libs__/commons-lang-2.6.jar
6292363 5712 -rwxr-xr-x   1 yarn     hadoop    5847591 Apr  5 13:43 ./__spark_libs__/spark-mllib_2.11-2.0.1.jar
5243167  720 -rwxr-xr-x   1 yarn     hadoop     736658 Mar 28 23:40 ./__spark_libs__/httpclient-4.5.2.jar
5243141  232 -rwxr-xr-x   1 yarn     hadoop     236660 Mar 28 23:40 ./__spark_libs__/ST4-4.0.4.jar
6292675 1972 -rwxr-xr-x   1 yarn     hadoop    2015514 Apr  5 13:43 ./__spark_libs__/hadoop-yarn-api-2.7.1.jar
5243814 5612 -rwxr-xr-x   1 yarn     hadoop    5744974 Apr  4 16:10 ./__spark_libs__/scala-library-2.11.8.jar
6292368  300 -rwxr-xr-x   1 yarn     hadoop     305001 Apr  5 13:43 ./__spark_libs__/commons-httpclient-3.1.jar
5243003  576 -rwxr-xr-x   1 yarn     hadoop     589462 Mar 28 23:40 ./__spark_libs__/json4s-core_2.11-3.2.11.jar
5243019  188 -rwxr-xr-x   1 yarn     hadoop     190432 Mar 28 23:40 ./__spark_libs__/gson-2.2.4.jar
5243045  244 -rwxr-xr-x   1 yarn     hadoop     248171 Mar 28 23:40 ./__spark_libs__/curator-recipes-2.6.0.jar
6292616  696 -rwxr-xr-x   1 yarn     hadoop     710492 Apr  5 13:43 ./__spark_libs__/guice-3.0.jar
5242897  636 -rwxr-xr-x   1 yarn     hadoop     648678 Mar 28 23:40 ./__spark_libs__/scala-xml_2.11-1.0.2.jar
6292253  100 -rwxr-xr-x   1 yarn     hadoop     100636 Apr  5 13:43 ./__spark_libs__/jsp-api-2.1.jar
5243068   20 -rwxr-xr-x   1 yarn     hadoop      19827 Mar 28 23:40 ./__spark_libs__/opencsv-2.3.jar
6292587  616 -rwxr-xr-x   1 yarn     hadoop     627814 Apr  5 13:43 ./__spark_libs__/joda-time-2.9.3.jar
5243156  528 -rwxr-xr-x   1 yarn     hadoop     539912 Mar 28 23:40 ./__spark_libs__/jetty-6.1.26.jar
6292646  512 -rwxr-xr-x   1 yarn     hadoop     521157 Apr  5 13:43 ./__spark_libs__/mail-1.4.7.jar
6292374  200 -rwxr-xr-x   1 yarn     hadoop     201124 Apr  5 13:43 ./__spark_libs__/jdo-api-3.0.1.jar
5243108   68 -rwxr-xr-x   1 yarn     hadoop      65653 Mar 28 23:40 ./__spark_libs__/spark-launcher_2.11-2.0.1.jar
6292808    8 -rwxr-xr-x   1 yarn     hadoop       5310 Apr  5 13:43 ./__spark_libs__/pmml-schema-1.2.15.jar
5245273   68 -rwxr-xr-x   1 yarn     hadoop      68866 Apr  4 16:10 ./__spark_libs__/curator-client-2.6.0.jar
5243162  436 -rwxr-xr-x   1 yarn     hadoop     442406 Mar 28 23:40 ./__spark_libs__/calcite-linq4j-1.2.0-incubating.jar
5244220  164 -rwxr-xr-x   1 yarn     hadoop     164368 Apr  4 16:10 ./__spark_libs__/antlr-runtime-3.4.jar
6292243   16 -rwxr-xr-x   1 yarn     hadoop      14766 Apr  5 13:43 ./__spark_libs__/aopalliance-repackaged-2.4.0-b34.jar
5243022   12 -rwxr-xr-x   1 yarn     hadoop       9939 Mar 28 23:40 ./__spark_libs__/slf4j-log4j12-1.7.16.jar
6292526 6144 -rwxr-xr-x   1 yarn     hadoop    6290315 Apr  5 13:43 ./__spark_libs__/spark-sql_2.11-2.0.1.jar
6292295  116 -rwxr-xr-x   1 yarn     hadoop     114913 Apr  5 13:43 ./__spark_libs__/py4j-0.10.3.jar
5242921    4 -rwxr-xr-x   1 yarn     hadoop       2545 Mar 28 23:40 ./__spark_libs__/hadoop-client-2.7.1.jar
6292305  380 -rwxr-xr-x   1 yarn     hadoop     387188 Apr  5 13:43 ./__spark_libs__/parquet-format-2.3.0-incubating.jar
6292462 1204 -rwxr-xr-x   1 yarn     hadoop    1229125 Apr  5 13:43 ./__spark_libs__/xercesImpl-2.9.1.jar
6292679  212 -rwxr-xr-x   1 yarn     hadoop     213911 Apr  5 13:43 ./__spark_libs__/jline-2.12.1.jar
6292599   16 -rwxr-xr-x   1 yarn     hadoop      15010 Apr  5 13:43 ./__spark_libs__/xmlenc-0.52.jar
5243048 1996 -rwxr-xr-x   1 yarn     hadoop    2041628 Mar 28 23:40 ./__spark_libs__/jets3t-0.9.3.jar
5243035   64 -rwxr-xr-x   1 yarn     hadoop      63316 Mar 28 23:40 ./__spark_libs__/spark-repl_2.11-2.0.1.jar
6292700 2140 -rwxr-xr-x   1 yarn     hadoop    2189117 Apr  5 13:43 ./__spark_libs__/guava-14.0.1.jar
6292614  352 -rwxr-xr-x   1 yarn     hadoop     358390 Apr  5 13:43 ./__spark_libs__/kryo-shaded-3.0.3.jar
6292180  224 -rwxr-xr-x   1 yarn     hadoop     227712 Apr  5 13:43 ./__spark_libs__/libthrift-0.9.2.jar
6292666   44 -rwxr-xr-x   1 yarn     hadoop      45015 Apr  5 13:43 ./__spark_libs__/hadoop-mapreduce-client-shuffle-2.7.1.jar
5243009  144 -rwxr-xr-x   1 yarn     hadoop     144660 Mar 28 23:40 ./__spark_libs__/xbean-asm5-shaded-4.4.jar
6292287   24 -rwxr-xr-x   1 yarn     hadoop      23346 Apr  5 13:43 ./__spark_libs__/stax-api-1.0-2.jar
6292229   48 -rwxr-xr-x   1 yarn     hadoop      46983 Apr  5 13:43 ./__spark_libs__/jackson-annotations-2.6.5.jar
5243120  112 -rwxr-xr-x   1 yarn     hadoop     110600 Mar 28 23:40 ./__spark_libs__/bonecp-0.8.0.RELEASE.jar
6292663   28 -rwxr-xr-x   1 yarn     hadoop      27084 Apr  5 13:43 ./__spark_libs__/jackson-xc-1.9.13.jar
5243077  600 -rwxr-xr-x   1 yarn     hadoop     613299 Mar 28 23:40 ./__spark_libs__/janino-2.7.8.jar
6292268  280 -rwxr-xr-x   1 yarn     hadoop     285447 Apr  5 13:43 ./__spark_libs__/parquet-encoding-1.7.0.jar
6292221   44 -rwxr-xr-x   1 yarn     hadoop      44925 Apr  5 13:43 ./__spark_libs__/apacheds-i18n-2.0.0-M15.jar
6292264  416 -rwxr-xr-x   1 yarn     hadoop     423753 Apr  5 13:43 ./__spark_libs__/scala-parser-combinators_2.11-1.0.4.jar
5243834   72 -rwxr-xr-x   1 yarn     hadoop      72733 Apr  4 16:10 ./__spark_libs__/jersey-media-jaxb-2.22.2.jar
6292377    8 -rwxr-xr-x   1 yarn     hadoop       4596 Apr  5 13:43 ./__spark_libs__/jul-to-slf4j-1.7.16.jar
5243054  284 -rwxr-xr-x   1 yarn     hadoop     290506 Mar 28 23:40 ./__spark_libs__/univocity-parsers-2.1.1.jar
5242960   96 -rwxr-xr-x   1 yarn     hadoop      96221 Mar 28 23:40 ./__spark_libs__/commons-pool-1.5.4.jar
6292620   64 -rwxr-xr-x   1 yarn     hadoop      65261 Apr  5 13:43 ./__spark_libs__/oro-2.0.8.jar
6292548 2304 -rwxr-xr-x   1 yarn     hadoop    2355465 Apr  5 13:43 ./__spark_libs__/spark-network-common_2.11-2.0.1.jar
6292340  356 -rwxr-xr-x   1 yarn     hadoop     363908 Apr  5 13:43 ./__spark_libs__/hadoop-yarn-server-common-2.7.1.jar
6292324   72 -rwxr-xr-x   1 yarn     hadoop      70688 Apr  5 13:43 ./__spark_libs__/hadoop-auth-2.7.1.jar
6292275  684 -rwxr-xr-x   1 yarn     hadoop     698375 Apr  5 13:43 ./__spark_libs__/jersey-common-2.22.2.jar
5242950  188 -rwxr-xr-x   1 yarn     hadoop     188671 Mar 28 23:40 ./__spark_libs__/commons-beanutils-1.7.0.jar
6292794 2008 -rwxr-xr-x   1 yarn     hadoop    2054931 Apr  5 13:43 ./__spark_libs__/netty-all-4.0.29.Final.jar
5243015   40 -rwxr-xr-x   1 yarn     hadoop      39280 Mar 28 23:40 ./__spark_libs__/metrics-jvm-3.1.2.jar
6292713   68 -rwxr-xr-x   1 yarn     hadoop      69409 Apr  5 13:43 ./__spark_libs__/activation-1.1.1.jar
6292283   48 -rwxr-xr-x   1 yarn     hadoop      48720 Apr  5 13:43 ./__spark_libs__/snappy-0.2.jar
6292798  164 -rwxr-xr-x   1 yarn     hadoop     167421 Apr  5 13:43 ./__spark_libs__/jersey-client-2.22.2.jar
6292636   20 -rwxr-xr-x   1 yarn     hadoop      20235 Apr  5 13:43 ./__spark_libs__/osgi-resource-locator-1.0.1.jar
6292299    8 -rwxr-xr-x   1 yarn     hadoop       5711 Apr  5 13:43 ./__spark_libs__/minlog-1.3.0.jar
5243854  436 -rwxr-xr-x   1 yarn     hadoop     445288 Apr  4 16:10 ./__spark_libs__/antlr-2.7.7.jar
6292682 3352 -rwxr-xr-x   1 yarn     hadoop    3431544 Apr  5 13:43 ./__spark_libs__/hadoop-common-2.7.1.jar
5243126 1592 -rwxr-xr-x   1 yarn     hadoop    1627065 Mar 28 23:40 ./__spark_libs__/jersey-bundle-1.19.1.jar
5243051  160 -rwxr-xr-x   1 yarn     hadoop     160519 Mar 28 23:40 ./__spark_libs__/commons-dbcp-1.4.jar
6292529   80 -rwxr-xr-x   1 yarn     hadoop      79912 Apr  5 13:43 ./__spark_libs__/api-util-1.0.0-M20.jar
6292766   44 -rwxr-xr-x   1 yarn     hadoop      41123 Apr  5 13:43 ./__spark_libs__/commons-cli-1.2.jar
6292634  504 -rwxr-xr-x   1 yarn     hadoop     515604 Apr  5 13:43 ./__spark_libs__/jackson-module-scala_2.11-2.6.5.jar
5242985  428 -rwxr-xr-x   1 yarn     hadoop     436303 Mar 28 23:40 ./__spark_libs__/avro-1.7.7.jar
5242965   64 -rwxr-xr-x   1 yarn     hadoop      63777 Mar 28 23:40 ./__spark_libs__/validation-api-1.1.0.Final.jar
5243114 1256 -rwxr-xr-x   1 yarn     hadoop    1282424 Mar 28 23:40 ./__spark_libs__/ivy-2.4.0.jar
5242973   40 -rwxr-xr-x   1 yarn     hadoop      40341 Mar 28 23:40 ./__spark_libs__/json4s-jackson_2.11-3.2.11.jar
5245288  332 -rwxr-xr-x   1 yarn     hadoop     339666 Apr  4 16:10 ./__spark_libs__/datanucleus-api-jdo-3.2.6.jar
6292214 3440 -rwxr-xr-x   1 yarn     hadoop    3519262 Apr  5 13:43 ./__spark_libs__/calcite-core-1.2.0-incubating.jar
5242963   16 -rwxr-xr-x   1 yarn     hadoop      15305 Mar 28 23:40 ./__spark_libs__/spark-tags_2.11-2.0.1.jar
5243007   32 -rwxr-xr-x   1 yarn     hadoop      29555 Mar 28 23:40 ./__spark_libs__/paranamer-2.3.jar
6292755  400 -rwxr-xr-x   1 yarn     hadoop     409467 Apr  5 13:43 ./__spark_libs__/mx4j-3.0.2.jar
5243095   40 -rwxr-xr-x   1 yarn     hadoop      40509 Mar 28 23:40 ./__spark_libs__/slf4j-api-1.7.16.jar
5242975 2044 -rwxr-xr-x   1 yarn     hadoop    2090370 Mar 28 23:40 ./__spark_libs__/spark-streaming_2.11-2.0.1.jar
6292639   24 -rwxr-xr-x   1 yarn     hadoop      20852 Apr  5 13:43 ./__spark_libs__/metrics-graphite-3.1.2.jar
5243893  184 -rwxr-xr-x   1 yarn     hadoop     185140 Apr  4 16:10 ./__spark_libs__/commons-io-2.4.jar
6292365 1032 -rwxr-xr-x   1 yarn     hadoop    1056168 Apr  5 13:43 ./__spark_libs__/snappy-java-1.1.2.6.jar
6292592   36 -rwxr-xr-x   1 yarn     hadoop      33015 Apr  5 13:43 ./__spark_libs__/jsr305-1.3.9.jar
5243102  764 -rwxr-xr-x   1 yarn     hadoop     780664 Mar 28 23:40 ./__spark_libs__/jackson-mapper-asl-1.9.13.jar
5242941   48 -rwxr-xr-x   1 yarn     hadoop      45944 Mar 28 23:40 ./__spark_libs__/json-20090211.jar
6292205  776 -rwxr-xr-x   1 yarn     hadoop     792964 Apr  5 13:43 ./__spark_libs__/zookeeper-3.4.6.jar
6292355  232 -rwxr-xr-x   1 yarn     hadoop     236880 Apr  5 13:43 ./__spark_libs__/lz4-1.3.0.jar
6292650  896 -rwxr-xr-x   1 yarn     hadoop     917052 Apr  5 13:43 ./__spark_libs__/parquet-column-1.7.0.jar
5243825  256 -rwxr-xr-x   1 yarn     hadoop     258370 Apr  4 16:10 ./__spark_libs__/calcite-avatica-1.2.0-incubating.jar
5243039  932 -rwxr-xr-x   1 yarn     hadoop     951701 Mar 28 23:40 ./__spark_libs__/jersey-server-2.22.2.jar
6292727  140 -rwxr-xr-x   1 yarn     hadoop     142631 Apr  5 13:43 ./__spark_libs__/hadoop-yarn-client-2.7.1.jar
5242954    8 -rwxr-xr-x   1 yarn     hadoop       5950 Mar 28 23:40 ./__spark_libs__/javax.inject-2.4.0-b34.jar
6292715  420 -rwxr-xr-x   1 yarn     hadoop     427780 Apr  5 13:43 ./__spark_libs__/jodd-core-3.5.2.jar
6292777  296 -rwxr-xr-x   1 yarn     hadoop     302248 Apr  5 13:43 ./__spark_libs__/antlr4-runtime-4.5.3.jar
6292285  952 -rwxr-xr-x   1 yarn     hadoop     971310 Apr  5 13:43 ./__spark_libs__/jersey-guava-2.22.2.jar
6292278   20 -rwxr-xr-x   1 yarn     hadoop      16560 Apr  5 13:43 ./__spark_libs__/api-asn1-api-1.0.0-M20.jar
5243855  104 -rwxr-xr-x   1 yarn     hadoop     105134 Apr  4 16:10 ./__spark_libs__/jaxb-api-2.2.2.jar
6292627   20 -rwxr-xr-x   1 yarn     hadoop      18482 Apr  5 13:43 ./__spark_libs__/eigenbase-properties-1.1.5.jar
5243815  112 -rwxr-xr-x   1 yarn     hadoop     112558 Apr  4 16:10 ./__spark_libs__/metrics-core-3.1.2.jar
6292347  220 -rwxr-xr-x   1 yarn     hadoop     223573 Apr  5 13:43 ./__spark_libs__/chill_2.11-0.8.0.jar
6292694  440 -rwxr-xr-x   1 yarn     hadoop     448794 Apr  5 13:43 ./__spark_libs__/apache-log4j-extras-1.2.17.jar
6292226   56 -rwxr-xr-x   1 yarn     hadoop      55511 Apr  5 13:43 ./__spark_libs__/spark-network-shuffle_2.11-2.0.1.jar
5243087   92 -rwxr-xr-x   1 yarn     hadoop      93407 Mar 28 23:40 ./__spark_libs__/pyrolite-4.9.jar
6292762   28 -rwxr-xr-x   1 yarn     hadoop      26514 Apr  5 13:43 ./__spark_libs__/stax-api-1.0.1.jar
5243159  192 -rwxr-xr-x   1 yarn     hadoop     192993 Mar 28 23:40 ./__spark_libs__/avro-ipc-1.7.7.jar
6292632  320 -rwxr-xr-x   1 yarn     hadoop     326724 Apr  5 13:43 ./__spark_libs__/httpcore-4.4.4.jar
6292744  468 -rwxr-xr-x   1 yarn     hadoop     477970 Apr  5 13:43 ./__spark_libs__/lift-json_2.11-2.6.3.jar
6292796  508 -rwxr-xr-x   1 yarn     hadoop     516127 Apr  5 13:43 ./__spark_libs__/hadoop-mapreduce-client-app-2.7.1.jar
6292297  292 -rwxr-xr-x   1 yarn     hadoop     298829 Apr  5 13:43 ./__spark_libs__/commons-configuration-1.6.jar
6292257 5380 -rwxr-xr-x   1 yarn     hadoop    5505200 Apr  5 13:43 ./__spark_libs__/hive-metastore-1.2.1.spark2.jar
6292216  100 -rwxr-xr-x   1 yarn     hadoop     100680 Apr  5 13:43 ./__spark_libs__/hive-jdbc-1.2.1.spark2.jar
6292771   24 -rwxr-xr-x   1 yarn     hadoop      21243 Apr  5 13:43 ./__spark_libs__/parquet-generator-1.7.0.jar
15861144    4 drwx------   2 yarn     hadoop       4096 May 16 11:10 ./__spark_conf__
15861168    8 -r-x------   1 yarn     hadoop       5404 May 16 11:10 ./__spark_conf__/capacity-scheduler.xml
15861298    4 -r-x------   1 yarn     hadoop       1020 May 16 11:10 ./__spark_conf__/commons-logging.properties
15861284    4 -r-x------   1 yarn     hadoop       1019 May 16 11:10 ./__spark_conf__/container-executor.cfg
15861163    4 -r-x------   1 yarn     hadoop       1631 May 16 11:10 ./__spark_conf__/kms-log4j.properties
15861269    8 -r-x------   1 yarn     hadoop       6572 May 16 11:10 ./__spark_conf__/hdfs-site.xml
15861170    4 -r-x------   1 yarn     hadoop       3518 May 16 11:10 ./__spark_conf__/kms-acls.xml
15861310    4 -r-x------   1 yarn     hadoop       1000 May 16 11:10 ./__spark_conf__/ssl-server.xml
15861150   20 -r-x------   1 yarn     hadoop      16530 May 16 11:10 ./__spark_conf__/yarn-site.xml
15861299    4 -r-x------   1 yarn     hadoop       2490 May 16 11:10 ./__spark_conf__/hadoop-metrics.properties
15861148   12 -r-x------   1 yarn     hadoop       8699 May 16 11:10 ./__spark_conf__/log4j.properties
15861314    4 -r-x------   1 yarn     hadoop        945 May 16 11:10 ./__spark_conf__/taskcontroller.cfg
15861304    4 -r-x------   1 yarn     hadoop       2316 May 16 11:10 ./__spark_conf__/ssl-client.xml.example
15861311    4 -r-x------   1 yarn     hadoop       2268 May 16 11:10 ./__spark_conf__/ssl-server.xml.example
15861149    4 -r-x------   1 yarn     hadoop       2302 May 16 11:10 ./__spark_conf__/hadoop-metrics2.properties
15861303    4 -r-x------   1 yarn     hadoop       1602 May 16 11:10 ./__spark_conf__/health_check
15861307    4 -r-x------   1 yarn     hadoop        229 May 16 11:10 ./__spark_conf__/slaves
15861172    4 -r-x------   1 yarn     hadoop       2358 May 16 11:10 ./__spark_conf__/topology_script.py
15861159    0 -r-x------   1 yarn     hadoop          0 May 16 11:10 ./__spark_conf__/yarn.exclude
15861309    4 -r-x------   1 yarn     hadoop        951 May 16 11:10 ./__spark_conf__/mapred-env.cmd
15861147    8 -r-x------   1 yarn     hadoop       5467 May 16 11:10 ./__spark_conf__/hadoop-env.sh
15861301    4 -r-x------   1 yarn     hadoop        690 May 16 11:10 ./__spark_conf__/mapred-env.sh
15861305    4 -r-x------   1 yarn     hadoop       1527 May 16 11:10 ./__spark_conf__/kms-env.sh
15861313    8 -r-x------   1 yarn     hadoop       5511 May 16 11:10 ./__spark_conf__/kms-site.xml
15861145    8 -r-x------   1 yarn     hadoop       6836 May 16 11:10 ./__spark_conf__/mapred-site.xml
15861300    8 -r-x------   1 yarn     hadoop       4221 May 16 11:10 ./__spark_conf__/task-log4j.properties
15861164    4 -r-x------   1 yarn     hadoop       2250 May 16 11:10 ./__spark_conf__/yarn-env.cmd
15861171    4 -r-x------   1 yarn     hadoop        758 May 16 11:10 ./__spark_conf__/mapred-site.xml.template
15861151    4 -r-x------   1 yarn     hadoop       3979 May 16 11:10 ./__spark_conf__/hadoop-env.cmd
15861315   36 -r-x------   1 yarn     hadoop      34061 May 16 11:10 ./__spark_conf__/__spark_conf__.properties
15861308    4 -r-x------   1 yarn     hadoop        724 May 16 11:10 ./__spark_conf__/topology_mappings.data
15861162    8 -r-x------   1 yarn     hadoop       4197 May 16 11:10 ./__spark_conf__/core-site.xml
15861173    4 -r-x------   1 yarn     hadoop       1335 May 16 11:10 ./__spark_conf__/configuration.xsl
15861306    4 -r-x------   1 yarn     hadoop       1308 May 16 11:10 ./__spark_conf__/hadoop-policy.xml
15861312    8 -r-x------   1 yarn     hadoop       4113 May 16 11:10 ./__spark_conf__/mapred-queues.xml.template
15861177    8 -r-x------   1 yarn     hadoop       5006 May 16 11:10 ./__spark_conf__/yarn-env.sh
15861165    4 -r-x------   1 yarn     hadoop        884 May 16 11:10 ./__spark_conf__/ssl-client.xml
broken symlinks(find -L . -maxdepth 5 -type l -ls):
End of LogType:directory.info

LogType:launch_container.sh
Log Upload Time:Tue May 16 11:12:21 +0700 2017
LogLength:75408
Log Contents:
#!/bin/bash

export SPARK_YARN_STAGING_DIR="hdfs://c408.hadoop.gda.lo:8020/user/fairy/.sparkStaging/application_1494395298335_303847"
export LOCAL_DIRS="/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847"
export HADOOP_CONF_DIR="/usr/hdp/current/hadoop-client/conf"
export NM_HTTP_PORT="8042"
export JAVA_HOME="/usr/jdk64/jdk1.8.0_92/"
export LOG_DIRS="/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000003"
export NM_AUX_SERVICE_mapreduce_shuffle="AAA0+gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA=
"
export NM_PORT="45454"
export USER="fairy"
export HADOOP_YARN_HOME="/usr/hdp/current/hadoop-yarn-nodemanager"
export CLASSPATH="$PWD:$PWD/__spark_conf__:$PWD/__spark_libs__/*:$HADOOP_CONF_DIR:/usr/hdp/current/hadoop-client/*:/usr/hdp/current/hadoop-client/lib/*:/usr/hdp/current/hadoop-hdfs-client/*:/usr/hdp/current/hadoop-hdfs-client/lib/*:/usr/hdp/current/hadoop-yarn-client/*:/usr/hdp/current/hadoop-yarn-client/lib/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/2.4.2.0-258/hadoop/lib/hadoop-lzo-0.6.0.2.4.2.0-258.jar:/etc/hadoop/conf/secure"
export SPARK_LOG_URL_STDOUT="http://c433.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_01_000003/fairy/stdout?start=-4096"
export NM_HOST="c433.hadoop.gda.lo"
export SPARK_YARN_MODE="true"
export HADOOP_TOKEN_FILE_LOCATION="/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_01_000003/container_tokens"
export NM_AUX_SERVICE_spark_shuffle=""
export SPARK_USER="fairy"
export SPARK_LOG_URL_STDERR="http://c433.hadoop.gda.lo:8042/node/containerlogs/container_e81_1494395298335_303847_01_000003/fairy/stderr?start=-4096"
export LOGNAME="fairy"
export JVM_PID="$$"
export PWD="/hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/container_e81_1494395298335_303847_01_000003"
export HOME="/home/"
export CONTAINER_ID="container_e81_1494395298335_303847_01_000003"
export MALLOC_ARENA_MAX="4"
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/57/jersey-server-2.22.2.jar" "__spark_libs__/jersey-server-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/44/spark-mllib-local_2.11-2.0.1.jar" "__spark_libs__/spark-mllib-local_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3700/hadoop-mapreduce-client-common-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/58/jackson-module-paranamer-2.6.5.jar" "__spark_libs__/jackson-module-paranamer-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3663/parquet-jackson-1.7.0.jar" "__spark_libs__/parquet-jackson-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3691/commons-cli-1.2.jar" "__spark_libs__/commons-cli-1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3693/stream-2.7.0.jar" "__spark_libs__/stream-2.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3675/hadoop-mapreduce-client-jobclient-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-jobclient-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3690/stax-api-1.0.1.jar" "__spark_libs__/stax-api-1.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/46/jersey-container-servlet-core-2.22.2.jar" "__spark_libs__/jersey-container-servlet-core-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3657/osgi-resource-locator-1.0.1.jar" "__spark_libs__/osgi-resource-locator-1.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3158/spire-macros_2.11-0.7.4.jar" "__spark_libs__/spire-macros_2.11-0.7.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3606/scala-parser-combinators_2.11-1.0.4.jar" "__spark_libs__/scala-parser-combinators_2.11-1.0.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3623/commons-lang3-3.3.2.jar" "__spark_libs__/commons-lang3-3.3.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3643/derby-10.12.1.1.jar" "__spark_libs__/derby-10.12.1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3144/hadoop-mapreduce-client-core-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-core-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/68/commons-digester-1.8.jar" "__spark_libs__/commons-digester-1.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/89/compress-lzf-1.0.3.jar" "__spark_libs__/compress-lzf-1.0.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3688/mx4j-3.0.2.jar" "__spark_libs__/mx4j-3.0.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3634/minlog-1.3.0.jar" "__spark_libs__/minlog-1.3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3618/log4j-1.2.17.jar" "__spark_libs__/log4j-1.2.17.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/62/univocity-parsers-2.1.1.jar" "__spark_libs__/univocity-parsers-2.1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3647/xmlenc-0.52.jar" "__spark_libs__/xmlenc-0.52.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/39/avro-1.7.7.jar" "__spark_libs__/avro-1.7.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/28/javax.servlet-api-3.1.0.jar" "__spark_libs__/javax.servlet-api-3.1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3610/api-asn1-api-1.0.0-M20.jar" "__spark_libs__/api-asn1-api-1.0.0-M20.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/84/bonecp-0.8.0.RELEASE.jar" "__spark_libs__/bonecp-0.8.0.RELEASE.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/48/commons-math3-3.4.1.jar" "__spark_libs__/commons-math3-3.4.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3593/apacheds-i18n-2.0.0-M15.jar" "__spark_libs__/apacheds-i18n-2.0.0-M15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3645/joda-time-2.9.3.jar" "__spark_libs__/joda-time-2.9.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3619/parquet-format-2.3.0-incubating.jar" "__spark_libs__/parquet-format-2.3.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3617/commons-configuration-1.6.jar" "__spark_libs__/commons-configuration-1.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3602/jsp-api-2.1.jar" "__spark_libs__/jsp-api-2.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/35/hadoop-hdfs-2.7.1.jar" "__spark_libs__/hadoop-hdfs-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3159/mesos-0.21.1-shaded-protobuf.jar" "__spark_libs__/mesos-0.21.1-shaded-protobuf.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3589/zookeeper-3.4.6.jar" "__spark_libs__/zookeeper-3.4.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3599/aopalliance-repackaged-2.4.0-b34.jar" "__spark_libs__/aopalliance-repackaged-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3590/super-csv-2.2.0.jar" "__spark_libs__/super-csv-2.2.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/45/json4s-core_2.11-3.2.11.jar" "__spark_libs__/json4s-core_2.11-3.2.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3600/commons-lang-2.6.jar" "__spark_libs__/commons-lang-2.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/76/slf4j-api-1.7.16.jar" "__spark_libs__/slf4j-api-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/65/scalap-2.11.8.jar" "__spark_libs__/scalap-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3636/spark-core_2.11-2.0.1.jar" "__spark_libs__/spark-core_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3147/calcite-avatica-1.2.0-incubating.jar" "__spark_libs__/calcite-avatica-1.2.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3163/datanucleus-api-jdo-3.2.6.jar" "__spark_libs__/datanucleus-api-jdo-3.2.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3637/xercesImpl-2.9.1.jar" "__spark_libs__/xercesImpl-2.9.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/96/hk2-locator-2.4.0-b34.jar" "__spark_libs__/hk2-locator-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/59/curator-recipes-2.6.0.jar" "__spark_libs__/curator-recipes-2.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3668/spark-graphx_2.11-2.0.1.jar" "__spark_libs__/spark-graphx_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3156/jaxb-api-2.2.2.jar" "__spark_libs__/jaxb-api-2.2.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/75/commons-codec-1.10.jar" "__spark_libs__/commons-codec-1.10.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3591/calcite-core-1.2.0-incubating.jar" "__spark_libs__/calcite-core-1.2.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/26/scala-xml_2.11-1.0.2.jar" "__spark_libs__/scala-xml_2.11-1.0.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3155/commons-io-2.4.jar" "__spark_libs__/commons-io-2.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/32/spark-tags_2.11-2.0.1.jar" "__spark_libs__/spark-tags_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/33/objenesis-2.1.jar" "__spark_libs__/objenesis-2.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/79/breeze_2.11-0.11.2.jar" "__spark_libs__/breeze_2.11-0.11.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3639/spark-sql_2.11-2.0.1.jar" "__spark_libs__/spark-sql_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3597/hive-cli-1.2.1.spark2.jar" "__spark_libs__/hive-cli-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/82/ivy-2.4.0.jar" "__spark_libs__/ivy-2.4.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3667/hadoop-mapreduce-client-shuffle-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-shuffle-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3640/api-util-1.0.0-M20.jar" "__spark_libs__/api-util-1.0.0-M20.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3666/jackson-xc-1.9.13.jar" "__spark_libs__/jackson-xc-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3605/spark-unsafe_2.11-2.0.1.jar" "__spark_libs__/spark-unsafe_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3644/datanucleus-rdbms-3.2.9.jar" "__spark_libs__/datanucleus-rdbms-3.2.9.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3650/kryo-shaded-3.0.3.jar" "__spark_libs__/kryo-shaded-3.0.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3649/java-xmlbuilder-1.0.jar" "__spark_libs__/java-xmlbuilder-1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3628/apacheds-kerberos-codec-2.0.0-M15.jar" "__spark_libs__/apacheds-kerberos-codec-2.0.0-M15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3145/scala-library-2.11.8.jar" "__spark_libs__/scala-library-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/78/jackson-mapper-asl-1.9.13.jar" "__spark_libs__/jackson-mapper-asl-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3687/spark-hive_2.11-2.0.1.jar" "__spark_libs__/spark-hive_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/95/jetty-6.1.26.jar" "__spark_libs__/jetty-6.1.26.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3150/jersey-media-jaxb-2.22.2.jar" "__spark_libs__/jersey-media-jaxb-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3676/apache-log4j-extras-1.2.17.jar" "__spark_libs__/apache-log4j-extras-1.2.17.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3621/hive-jdbc-1.2.1.spark2.jar" "__spark_libs__/hive-jdbc-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/77/spark-yarn_2.11-2.0.1.jar" "__spark_libs__/spark-yarn_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3143/metrics-core-3.1.2.jar" "__spark_libs__/metrics-core-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3654/eigenbase-properties-1.1.5.jar" "__spark_libs__/eigenbase-properties-1.1.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3679/activation-1.1.1.jar" "__spark_libs__/activation-1.1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/55/jackson-core-2.6.5.jar" "__spark_libs__/jackson-core-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3625/hadoop-yarn-server-common-2.7.1.jar" "__spark_libs__/hadoop-yarn-server-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3592/guice-servlet-3.0.jar" "__spark_libs__/guice-servlet-3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3665/jackson-jaxrs-1.9.13.jar" "__spark_libs__/jackson-jaxrs-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3596/jackson-annotations-2.6.5.jar" "__spark_libs__/jackson-annotations-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3622/hadoop-auth-2.7.1.jar" "__spark_libs__/hadoop-auth-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/81/hadoop-yarn-server-web-proxy-2.7.1.jar" "__spark_libs__/hadoop-yarn-server-web-proxy-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3620/jpam-1.1.jar" "__spark_libs__/jpam-1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3614/javax.ws.rs-api-2.0.1.jar" "__spark_libs__/javax.ws.rs-api-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3655/snappy-java-1.1.2.6.jar" "__spark_libs__/snappy-java-1.1.2.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3658/metrics-graphite-3.1.2.jar" "__spark_libs__/metrics-graphite-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3626/commons-collections-3.2.2.jar" "__spark_libs__/commons-collections-3.2.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3701/pmml-schema-1.2.15.jar" "__spark_libs__/pmml-schema-1.2.15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3157/bcprov-jdk15on-1.51.jar" "__spark_libs__/bcprov-jdk15on-1.51.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3160/netty-3.8.0.Final.jar" "__spark_libs__/netty-3.8.0.Final.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/40/jcl-over-slf4j-1.7.16.jar" "__spark_libs__/jcl-over-slf4j-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/63/hk2-api-2.4.0-b34.jar" "__spark_libs__/hk2-api-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3653/javassist-3.18.1-GA.jar" "__spark_libs__/javassist-3.18.1-GA.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3153/antlr-2.7.7.jar" "__spark_libs__/antlr-2.7.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/29/javax.inject-2.4.0-b34.jar" "__spark_libs__/javax.inject-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3646/jsr305-1.3.9.jar" "__spark_libs__/jsr305-1.3.9.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3686/hk2-utils-2.4.0-b34.jar" "__spark_libs__/hk2-utils-2.4.0-b34.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3689/spark-hive-thriftserver_2.11-2.0.1.jar" "__spark_libs__/spark-hive-thriftserver_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3608/jtransforms-2.4.0.jar" "__spark_libs__/jtransforms-2.4.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3607/parquet-encoding-1.7.0.jar" "__spark_libs__/parquet-encoding-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/69/jersey-container-servlet-2.22.2.jar" "__spark_libs__/jersey-container-servlet-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/usercache/fairy/filecache/11522/__spark_conf__.zip" "__spark_conf__"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/85/chill-java-0.8.0.jar" "__spark_libs__/chill-java-0.8.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3651/guice-3.0.jar" "__spark_libs__/guice-3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/71/breeze-macros_2.11-0.11.2.jar" "__spark_libs__/breeze-macros_2.11-0.11.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3616/py4j-0.10.3.jar" "__spark_libs__/py4j-0.10.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3685/lift-json_2.11-2.6.3.jar" "__spark_libs__/lift-json_2.11-2.6.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3146/commons-compress-1.4.1.jar" "__spark_libs__/commons-compress-1.4.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/73/datanucleus-core-3.2.10.jar" "__spark_libs__/datanucleus-core-3.2.10.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3586/commons-beanutils-core-1.8.0.jar" "__spark_libs__/commons-beanutils-core-1.8.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/60/jets3t-0.9.3.jar" "__spark_libs__/jets3t-0.9.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3680/jodd-core-3.5.2.jar" "__spark_libs__/jodd-core-3.5.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3154/antlr-runtime-3.4.jar" "__spark_libs__/antlr-runtime-3.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/51/paranamer-2.3.jar" "__spark_libs__/paranamer-2.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3638/commons-net-2.2.jar" "__spark_libs__/commons-net-2.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3632/jdo-api-3.0.1.jar" "__spark_libs__/jdo-api-3.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3151/spark-sketch_2.11-2.0.1.jar" "__spark_libs__/spark-sketch_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/92/hive-exec-1.2.1.spark2.jar" "__spark_libs__/hive-exec-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/36/spark-streaming_2.11-2.0.1.jar" "__spark_libs__/spark-streaming_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/67/opencsv-2.3.jar" "__spark_libs__/opencsv-2.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3630/spark-mllib_2.11-2.0.1.jar" "__spark_libs__/spark-mllib_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3682/jta-1.1.jar" "__spark_libs__/jta-1.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3692/parquet-generator-1.7.0.jar" "__spark_libs__/parquet-generator-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/25/json-20090211.jar" "__spark_libs__/json-20090211.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3699/hadoop-mapreduce-client-app-2.7.1.jar" "__spark_libs__/hadoop-mapreduce-client-app-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3697/netty-all-4.0.29.Final.jar" "__spark_libs__/netty-all-4.0.29.Final.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3662/parquet-column-1.7.0.jar" "__spark_libs__/parquet-column-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3609/jersey-common-2.22.2.jar" "__spark_libs__/jersey-common-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/41/json4s-jackson_2.11-3.2.11.jar" "__spark_libs__/json4s-jackson_2.11-3.2.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3672/hadoop-common-2.7.1.jar" "__spark_libs__/hadoop-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/54/gson-2.2.4.jar" "__spark_libs__/gson-2.2.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/83/slf4j-log4j12-1.7.16.jar" "__spark_libs__/slf4j-log4j12-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/86/jersey-bundle-1.19.1.jar" "__spark_libs__/jersey-bundle-1.19.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3588/jetty-util-6.1.26.jar" "__spark_libs__/jetty-util-6.1.26.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/27/hadoop-client-2.7.1.jar" "__spark_libs__/hadoop-client-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3624/parquet-common-1.7.0.jar" "__spark_libs__/parquet-common-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3631/commons-httpclient-3.1.jar" "__spark_libs__/commons-httpclient-3.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3595/spark-network-shuffle_2.11-2.0.1.jar" "__spark_libs__/spark-network-shuffle_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/74/core-1.1.2.jar" "__spark_libs__/core-1.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3642/spark-network-common_2.11-2.0.1.jar" "__spark_libs__/spark-network-common_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3695/libfb303-0.9.2.jar" "__spark_libs__/libfb303-0.9.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3656/httpcore-4.4.4.jar" "__spark_libs__/httpcore-4.4.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3696/spire_2.11-0.7.4.jar" "__spark_libs__/spire_2.11-0.7.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/52/mysql-connector-java-5.0.8-bin.jar" "__spark_libs__/mysql-connector-java-5.0.8-bin.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/38/scala-compiler-2.11.8.jar" "__spark_libs__/scala-compiler-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/53/JavaEWAH-0.3.2.jar" "__spark_libs__/JavaEWAH-0.3.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/56/spark-repl_2.11-2.0.1.jar" "__spark_libs__/spark-repl_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3601/stringtemplate-3.2.1.jar" "__spark_libs__/stringtemplate-3.2.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3615/hadoop-yarn-common-2.7.1.jar" "__spark_libs__/hadoop-yarn-common-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3603/hive-metastore-1.2.1.spark2.jar" "__spark_libs__/hive-metastore-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/30/htrace-core-3.1.0-incubating.jar" "__spark_libs__/htrace-core-3.1.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3149/spark-catalyst_2.11-2.0.1.jar" "__spark_libs__/spark-catalyst_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3648/commons-compiler-2.7.6.jar" "__spark_libs__/commons-compiler-2.7.6.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3629/lz4-1.3.0.jar" "__spark_libs__/lz4-1.3.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3661/mail-1.4.7.jar" "__spark_libs__/mail-1.4.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3664/xz-1.0.jar" "__spark_libs__/xz-1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3627/chill_2.11-0.8.0.jar" "__spark_libs__/chill_2.11-0.8.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3669/json4s-ast_2.11-3.2.11.jar" "__spark_libs__/json4s-ast_2.11-3.2.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3678/jackson-databind-2.6.5.jar" "__spark_libs__/jackson-databind-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3587/avro-mapred-1.7.7-hadoop2.jar" "__spark_libs__/avro-mapred-1.7.7-hadoop2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3612/jersey-guava-2.22.2.jar" "__spark_libs__/jersey-guava-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3673/jackson-module-scala_2.11-2.6.5.jar" "__spark_libs__/jackson-module-scala_2.11-2.6.5.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/49/metrics-jvm-3.1.2.jar" "__spark_libs__/metrics-jvm-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3594/metrics-json-3.1.2.jar" "__spark_libs__/metrics-json-3.1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3671/jline-2.12.1.jar" "__spark_libs__/jline-2.12.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3161/curator-client-2.6.0.jar" "__spark_libs__/curator-client-2.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3635/javax.annotation-api-1.2.jar" "__spark_libs__/javax.annotation-api-1.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/47/xbean-asm5-shaded-4.4.jar" "__spark_libs__/xbean-asm5-shaded-4.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/66/jackson-core-asl-1.9.13.jar" "__spark_libs__/jackson-core-asl-1.9.13.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/87/javax.inject-1.jar" "__spark_libs__/javax.inject-1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3613/stax-api-1.0-2.jar" "__spark_libs__/stax-api-1.0-2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3162/hadoop-annotations-2.7.1.jar" "__spark_libs__/hadoop-annotations-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3674/arpack_combined_all-0.1.jar" "__spark_libs__/arpack_combined_all-0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3677/guava-14.0.1.jar" "__spark_libs__/guava-14.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/97/avro-ipc-1.7.7.jar" "__spark_libs__/avro-ipc-1.7.7.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3659/curator-framework-2.6.0.jar" "__spark_libs__/curator-framework-2.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/91/leveldbjni-all-1.8.jar" "__spark_libs__/leveldbjni-all-1.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3694/antlr4-runtime-4.5.3.jar" "__spark_libs__/antlr4-runtime-4.5.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/50/parquet-hadoop-bundle-1.6.0.jar" "__spark_libs__/parquet-hadoop-bundle-1.6.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/70/janino-2.7.8.jar" "__spark_libs__/janino-2.7.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3670/hadoop-yarn-api-2.7.1.jar" "__spark_libs__/hadoop-yarn-api-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/94/base64-2.3.8.jar" "__spark_libs__/base64-2.3.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3148/scala-reflect-2.11.8.jar" "__spark_libs__/scala-reflect-2.11.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/usercache/fairy/filecache/11521/stats-etlr-1.0.jar" "__app__.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/61/commons-dbcp-1.4.jar" "__spark_libs__/commons-dbcp-1.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/43/commons-beanutils-1.7.0.jar" "__spark_libs__/commons-beanutils-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/80/spark-launcher_2.11-2.0.1.jar" "__spark_libs__/spark-launcher_2.11-2.0.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3633/jul-to-slf4j-1.7.16.jar" "__spark_libs__/jul-to-slf4j-1.7.16.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/72/pyrolite-4.9.jar" "__spark_libs__/pyrolite-4.9.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3641/hive-beeline-1.2.1.spark2.jar" "__spark_libs__/hive-beeline-1.2.1.spark2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3684/RoaringBitmap-0.5.11.jar" "__spark_libs__/RoaringBitmap-0.5.11.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3585/libthrift-0.9.2.jar" "__spark_libs__/libthrift-0.9.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3698/jersey-client-2.22.2.jar" "__spark_libs__/jersey-client-2.22.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3660/commons-logging-1.1.3.jar" "__spark_libs__/commons-logging-1.1.3.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/98/calcite-linq4j-1.2.0-incubating.jar" "__spark_libs__/calcite-linq4j-1.2.0-incubating.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3652/oro-2.0.8.jar" "__spark_libs__/oro-2.0.8.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3683/javolution-5.5.1.jar" "__spark_libs__/javolution-5.5.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/99/httpclient-4.5.2.jar" "__spark_libs__/httpclient-4.5.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3611/snappy-0.2.jar" "__spark_libs__/snappy-0.2.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/93/aopalliance-1.0.jar" "__spark_libs__/aopalliance-1.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3681/hadoop-yarn-client-2.7.1.jar" "__spark_libs__/hadoop-yarn-client-2.7.1.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/88/parquet-hadoop-1.7.0.jar" "__spark_libs__/parquet-hadoop-1.7.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/31/commons-pool-1.5.4.jar" "__spark_libs__/commons-pool-1.5.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/90/ST4-4.0.4.jar" "__spark_libs__/ST4-4.0.4.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/3152/protobuf-java-2.5.0.jar" "__spark_libs__/protobuf-java-2.5.0.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/42/pmml-model-1.2.15.jar" "__spark_libs__/pmml-model-1.2.15.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
mkdir -p __spark_libs__
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
ln -sf "/hadoop/yarn/local/filecache/34/validation-api-1.1.0.Final.jar" "__spark_libs__/validation-api-1.1.0.Final.jar"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
# Creating copy of launch script
cp "launch_container.sh" "/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000003/launch_container.sh"
chmod 640 "/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000003/launch_container.sh"
# Determining directory contents
echo "ls -l:" 1>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000003/directory.info"
ls -l 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000003/directory.info"
echo "find -L . -maxdepth 5 -ls:" 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000003/directory.info"
find -L . -maxdepth 5 -ls 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000003/directory.info"
echo "broken symlinks(find -L . -maxdepth 5 -type l -ls):" 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000003/directory.info"
find -L . -maxdepth 5 -type l -ls 1>>"/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000003/directory.info"
exec /bin/bash -c "$JAVA_HOME/bin/java -server -Xmx3072m '-Dhdp.version=2.4.2.0-258' -Djava.io.tmpdir=$PWD/tmp '-Dspark.history.ui.port=18080' '-Dspark.ui.port=0' '-Dspark.driver.port=19210' -Dspark.yarn.app.container.log.dir=/hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000003 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.60.43.33:19210 --executor-id 2 --hostname c433.hadoop.gda.lo --cores 1 --app-id application_1494395298335_303847 --user-class-path file:$PWD/__app__.jar 1> /hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000003/stdout 2> /hadoop/yarn/log/application_1494395298335_303847/container_e81_1494395298335_303847_01_000003/stderr"
hadoop_shell_errorcode=$?
if [ $hadoop_shell_errorcode -ne 0 ]
then
  exit $hadoop_shell_errorcode
fi
End of LogType:launch_container.sh

LogType:stderr
Log Upload Time:Tue May 16 11:12:21 +0700 2017
LogLength:29757
Log Contents:
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/hadoop/yarn/local/filecache/83/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/hdp/2.4.2.0-258/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
17/05/16 11:10:33 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 39472@c433.hadoop.gda.lo
17/05/16 11:10:33 INFO util.SignalUtils: Registered signal handler for TERM
17/05/16 11:10:33 INFO util.SignalUtils: Registered signal handler for HUP
17/05/16 11:10:33 INFO util.SignalUtils: Registered signal handler for INT
17/05/16 11:10:34 INFO spark.SecurityManager: Changing view acls to: yarn,fairy
17/05/16 11:10:34 INFO spark.SecurityManager: Changing modify acls to: yarn,fairy
17/05/16 11:10:34 INFO spark.SecurityManager: Changing view acls groups to: 
17/05/16 11:10:34 INFO spark.SecurityManager: Changing modify acls groups to: 
17/05/16 11:10:34 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(yarn, fairy); groups with view permissions: Set(); users  with modify permissions: Set(yarn, fairy); groups with modify permissions: Set()
17/05/16 11:10:35 INFO client.TransportClientFactory: Successfully created connection to /10.60.43.33:19210 after 123 ms (0 ms spent in bootstraps)
17/05/16 11:10:35 INFO spark.SecurityManager: Changing view acls to: yarn,fairy
17/05/16 11:10:35 INFO spark.SecurityManager: Changing modify acls to: yarn,fairy
17/05/16 11:10:35 INFO spark.SecurityManager: Changing view acls groups to: 
17/05/16 11:10:35 INFO spark.SecurityManager: Changing modify acls groups to: 
17/05/16 11:10:35 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(yarn, fairy); groups with view permissions: Set(); users  with modify permissions: Set(yarn, fairy); groups with modify permissions: Set()
17/05/16 11:10:35 INFO client.TransportClientFactory: Successfully created connection to /10.60.43.33:19210 after 2 ms (0 ms spent in bootstraps)
17/05/16 11:10:35 INFO storage.DiskBlockManager: Created local directory at /hadoop/yarn/local/usercache/fairy/appcache/application_1494395298335_303847/blockmgr-fc77a1be-9588-4167-91c7-3e6229b695b1
17/05/16 11:10:35 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MB
17/05/16 11:10:36 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@10.60.43.33:19210
17/05/16 11:10:36 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver
17/05/16 11:10:36 INFO executor.Executor: Starting executor ID 2 on host c433.hadoop.gda.lo
17/05/16 11:10:36 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 28145.
17/05/16 11:10:36 INFO netty.NettyBlockTransferService: Server created on c433.hadoop.gda.lo:28145
17/05/16 11:10:36 INFO storage.BlockManager: external shuffle service port = 7337
17/05/16 11:10:36 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(2, c433.hadoop.gda.lo, 28145)
17/05/16 11:10:36 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(2, c433.hadoop.gda.lo, 28145)
17/05/16 11:10:36 INFO storage.BlockManager: Registering executor with local external shuffle service.
17/05/16 11:10:36 INFO client.TransportClientFactory: Successfully created connection to c433.hadoop.gda.lo/10.60.43.33:7337 after 1 ms (0 ms spent in bootstraps)
17/05/16 11:10:51 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 2
17/05/16 11:10:51 INFO executor.Executor: Running task 2.0 in stage 1.0 (TID 2)
17/05/16 11:10:51 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 0
17/05/16 11:10:51 INFO client.TransportClientFactory: Successfully created connection to /10.60.43.33:5141 after 3 ms (0 ms spent in bootstraps)
17/05/16 11:10:51 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.2 KB, free 1458.6 MB)
17/05/16 11:10:51 INFO broadcast.TorrentBroadcast: Reading broadcast variable 0 took 158 ms
17/05/16 11:10:51 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 30.3 KB, free 1458.6 MB)
17/05/16 11:10:51 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
17/05/16 11:10:51 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.60.43.33:19210)
17/05/16 11:10:51 INFO spark.MapOutputTrackerWorker: Got the output locations
17/05/16 11:10:51 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:51 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
17/05/16 11:10:52 INFO codegen.CodeGenerator: Code generated in 360.172035 ms
17/05/16 11:10:52 INFO codegen.CodeGenerator: Code generated in 13.94937 ms
17/05/16 11:10:52 INFO codegen.CodeGenerator: Code generated in 13.090859 ms
17/05/16 11:10:52 INFO codegen.CodeGenerator: Code generated in 13.556861 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 2.0 in stage 1.0 (TID 2). 4292 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 15
17/05/16 11:10:52 INFO executor.Executor: Running task 15.0 in stage 1.0 (TID 15)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 15.0 in stage 1.0 (TID 15). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 20
17/05/16 11:10:52 INFO executor.Executor: Running task 20.0 in stage 1.0 (TID 20)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 20.0 in stage 1.0 (TID 20). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 24
17/05/16 11:10:52 INFO executor.Executor: Running task 24.0 in stage 1.0 (TID 24)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 24.0 in stage 1.0 (TID 24). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 28
17/05/16 11:10:52 INFO executor.Executor: Running task 28.0 in stage 1.0 (TID 28)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 28.0 in stage 1.0 (TID 28). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 31
17/05/16 11:10:52 INFO executor.Executor: Running task 31.0 in stage 1.0 (TID 31)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 31.0 in stage 1.0 (TID 31). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 34
17/05/16 11:10:52 INFO executor.Executor: Running task 34.0 in stage 1.0 (TID 34)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 34.0 in stage 1.0 (TID 34). 3414 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 38
17/05/16 11:10:52 INFO executor.Executor: Running task 38.0 in stage 1.0 (TID 38)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 38.0 in stage 1.0 (TID 38). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 42
17/05/16 11:10:52 INFO executor.Executor: Running task 42.0 in stage 1.0 (TID 42)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 42.0 in stage 1.0 (TID 42). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 45
17/05/16 11:10:52 INFO executor.Executor: Running task 45.0 in stage 1.0 (TID 45)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 45.0 in stage 1.0 (TID 45). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 49
17/05/16 11:10:52 INFO executor.Executor: Running task 49.0 in stage 1.0 (TID 49)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 49.0 in stage 1.0 (TID 49). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 52
17/05/16 11:10:52 INFO executor.Executor: Running task 52.0 in stage 1.0 (TID 52)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 52.0 in stage 1.0 (TID 52). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 55
17/05/16 11:10:52 INFO executor.Executor: Running task 55.0 in stage 1.0 (TID 55)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 55.0 in stage 1.0 (TID 55). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 59
17/05/16 11:10:52 INFO executor.Executor: Running task 59.0 in stage 1.0 (TID 59)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 59.0 in stage 1.0 (TID 59). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 63
17/05/16 11:10:52 INFO executor.Executor: Running task 63.0 in stage 1.0 (TID 63)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 63.0 in stage 1.0 (TID 63). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 67
17/05/16 11:10:52 INFO executor.Executor: Running task 67.0 in stage 1.0 (TID 67)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 67.0 in stage 1.0 (TID 67). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 70
17/05/16 11:10:52 INFO executor.Executor: Running task 70.0 in stage 1.0 (TID 70)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 70.0 in stage 1.0 (TID 70). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 74
17/05/16 11:10:52 INFO executor.Executor: Running task 74.0 in stage 1.0 (TID 74)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 74.0 in stage 1.0 (TID 74). 3327 bytes result sent to driver
17/05/16 11:10:52 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 78
17/05/16 11:10:52 INFO executor.Executor: Running task 78.0 in stage 1.0 (TID 78)
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:52 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:52 INFO executor.Executor: Finished task 78.0 in stage 1.0 (TID 78). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 81
17/05/16 11:10:53 INFO executor.Executor: Running task 81.0 in stage 1.0 (TID 81)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 81.0 in stage 1.0 (TID 81). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 84
17/05/16 11:10:53 INFO executor.Executor: Running task 84.0 in stage 1.0 (TID 84)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 84.0 in stage 1.0 (TID 84). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 89
17/05/16 11:10:53 INFO executor.Executor: Running task 89.0 in stage 1.0 (TID 89)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 89.0 in stage 1.0 (TID 89). 3414 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 92
17/05/16 11:10:53 INFO executor.Executor: Running task 92.0 in stage 1.0 (TID 92)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 92.0 in stage 1.0 (TID 92). 3414 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 96
17/05/16 11:10:53 INFO executor.Executor: Running task 96.0 in stage 1.0 (TID 96)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 96.0 in stage 1.0 (TID 96). 3414 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 102
17/05/16 11:10:53 INFO executor.Executor: Running task 102.0 in stage 1.0 (TID 102)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 102.0 in stage 1.0 (TID 102). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 107
17/05/16 11:10:53 INFO executor.Executor: Running task 107.0 in stage 1.0 (TID 107)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 107.0 in stage 1.0 (TID 107). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 111
17/05/16 11:10:53 INFO executor.Executor: Running task 111.0 in stage 1.0 (TID 111)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 111.0 in stage 1.0 (TID 111). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 115
17/05/16 11:10:53 INFO executor.Executor: Running task 115.0 in stage 1.0 (TID 115)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 115.0 in stage 1.0 (TID 115). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 121
17/05/16 11:10:53 INFO executor.Executor: Running task 121.0 in stage 1.0 (TID 121)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 121.0 in stage 1.0 (TID 121). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 126
17/05/16 11:10:53 INFO executor.Executor: Running task 126.0 in stage 1.0 (TID 126)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 126.0 in stage 1.0 (TID 126). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 130
17/05/16 11:10:53 INFO executor.Executor: Running task 130.0 in stage 1.0 (TID 130)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 130.0 in stage 1.0 (TID 130). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 134
17/05/16 11:10:53 INFO executor.Executor: Running task 134.0 in stage 1.0 (TID 134)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 134.0 in stage 1.0 (TID 134). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 138
17/05/16 11:10:53 INFO executor.Executor: Running task 138.0 in stage 1.0 (TID 138)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 138.0 in stage 1.0 (TID 138). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 143
17/05/16 11:10:53 INFO executor.Executor: Running task 143.0 in stage 1.0 (TID 143)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 143.0 in stage 1.0 (TID 143). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 147
17/05/16 11:10:53 INFO executor.Executor: Running task 147.0 in stage 1.0 (TID 147)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 147.0 in stage 1.0 (TID 147). 3414 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 151
17/05/16 11:10:53 INFO executor.Executor: Running task 151.0 in stage 1.0 (TID 151)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 151.0 in stage 1.0 (TID 151). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 155
17/05/16 11:10:53 INFO executor.Executor: Running task 155.0 in stage 1.0 (TID 155)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 155.0 in stage 1.0 (TID 155). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 159
17/05/16 11:10:53 INFO executor.Executor: Running task 159.0 in stage 1.0 (TID 159)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 159.0 in stage 1.0 (TID 159). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 163
17/05/16 11:10:53 INFO executor.Executor: Running task 163.0 in stage 1.0 (TID 163)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 163.0 in stage 1.0 (TID 163). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 168
17/05/16 11:10:53 INFO executor.Executor: Running task 168.0 in stage 1.0 (TID 168)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 168.0 in stage 1.0 (TID 168). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 171
17/05/16 11:10:53 INFO executor.Executor: Running task 171.0 in stage 1.0 (TID 171)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 171.0 in stage 1.0 (TID 171). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 176
17/05/16 11:10:53 INFO executor.Executor: Running task 176.0 in stage 1.0 (TID 176)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 176.0 in stage 1.0 (TID 176). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 178
17/05/16 11:10:53 INFO executor.Executor: Running task 178.0 in stage 1.0 (TID 178)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 178.0 in stage 1.0 (TID 178). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 182
17/05/16 11:10:53 INFO executor.Executor: Running task 182.0 in stage 1.0 (TID 182)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 182.0 in stage 1.0 (TID 182). 3400 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 189
17/05/16 11:10:53 INFO executor.Executor: Running task 189.0 in stage 1.0 (TID 189)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 189.0 in stage 1.0 (TID 189). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 192
17/05/16 11:10:53 INFO executor.Executor: Running task 192.0 in stage 1.0 (TID 192)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 192.0 in stage 1.0 (TID 192). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 197
17/05/16 11:10:53 INFO executor.Executor: Running task 197.0 in stage 1.0 (TID 197)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks out of 0 blocks
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 197.0 in stage 1.0 (TID 197). 3327 bytes result sent to driver
17/05/16 11:10:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 200
17/05/16 11:10:53 INFO executor.Executor: Running task 0.0 in stage 2.0 (TID 200)
17/05/16 11:10:53 INFO spark.MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
17/05/16 11:10:53 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 1
17/05/16 11:10:53 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.4 KB, free 1458.6 MB)
17/05/16 11:10:53 INFO broadcast.TorrentBroadcast: Reading broadcast variable 1 took 14 ms
17/05/16 11:10:53 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.8 KB, free 1458.5 MB)
17/05/16 11:10:53 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them
17/05/16 11:10:53 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.60.43.33:19210)
17/05/16 11:10:53 INFO spark.MapOutputTrackerWorker: Got the output locations
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Getting 200 non-empty blocks out of 200 blocks
17/05/16 11:10:53 INFO client.TransportClientFactory: Successfully created connection to c411.hadoop.gda.lo/10.60.43.11:7337 after 2 ms (0 ms spent in bootstraps)
17/05/16 11:10:53 INFO client.TransportClientFactory: Successfully created connection to c432.hadoop.gda.lo/10.60.43.32:7337 after 16 ms (0 ms spent in bootstraps)
17/05/16 11:10:53 INFO client.TransportClientFactory: Successfully created connection to c414.hadoop.gda.lo/10.60.43.14:7337 after 21 ms (0 ms spent in bootstraps)
17/05/16 11:10:53 INFO storage.ShuffleBlockFetcherIterator: Started 3 remote fetches in 60 ms
17/05/16 11:10:53 INFO codegen.CodeGenerator: Code generated in 24.930102 ms
17/05/16 11:10:53 INFO executor.Executor: Finished task 0.0 in stage 2.0 (TID 200). 2722 bytes result sent to driver
17/05/16 11:10:54 INFO executor.CoarseGrainedExecutorBackend: Driver commanded a shutdown
17/05/16 11:10:54 INFO memory.MemoryStore: MemoryStore cleared
17/05/16 11:10:54 INFO storage.BlockManager: BlockManager stopped
17/05/16 11:10:54 INFO util.ShutdownHookManager: Shutdown hook called
End of LogType:stderr

LogType:stdout
Log Upload Time:Tue May 16 11:12:21 +0700 2017
LogLength:0
Log Contents:
End of LogType:stdout

